{"entries":[{"timestamp":1771458142143,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.blocks","patch":[{"start1":0,"length1":131,"diffs":[[1,"<xml xmlns=\"http://www.w3.org/1999/xhtml\">\n  <variables></variables>\n  <block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block>\n</xml>"]]}]},{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":1,"diffs":[[1," "]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":72,"length1":102,"diffs":[[1,"        \"device\": \"*\"\n"]]},{"start1":179,"length1":43,"diffs":[[1,"        \"assets.json\"\n"]]},{"start1":208,"length1":31,"diffs":[[1,"    \"additionalFilePaths\": []\n"]]}]},{"type":"added","filename":"tensor.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = new Array<number>(size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n    }\n}"}]},{"timestamp":1771458737350,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":0,"diffs":[[1,"\n"]]}]},{"type":"edited","filename":"tensor.ts","patch":[{"start1":67,"length1":39,"diffs":[[1,"        let arr = new Array<number>(size)\n"]]},{"start1":1418,"length1":6038,"diffs":[[1,""]]}]}]},{"timestamp":1771458992554,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":282,"length1":41,"diffs":[[1,"        \"tensor.ts\"\n"]]}]},{"type":"added","filename":"linear.ts","value":""}]},{"timestamp":1771468727252,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"linear.ts","patch":[{"start1":0,"length1":22,"diffs":[[1,""]]}]}]},{"timestamp":1771468727269,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":233,"diffs":[[1,""]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":303,"length1":38,"diffs":[[1,"        \"linear.ts\"\n"]]}]},{"type":"edited","filename":"linear.ts","patch":[{"start1":0,"length1":2907,"diffs":[[1,"// Add your code here\n"]]}]},{"type":"added","filename":"cnn.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}"}]},{"timestamp":1771469306258,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":324,"length1":132,"diffs":[[1,"        \"cnn.ts\"\n"]]}]},{"type":"added","filename":"adam.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}"},{"type":"added","filename":"activactions.ts","value":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"softmax.ts","value":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"sequtial.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}"},{"type":"added","filename":"embedding.ts","value":"namespace TorchNew {\n\n    // Safe allocator\n    function alloc(size: number): number[] {\n        let arr = new Array(size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}"}]},{"timestamp":1771469901657,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":433,"length1":176,"diffs":[[1,"        \"embedding.ts\"\n"]]}]},{"type":"edited","filename":"embedding.ts","patch":[{"start1":22,"length1":0,"diffs":[[1,"    // Safe allocator\n"]]},{"start1":89,"length1":40,"diffs":[[1,"        let arr = new Array(size)\n"]]}]},{"type":"added","filename":"avgpoolnd.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}"},{"type":"added","filename":"flatten.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"layernorm.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"dropout.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"crossentropyloss.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"positalencoding.ts","value":"// Add your code here\n"}]},{"timestamp":1771470487031,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":580,"length1":138,"diffs":[[1,"        \"positalencoding.ts\"\n"]]}]},{"type":"edited","filename":"positalencoding.ts","patch":[{"start1":22,"length1":2600,"diffs":[[1,""]]}]},{"type":"added","filename":"maxpoolnd.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class MaxPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        lastInput: Tensor\n        lastMaxIndex: number[]   // stores argmax positions for backward\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Store argmax indices for backward\n            this.lastMaxIndex = alloc(total)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                let maxVal = -1e30\n                let maxIndex = -1\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let flat = x.index(idxX)\n                            let v = x.data[flat]\n                            if (v > maxVal) {\n                                maxVal = v\n                                maxIndex = flat\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                outData[p] = maxVal\n                this.lastMaxIndex[p] = maxIndex\n\n                p++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                let idx = this.lastMaxIndex[p]\n                if (idx >= 0) {\n                    dXdata[idx] += gradOut.data[p]\n                }\n                p++\n            }\n\n            return dX\n        }\n    }\n}"},{"type":"added","filename":"convtransposend.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class ConvTransposeND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        weight: Tensor\n        bias: Tensor\n\n        dW: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], stride: number[], padding: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [inChannels, outChannels, ...kernelShape]\n            let wShape = alloc(2 + dims)\n            wShape[0] = inChannels\n            wShape[1] = outChannels\n            let i = 0\n            while (i < dims) {\n                wShape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            let wSize = 1\n            i = 0\n            while (i < wShape.length) {\n                wSize *= wShape[i]\n                i++\n            }\n\n            let wData = alloc(wSize)\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wSize) {\n                wData[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wData, wShape)\n            this.dW = new Tensor(alloc(wSize), wShape)\n\n            // Bias: [outChannels]\n            let bData = alloc(outChannels)\n            this.bias = new Tensor(bData, [outChannels])\n            this.dB = new Tensor(alloc(outChannels), [outChannels])\n        }\n\n        // ---------------------------------------------------------\n        // Forward (fractionally strided convolution)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let inC = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let s = this.stride[i]\n                let p = this.padding[i]\n\n                // Transposed conv output formula:\n                outSpatial[i] = (inSize - 1) * s - 2 * p + k\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Compute base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute output index\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= outSpatial[d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                // Weight index\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n\n                                let wVal = this.weight.get(idxW)\n                                let outIdx = out.index(idxOut)\n                                outData[outIdx] += x.data[p] * wVal\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            // Add bias\n            let q = 0\n            while (q < outData.length) {\n                let idx = Tensor.unravelIndex(q, outShape)\n                let oc = idx[1]\n                outData[q] += this.bias.data[oc]\n                q++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dW.data.length) {\n                this.dW.data[i] = 0\n                i++\n            }\n            i = 0\n            while (i < this.dB.data.length) {\n                this.dB.data[i] = 0\n                i++\n            }\n\n            // dB: sum over gradOut\n            let q = 0\n            while (q < gradOut.data.length) {\n                let idx = Tensor.unravelIndex(q, gradOut.shape)\n                let oc = idx[1]\n                this.dB.data[oc] += gradOut.data[q]\n                q++\n            }\n\n            // dX\n            let dXdata = alloc(x.data.length)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions for dW and dX\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= gradOut.shape[2 + d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                let go = gradOut.get(idxOut)\n\n                                // dW\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n                                let wIdx = this.weight.index(idxW)\n                                this.dW.data[wIdx] += x.data[p] * go\n\n                                // dX\n                                dXdata[p] += this.weight.data[wIdx] * go\n\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}"},{"type":"added","filename":"rnn.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n    // tanh implementation (same as in activations.ts)\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class RNN {\n        inputDim: number\n        hiddenDim: number\n\n        Wxh: Tensor\n        Whh: Tensor\n        b: Tensor\n\n        dWxh: Tensor\n        dWhh: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n        lastH: number[]      // all hidden states (flattened)\n        lastA: number[]      // all pre-activations (Wxh x + Whh h + b)\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            // Weight shapes\n            this.Wxh = new Tensor(\n                this.initWeights(inputDim * hiddenDim),\n                [hiddenDim, inputDim]\n            )\n\n            this.Whh = new Tensor(\n                this.initWeights(hiddenDim * hiddenDim),\n                [hiddenDim, hiddenDim]\n            )\n\n            this.b = new Tensor(\n                alloc(hiddenDim),\n                [hiddenDim]\n            )\n\n            this.dWxh = new Tensor(alloc(inputDim * hiddenDim), [hiddenDim, inputDim])\n            this.dWhh = new Tensor(alloc(hiddenDim * hiddenDim), [hiddenDim, hiddenDim])\n            this.dB = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeights(size: number): number[] {\n            let arr = alloc(size)\n            let scale = 1 / Math.sqrt(size)\n            let i = 0\n            while (i < size) {\n                arr[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return arr\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            // Store hidden states and pre-activations\n            this.lastH = alloc(batch * (seq + 1) * hDim)  // includes h0 = 0\n            this.lastA = alloc(batch * seq * hDim)\n\n            // Initialize h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            // Forward through time\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    // Compute h_t\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.b.data[j]\n\n                        // Wxh * x_t\n                        let k = 0\n                        while (k < inDim) {\n                            let w = this.Wxh.data[j * inDim + k]\n                            let xv = x.data[baseX + k]\n                            sum += w * xv\n                            k++\n                        }\n\n                        // Whh * h_{t-1}\n                        k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[j * hDim + k]\n                            let hv = this.lastH[baseHprev + k]\n                            sum += w * hv\n                            k++\n                        }\n\n                        this.lastA[baseA + j] = sum\n                        this.lastH[baseHcur + j] = fastTanh(sum)\n\n                        // Output is h_t\n                        outData[(b * seq + t) * hDim + j] = this.lastH[baseHcur + j]\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWxh.data.length) { this.dWxh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWhh.data.length) { this.dWhh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dB.data.length) { this.dB.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)  // gradient from next time step\n\n            // BPTT\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    // dL/dh_t = gradOut + dHnext\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dA = dH * (1 - tanh(a)^2)\n                    let dA = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let h = this.lastH[baseHcur + j]\n                        dA[j] = dH[j] * (1 - h * h)\n                        j++\n                    }\n\n                    // Accumulate gradients\n                    j = 0\n                    while (j < hDim) {\n                        // dB\n                        this.dB.data[j] += dA[j]\n\n                        // dWxh\n                        let k = 0\n                        while (k < inDim) {\n                            let idx = j * inDim + k\n                            this.dWxh.data[idx] += dA[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        // dWhh\n                        k = 0\n                        while (k < hDim) {\n                            let idx = j * hDim + k\n                            this.dWhh.data[idx] += dA[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dX\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Wxh.data[k * inDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // dHnext = Whh^T * dA\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[k * hDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"gru.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class GRU {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wz: Tensor; Uz: Tensor; bz: Tensor\n        Wr: Tensor; Ur: Tensor; br: Tensor\n        Wh: Tensor; Uh: Tensor; bh: Tensor\n\n        // Gradients\n        dWz: Tensor; dUz: Tensor; dBz: Tensor\n        dWr: Tensor; dUr: Tensor; dBr: Tensor\n        dWh: Tensor; dUh: Tensor; dBh: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastZ: number[]\n        lastR: number[]\n        lastHtilde: number[]\n        lastH: number[]   // includes h0\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wz = this.initWeight(hiddenDim, inputDim)\n            this.Uz = this.initWeight(hiddenDim, hiddenDim)\n            this.bz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wr = this.initWeight(hiddenDim, inputDim)\n            this.Ur = this.initWeight(hiddenDim, hiddenDim)\n            this.br = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wh = this.initWeight(hiddenDim, inputDim)\n            this.Uh = this.initWeight(hiddenDim, hiddenDim)\n            this.bh = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWz = this.initZero(hiddenDim, inputDim)\n            this.dUz = this.initZero(hiddenDim, hiddenDim)\n            this.dBz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWr = this.initZero(hiddenDim, inputDim)\n            this.dUr = this.initZero(hiddenDim, hiddenDim)\n            this.dBr = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWh = this.initZero(hiddenDim, inputDim)\n            this.dUh = this.initZero(hiddenDim, hiddenDim)\n            this.dBh = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastZ = alloc(batch * seq * hDim)\n            this.lastR = alloc(batch * seq * hDim)\n            this.lastHtilde = alloc(batch * seq * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // ---- Compute z_t ----\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.bz.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wz.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Uz.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let z = sigmoid(sum)\n                        this.lastZ[baseZ + j] = z\n                        j++\n                    }\n\n                    // ---- Compute r_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.br.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wr.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Ur.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let r = sigmoid(sum)\n                        this.lastR[baseR + j] = r\n                        j++\n                    }\n\n                    // ---- Compute h~_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.bh.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wh.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            let r = this.lastR[baseR + k]\n                            sum += this.Uh.data[j * hDim + k] * (r * hprev)\n                            k++\n                        }\n\n                        let htilde = fastTanh(sum)\n                        this.lastHtilde[baseHtilde + j] = htilde\n                        j++\n                    }\n\n                    // ---- Final h_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        let h = (1 - z) * hprev + z * htilde\n                        this.lastH[baseHcur + j] = h\n\n                        outData[(b * seq + t) * hDim + j] = h\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWz.data.length) { this.dWz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUz.data.length) { this.dUz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBz.data.length) { this.dBz.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWr.data.length) { this.dWr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUr.data.length) { this.dUr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBr.data.length) { this.dBr.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWh.data.length) { this.dWh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUh.data.length) { this.dUh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBh.data.length) { this.dBh.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Compute gate derivatives\n                    let dZ = alloc(hDim)\n                    let dHtilde = alloc(hDim)\n                    let dHprev = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        dZ[j] = dH[j] * (htilde - hprev)\n                        dHtilde[j] = dH[j] * z\n                        dHprev[j] = dH[j] * (1 - z)\n                        j++\n                    }\n\n                    // dA_h = dHtilde * (1 - htilde^2)\n                    let dA_h = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let htilde = this.lastHtilde[baseHtilde + j]\n                        dA_h[j] = dHtilde[j] * (1 - htilde * htilde)\n                        j++\n                    }\n\n                    // dA_z = dZ * z * (1 - z)\n                    let dA_z = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        dA_z[j] = dZ[j] * z * (1 - z)\n                        j++\n                    }\n\n                    // dA_r = dHtilde * Uh * hprev * r*(1-r)\n                    let dA_r = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Uh.data[k * hDim + j]\n                            let d = dA_h[k]\n                            sum += d * w\n                            k++\n                        }\n                        let r = this.lastR[baseR + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        dA_r[j] = sum * hprev * r * (1 - r)\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWh, dUh, dBh\n                    j = 0\n                    while (j < hDim) {\n                        this.dBh.data[j] += dA_h[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWh.data[j * inDim + k] += dA_h[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let r = this.lastR[baseR + k]\n                            let hprev = this.lastH[baseHprev + k]\n                            this.dUh.data[j * hDim + k] += dA_h[j] * (r * hprev)\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWz, dUz, dBz\n                    j = 0\n                    while (j < hDim) {\n                        this.dBz.data[j] += dA_z[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWz.data[j * inDim + k] += dA_z[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUz.data[j * hDim + k] += dA_z[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWr, dUr, dBr\n                    j = 0\n                    while (j < hDim) {\n                        this.dBr.data[j] += dA_r[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWr.data[j * inDim + k] += dA_r[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUr.data[j * hDim + k] += dA_r[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Wz.data[k * inDim + j]\n                            sum += dA_r[k] * this.Wr.data[k * inDim + j]\n                            sum += dA_h[k] * this.Wh.data[k * inDim + j]\n                            k++\n                        }\n\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = dHprev[j]\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Uz.data[k * hDim + j]\n                            sum += dA_r[k] * this.Ur.data[k * hDim + j]\n                            sum += dA_h[k] * this.Uh.data[k * hDim + j] * this.lastR[baseR + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"lstm.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = new Array(size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class LSTM {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wi: Tensor; Ui: Tensor; bi: Tensor\n        Wf: Tensor; Uf: Tensor; bf: Tensor\n        Wo: Tensor; Uo: Tensor; bo: Tensor\n        Wc: Tensor; Uc: Tensor; bc: Tensor\n\n        // Gradients\n        dWi: Tensor; dUi: Tensor; dBi: Tensor\n        dWf: Tensor; dUf: Tensor; dBf: Tensor\n        dWo: Tensor; dUo: Tensor; dBo: Tensor\n        dWc: Tensor; dUc: Tensor; dBc: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastI: number[]\n        lastF: number[]\n        lastO: number[]\n        lastCtilde: number[]\n        lastC: number[]\n        lastH: number[]\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wi = this.initWeight(hiddenDim, inputDim)\n            this.Ui = this.initWeight(hiddenDim, hiddenDim)\n            this.bi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wf = this.initWeight(hiddenDim, inputDim)\n            this.Uf = this.initWeight(hiddenDim, hiddenDim)\n            this.bf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wo = this.initWeight(hiddenDim, inputDim)\n            this.Uo = this.initWeight(hiddenDim, hiddenDim)\n            this.bo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wc = this.initWeight(hiddenDim, inputDim)\n            this.Uc = this.initWeight(hiddenDim, hiddenDim)\n            this.bc = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWi = this.initZero(hiddenDim, inputDim)\n            this.dUi = this.initZero(hiddenDim, hiddenDim)\n            this.dBi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWf = this.initZero(hiddenDim, inputDim)\n            this.dUf = this.initZero(hiddenDim, hiddenDim)\n            this.dBf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWo = this.initZero(hiddenDim, inputDim)\n            this.dUo = this.initZero(hiddenDim, hiddenDim)\n            this.dBo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWc = this.initZero(hiddenDim, inputDim)\n            this.dUc = this.initZero(hiddenDim, hiddenDim)\n            this.dBc = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastI = alloc(batch * seq * hDim)\n            this.lastF = alloc(batch * seq * hDim)\n            this.lastO = alloc(batch * seq * hDim)\n            this.lastCtilde = alloc(batch * seq * hDim)\n            this.lastC = alloc(batch * (seq + 1) * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // Initialize h0 = 0, c0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                this.lastC[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // ---- Compute gates ----\n                    let j = 0\n                    while (j < hDim) {\n                        // Input gate\n                        let sumI = this.bi.data[j]\n                        let sumF = this.bf.data[j]\n                        let sumO = this.bo.data[j]\n                        let sumC = this.bc.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            let xv = x.data[baseX + k]\n                            sumI += this.Wi.data[j * inDim + k] * xv\n                            sumF += this.Wf.data[j * inDim + k] * xv\n                            sumO += this.Wo.data[j * inDim + k] * xv\n                            sumC += this.Wc.data[j * inDim + k] * xv\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            sumI += this.Ui.data[j * hDim + k] * hprev\n                            sumF += this.Uf.data[j * hDim + k] * hprev\n                            sumO += this.Uo.data[j * hDim + k] * hprev\n                            sumC += this.Uc.data[j * hDim + k] * hprev\n                            k++\n                        }\n\n                        let iGate = sigmoid(sumI)\n                        let fGate = sigmoid(sumF)\n                        let oGate = sigmoid(sumO)\n                        let cTilde = fastTanh(sumC)\n\n                        this.lastI[baseI + j] = iGate\n                        this.lastF[baseF + j] = fGate\n                        this.lastO[baseO + j] = oGate\n                        this.lastCtilde[baseCtilde + j] = cTilde\n\n                        // Cell state\n                        let cprev = this.lastC[baseCprev + j]\n                        let ccur = fGate * cprev + iGate * cTilde\n                        this.lastC[baseCcur + j] = ccur\n\n                        // Hidden state\n                        let hcur = oGate * fastTanh(ccur)\n                        this.lastH[baseHcur + j] = hcur\n\n                        outData[(b * seq + t) * hDim + j] = hcur\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWi.data.length) { this.dWi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUi.data.length) { this.dUi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBi.data.length) { this.dBi.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWf.data.length) { this.dWf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUf.data.length) { this.dUf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBf.data.length) { this.dBf.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUo.data.length) { this.dUo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBo.data.length) { this.dBo.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWc.data.length) { this.dWc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUc.data.length) { this.dUc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBc.data.length) { this.dBc.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n            let dCnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dL/dc_t\n                    let dC = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let o = this.lastO[baseO + j]\n                        let ccur = this.lastC[baseCcur + j]\n                        dC[j] = dH[j] * o * (1 - fastTanh(ccur) * fastTanh(ccur)) + dCnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Gate derivatives\n                    let dI = alloc(hDim)\n                    let dF = alloc(hDim)\n                    let dO = alloc(hDim)\n                    let dCtilde = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let iGate = this.lastI[baseI + j]\n                        let fGate = this.lastF[baseF + j]\n                        let oGate = this.lastO[baseO + j]\n                        let cTilde = this.lastCtilde[baseCtilde + j]\n                        let cprev = this.lastC[baseCprev + j]\n\n                        dI[j] = dC[j] * cTilde * iGate * (1 - iGate)\n                        dF[j] = dC[j] * cprev * fGate * (1 - fGate)\n                        dO[j] = dH[j] * fastTanh(this.lastC[baseCcur + j]) * oGate * (1 - oGate)\n                        dCtilde[j] = dC[j] * iGate * (1 - cTilde * cTilde)\n\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWi, dUi, dBi\n                    j = 0\n                    while (j < hDim) {\n                        this.dBi.data[j] += dI[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWi.data[j * inDim + k] += dI[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUi.data[j * hDim + k] += dI[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWf, dUf, dBf\n                    j = 0\n                    while (j < hDim) {\n                        this.dBf.data[j] += dF[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWf.data[j * inDim + k] += dF[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUf.data[j * hDim + k] += dF[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWo, dUo, dBo\n                    j = 0\n                    while (j < hDim) {\n                        this.dBo.data[j] += dO[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWo.data[j * inDim + k] += dO[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUo.data[j * hDim + k] += dO[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWc, dUc, dBc\n                    j = 0\n                    while (j < hDim) {\n                        this.dBc.data[j] += dCtilde[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWc.data[j * inDim + k] += dCtilde[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUc.data[j * hDim + k] += dCtilde[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dI[k] * this.Wi.data[k * inDim + j]\n                            sum += dF[k] * this.Wf.data[k * inDim + j]\n                            sum += dO[k] * this.Wo.data[k * inDim + j]\n                            sum += dCtilde[k] * this.Wc.data[k * inDim + j]\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext and dCnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sumH = 0\n                        let sumC = dC[j] * this.lastF[baseF + j]\n\n                        let k"}]},{"timestamp":1771471037528,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":700,"length1":132,"diffs":[[1,"        \"lstm.ts\"\n"]]}]},{"type":"edited","filename":"lstm.ts","patch":[{"start1":21,"length1":0,"diffs":[[1,"\n"]]},{"start1":67,"length1":40,"diffs":[[1,"        let arr = new Array(size)\n"]]},{"start1":220,"length1":0,"diffs":[[1,"\n"]]},{"start1":14702,"length1":230,"diffs":[[1,""]]},{"start1":14739,"length1":0,"diffs":[[1,"                        let sumC = dC[j] * this.lastF[baseF + j]\n"]]},{"start1":14805,"length1":1258,"diffs":[[1,"                        let k"]]}]},{"type":"added","filename":"residual.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Residual {\n        layer: any\n        lastInput: Tensor\n        lastLayerOut: Tensor\n\n        constructor(layer: any) {\n            this.layer = layer\n        }\n\n        // ---------------------------------------------------------\n        // Forward: y = x + layer(x)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let outLayer = this.layer.forward(x)\n            this.lastLayerOut = outLayer\n\n            let size = x.data.length\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                outData[i] = x.data[i] + outLayer.data[i]\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dX = gradOut + dLayer\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradLayer = this.layer.backward(gradOut)\n\n            let size = gradOut.data.length\n            let dXdata = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dXdata[i] = gradOut.data[i] + gradLayer.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"feedforward.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class FeedForward {\n        layer1: any\n        activation: any\n        layer2: any\n\n        lastInput: Tensor\n        lastHidden: Tensor\n\n        constructor(inputDim: number, hiddenDim: number, outputDim: number, activation: any) {\n            this.layer1 = new Linear(inputDim, hiddenDim)\n            this.activation = activation\n            this.layer2 = new Linear(hiddenDim, outputDim)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let h1 = this.layer1.forward(x)\n            let h2 = this.activation.forward(h1)\n            this.lastHidden = h2\n\n            let out = this.layer2.forward(h2)\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            // Backprop through second linear\n            let g2 = this.layer2.backward(gradOut)\n\n            // Backprop through activation\n            let gAct = this.activation.backward(g2)\n\n            // Backprop through first linear\n            let g1 = this.layer1.backward(gAct)\n\n            return g1\n        }\n    }\n}"},{"type":"added","filename":"multiheadattention.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function softmaxRow(data: number[], start: number, length: number): void {\n        let maxVal = data[start]\n        let i = 1\n        while (i < length) {\n            let v = data[start + i]\n            if (v > maxVal) maxVal = v\n            i++\n        }\n\n        let sum = 0\n        i = 0\n        while (i < length) {\n            let e = Math.exp(data[start + i] - maxVal)\n            data[start + i] = e\n            sum += e\n            i++\n        }\n\n        i = 0\n        while (i < length) {\n            data[start + i] /= sum\n            i++\n        }\n    }\n\n    export class MultiHeadAttention {\n        embedDim: number\n        numHeads: number\n        headDim: number\n\n        Wq: Tensor; bq: Tensor\n        Wk: Tensor; bk: Tensor\n        Wv: Tensor; bv: Tensor\n        Wo: Tensor; bo: Tensor\n\n        dWq: Tensor; dbq: Tensor\n        dWk: Tensor; dbk: Tensor\n        dWv: Tensor; dbv: Tensor\n        dWo: Tensor; dbo: Tensor\n\n        lastInput: Tensor\n        lastQ: Tensor\n        lastK: Tensor\n        lastV: Tensor\n        lastScores: number[]\n        lastSoftmax: number[]\n        lastAttention: number[]\n\n        constructor(embedDim: number, numHeads: number) {\n            this.embedDim = embedDim\n            this.numHeads = numHeads\n            this.headDim = Math.idiv(embedDim, numHeads)\n\n            this.Wq = new Linear(embedDim, embedDim).weight\n            this.bq = new Linear(embedDim, embedDim).bias\n\n            this.Wk = new Linear(embedDim, embedDim).weight\n            this.bk = new Linear(embedDim, embedDim).bias\n\n            this.Wv = new Linear(embedDim, embedDim).weight\n            this.bv = new Linear(embedDim, embedDim).bias\n\n            this.Wo = new Linear(embedDim, embedDim).weight\n            this.bo = new Linear(embedDim, embedDim).bias\n\n            this.dWq = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbq = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWk = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbk = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWv = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbv = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWo = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbo = new Tensor(alloc(embedDim), [embedDim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, embedDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(x, this.Wq, this.bq)\n            let K = this.linearForward(x, this.Wk, this.bk)\n            let V = this.linearForward(x, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores and softmax\n            let scores = alloc(batch * H * seq * seq)\n            let softmaxOut = alloc(batch * H * seq * seq)\n            let attention = alloc(batch * seq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            // Compute attention scores\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n\n                    let i = 0\n                    while (i < seq) {\n                        let j = 0\n                        while (j < seq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * seq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < seq) {\n                        softmaxRow(scores, sBase + i2 * seq, seq)\n                        let j2 = 0\n                        while (j2 < seq) {\n                            softmaxOut[sBase + i2 * seq + j2] = scores[sBase + i2 * seq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * seq * E\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < seq) {\n                                let w = softmaxOut[sBase + i3 * seq + j3]\n                                let vv = V.data[kBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, seq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection: attention -> gradAtt\n            let attTensor = new Tensor(this.lastAttention, [batch, seq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // 2) Allocate grads for Q, K, V and softmax/scores\n            let dQdata = alloc(batch * seq * E)\n            let dKdata = alloc(batch * seq * E)\n            let dVdata = alloc(batch * seq * E)\n\n            let dSoft = alloc(batch * H * seq * seq)\n            let dScores = alloc(batch * H * seq * seq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 3) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n                    let outBase = b * seq * E\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n                    let vBase = (b * seq * E) + h * D\n\n                    // dSoft and dV from gradAtt\n                    let i2 = 0\n                    while (i2 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < seq) {\n                                let w = this.lastSoftmax[sBase + i2 * seq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * seq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 4) Softmax backward: dScores from dSoft and probs\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dScores[sBase + i3 * seq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 5) Scores = (QK^T)/sqrt(D) -> dQ, dK\n                    let i4 = 0\n                    while (i4 < seq) {\n                        let j4 = 0\n                        while (j4 < seq) {\n                            let ds = dScores[sBase + i4 * seq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 6) Merge dQ, dK, dV back to [batch, seq, E] tensors\n            let dQ = new Tensor(dQdata, [batch, seq, E])\n            let dK = new Tensor(dKdata, [batch, seq, E])\n            let dV = new Tensor(dVdata, [batch, seq, E])\n\n            // 7) Back through Q, K, V linears to input x\n            let dXq = this.linearBackward(x, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(x, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(x, dV, this.Wv, this.dWv, this.dbv)\n\n            // 8) Sum contributions: dX = dXq + dXk + dXv\n            let dXdata = alloc(x.data.length)\n            i = 0\n            while (i < dXdata.length) {\n                dXdata[i] = dXq.data[i] + dXk.data[i] + dXv.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear forward\n        // ---------------------------------------------------------\n        linearForward(x: Tensor, W: Tensor, b: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = b.shape[0]\n\n            let outData = alloc(batch * seq * outDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseO = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let sum = b.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += W.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        outData[baseO + j] = sum\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(outData, [batch, seq, outDim])\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear backward (accumulates dW, db)\n        // ---------------------------------------------------------\n        linearBackward(x: Tensor, gradOut: Tensor, W: Tensor, dW: Tensor, db: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = gradOut.shape[2]\n\n            let gradInData = alloc(batch * seq * inDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseG = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let g = gradOut.data[baseG + j]\n\n                        db.data[j] += g\n\n                        let k = 0\n                        while (k < inDim) {\n                            dW.data[j * inDim + k] += g * x.data[baseX + k]\n                            gradInData[baseX + k] += g * W.data[j * inDim + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(gradInData, [batch, seq, inDim])\n        }\n    }\n}"},{"type":"added","filename":"tranformerencoder.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = new Array(size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerEncoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        mha: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterMHA: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm(embedDim)\n            this.mha = new MultiHeadAttention(embedDim, numHeads)\n            this.ln2 = new LayerNorm(embedDim)\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            // 1) LayerNorm â†’ MHA â†’ Residual\n            let xNorm1 = this.ln1.forward(x)\n            let mhaOut = this.mha.forward(xNorm1)\n\n            let size = x.data.length\n            let afterMHAdata = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterMHAdata[i] = x.data[i] + mhaOut.data[i]\n                i++\n            }\n            let afterMHA = new Tensor(afterMHAdata, x.shape.slice(0))\n            this.lastAfterMHA = afterMHA\n\n            // 2) LayerNorm â†’ FeedForward â†’ Residual\n            let xNorm2 = this.ln2.forward(afterMHA)\n            let ffOut = this.ff.forward(xNorm2)\n\n            let afterFFdata = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFdata[i] = afterMHA.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFdata, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // 1) Backprop through second residual: d(afterMHA) += gradOut\n            let dAfterMHAdata = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterMHAdata[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            // 2) Backprop through FeedForward\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n\n            // 3) Backprop through LayerNorm2\n            let dNorm2 = this.ln2.backward(dFF)\n\n            // Add to dAfterMHA\n            i = 0\n            while (i < size) {\n                dAfterMHAdata[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterMHA = new Tensor(dAfterMHAdata, gradOut.shape.slice(0))\n\n            // 4) Backprop through first residual: dX += dAfterMHA\n            let dXdata = alloc(size)\n            let dMHAout = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterMHA.data[i]\n                dMHAout[i] = dAfterMHA.data[i]\n                i++\n            }\n\n            // 5) Backprop through MHA\n            let dMHA = this.mha.backward(new Tensor(dMHAout, gradOut.shape.slice(0)))\n\n            // 6) Backprop through LayerNorm1\n            let dNorm1 = this.ln1.backward(dMHA)\n\n            // Add to dX\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}"}]},{"timestamp":1771471632513,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":801,"length1":93,"diffs":[[1,"        \"tranformerencoder.ts\"\n"]]}]},{"type":"edited","filename":"multiheadattention.ts","patch":[{"start1":14907,"length1":9264,"diffs":[[1,""]]}]},{"type":"edited","filename":"tranformerencoder.ts","patch":[{"start1":67,"length1":40,"diffs":[[1,"        let arr = new Array(size)\n"]]},{"start1":525,"length1":99,"diffs":[[1,"            this.ln1 = new LayerNorm(embedDim)\n"]]},{"start1":638,"length1":288,"diffs":[[1,"            this.ln2 = new LayerNorm(embedDim)\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new ReLU())\n"]]}]},{"type":"added","filename":"tranformerdecoder.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerDecoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        ln3: LayerNorm\n\n        selfAtt: MultiHeadAttention\n        crossAtt: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterSelf: Tensor\n        lastAfterCross: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm([embedDim])\n            this.selfAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n            this.crossAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln3 = new LayerNorm([embedDim])\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new TorchNew.Activations.ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor, encoderOut: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let embedDim = x.shape[2]\n\n            // -----------------------------\n            // 1) Masked Self-Attention\n            // -----------------------------\n            let xNorm1 = this.ln1.forward(x)\n\n            // Apply causal mask: disallow attending to future tokens\n            let masked = this.applyCausalMask(xNorm1)\n\n            let selfOut = this.selfAtt.forward(masked)\n\n            let size = x.data.length\n            let afterSelfData = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterSelfData[i] = x.data[i] + selfOut.data[i]\n                i++\n            }\n            let afterSelf = new Tensor(afterSelfData, x.shape.slice(0))\n            this.lastAfterSelf = afterSelf\n\n            // -----------------------------\n            // 2) Cross-Attention\n            // -----------------------------\n            let xNorm2 = this.ln2.forward(afterSelf)\n\n            // Cross-attention: query = decoder, key/value = encoder\n            let crossOut = this.crossAtt.forwardKV(xNorm2, encoderOut, encoderOut)\n\n            let afterCrossData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterCrossData[i] = afterSelf.data[i] + crossOut.data[i]\n                i++\n            }\n            let afterCross = new Tensor(afterCrossData, x.shape.slice(0))\n            this.lastAfterCross = afterCross\n\n            // -----------------------------\n            // 3) FeedForward\n            // -----------------------------\n            let xNorm3 = this.ln3.forward(afterCross)\n            let ffOut = this.ff.forward(xNorm3)\n\n            let afterFFData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFData[i] = afterCross.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFData, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor, encoderOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // -----------------------------\n            // 1) Backprop through FF residual\n            // -----------------------------\n            let dAfterCrossData = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterCrossData[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n            let dNorm3 = this.ln3.backward(dFF)\n\n            i = 0\n            while (i < size) {\n                dAfterCrossData[i] += dNorm3.data[i]\n                i++\n            }\n\n            let dAfterCross = new Tensor(dAfterCrossData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 2) Backprop through cross-attention residual\n            // -----------------------------\n            let dAfterSelfData = alloc(size)\n            let dCrossOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] = dAfterCross.data[i]\n                dCrossOut[i] = dAfterCross.data[i]\n                i++\n            }\n\n            let dCross = this.crossAtt.backwardKV(\n                new Tensor(dCrossOut, gradOut.shape.slice(0)),\n                encoderOut,\n                encoderOut\n            )\n\n            let dNorm2 = this.ln2.backward(dCross)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterSelf = new Tensor(dAfterSelfData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 3) Backprop through self-attention residual\n            // -----------------------------\n            let dXdata = alloc(size)\n            let dSelfOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterSelf.data[i]\n                dSelfOut[i] = dAfterSelf.data[i]\n                i++\n            }\n\n            let dSelf = this.selfAtt.backward(new Tensor(dSelfOut, gradOut.shape.slice(0)))\n            let dNorm1 = this.ln1.backward(dSelf)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Causal mask: zero out future positions\n        // ---------------------------------------------------------\n        applyCausalMask(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let b = 0\n            while (b < batch) {\n                let i = 0\n                while (i < seq) {\n                    let j = 0\n                    while (j < seq) {\n                        let k = 0\n                        while (k < dim) {\n                            let idx = b * seq * dim + i * dim + k\n                            let src = b * seq * dim + j * dim + k\n\n                            if (j <= i) {\n                                outData[idx] = x.data[src]\n                            } else {\n                                outData[idx] = 0\n                            }\n\n                            k++\n                        }\n                        j++\n                    }\n                    i++\n                }\n                b++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}"},{"type":"added","filename":"tranformermodel.ts","value":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerModel {\n        embed: Embedding\n        posEnc: PositionalEncoding\n\n        encoderBlocks: TransformerEncoder[]\n        decoderBlocks: TransformerDecoder[]\n\n        finalLinear: Linear\n\n        lastInput: Tensor\n        lastEncoderOut: Tensor\n        lastDecoderOut: Tensor\n\n        constructor(\n            vocabSize: number,\n            embedDim: number,\n            numHeads: number,\n            ffHiddenDim: number,\n            numEncoderLayers: number,\n            numDecoderLayers: number,\n            maxSeqLen: number\n        ) {\n            this.embed = new Embedding(vocabSize, embedDim)\n            this.posEnc = new PositionalEncoding(maxSeqLen, embedDim)\n\n            this.encoderBlocks = []\n            let i = 0\n            while (i < numEncoderLayers) {\n                this.encoderBlocks.push(\n                    new TransformerEncoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.decoderBlocks = []\n            i = 0\n            while (i < numDecoderLayers) {\n                this.decoderBlocks.push(\n                    new TransformerDecoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.finalLinear = new Linear(embedDim, vocabSize)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(srcTokens: Tensor, tgtTokens: Tensor): Tensor {\n            // srcTokens: [batch, srcSeq]\n            // tgtTokens: [batch, tgtSeq]\n\n            this.lastInput = srcTokens\n\n            // 1) Embed + positional encode source\n            let srcEmb = this.embed.forward(srcTokens)\n            let srcPE = this.posEnc.forward(srcEmb)\n\n            // 2) Pass through encoder stack\n            let encOut = srcPE\n            let i = 0\n            while (i < this.encoderBlocks.length) {\n                encOut = this.encoderBlocks[i].forward(encOut)\n                i++\n            }\n            this.lastEncoderOut = encOut\n\n            // 3) Embed + positional encode target\n            let tgtEmb = this.embed.forward(tgtTokens)\n            let tgtPE = this.posEnc.forward(tgtEmb)\n\n            // 4) Pass through decoder stack (with cross-attention)\n            let decOut = tgtPE\n            i = 0\n            while (i < this.decoderBlocks.length) {\n                decOut = this.decoderBlocks[i].forward(decOut, encOut)\n                i++\n            }\n            this.lastDecoderOut = decOut\n\n            // 5) Final projection to vocab logits\n            let logits = this.finalLinear.forward(decOut)\n            return logits\n        }\n\n        backward(gradOut: Tensor): { dSrc: Tensor, dTgt: Tensor } {\n            // gradOut: dL/dLogits, [batch, tgtSeq, vocabSize]\n\n            // 1) Backprop through final linear projection\n            let dDec = this.finalLinear.backward(gradOut) // [batch, tgtSeq, embedDim]\n\n            // 2) Backprop through decoder stack (right-to-left)\n            let i = this.decoderBlocks.length - 1\n\n            // Accumulate gradients w.r.t. encoder output from all decoder blocks\n            let dEncAccum: Tensor = null\n\n            while (i >= 0) {\n                let block = this.decoderBlocks[i]\n\n                // block.backward returns gradients for decoder input and encoder output\n                let res = block.backward(dDec, this.lastEncoderOut)\n                let dDecIn = res.dX\n                let dEncFromBlock = res.dEnc\n\n                if (dEncAccum == null) {\n                    dEncAccum = dEncFromBlock\n                } else {\n                    let size = dEncAccum.data.length\n                    let j = 0\n                    while (j < size) {\n                        dEncAccum.data[j] += dEncFromBlock.data[j]\n                        j++\n                    }\n                }\n\n                dDec = dDecIn\n                i--\n            }\n\n            // 3) Backprop through encoder stack (right-to-left)\n            let dEnc = dEncAccum\n            i = this.encoderBlocks.length - 1\n            while (i >= 0) {\n                dEnc = this.encoderBlocks[i].backward(dEnc)\n                i--\n            }\n\n            // 4) Backprop through positional encoding (source path)\n            let dSrcPE = this.posEnc.backward(dEnc)\n\n            // 5) Backprop through embedding (source tokens)\n            let dSrc = this.embed.backward(dSrcPE)\n\n            // 6) Backprop through positional encoding (target path)\n            let dTgtPE = this.posEnc.backward(dDec)\n\n            // 7) Backprop through embedding (target tokens)\n            let dTgt = this.embed.backward(dTgtPE)\n\n            // Return both, even though gradients on token indices themselves\n            // are usually not used directly.\n            return {\n                dSrc: dSrc,\n                dTgt: dTgt\n            }\n        }\n    }\n}"}]},{"timestamp":1771472102525,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"tranformermodel.ts","patch":[{"start1":3020,"length1":0,"diffs":[[1,"            // gradOut: dL/dLogits, [batch, tgtSeq, vocabSize]\n\n"]]},{"start1":3143,"length1":141,"diffs":[[1,"            let dDec = this.finalLinear.backward(gradOut) // [batch, tgtSeq, embedDim]\n"]]},{"start1":3551,"length1":303,"diffs":[[1,"                // block.backward returns gradients for decoder input and encoder output\n                let res = block.backward(dDec, this.lastEncoderOut)\n                let dDecIn = res.dX\n                let dEncFromBlock = res.dEnc\n"]]},{"start1":3790,"length1":185,"diffs":[[1,""]]},{"start1":4963,"length1":0,"diffs":[[1,"            // Return both, even though gradients on token indices themselves\n            // are usually not used directly.\n"]]}]}]},{"timestamp":1771472550616,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"multiheadattention.ts","patch":[{"start1":1352,"length1":26,"diffs":[[1,""]]},{"start1":23770,"length1":14,"diffs":[[1,""]]}]},{"type":"edited","filename":"tranformerdecoder.ts","patch":[{"start1":549,"length1":28,"diffs":[[1,""]]},{"start1":4939,"length1":6,"diffs":[[1,""]]}]},{"type":"edited","filename":"tranformermodel.ts","patch":[{"start1":3079,"length1":244,"diffs":[[1,"            // finalLinear.backward returns a Tensor\n            let dDec = this.finalLinear.backward(gradOut)  // [batch, tgtSeq, embedDim]\n"]]},{"start1":3490,"length1":52,"diffs":[[1,"                let block = this.decoderBlocks[i]\n"]]},{"start1":3541,"length1":195,"diffs":[[1,"                // block.backward returns a Tensor for decoder input gradient\n                // BUT you also need encoder gradient, so your block must store it internally\n                // or expose a second method. Assuming backward returns only dX:\n                let dDecIn = block.backward(dDec)\n"]]},{"start1":3845,"length1":118,"diffs":[[1,"                // If your block produces encoder gradients, you must fetch them like:\n                let dEncFromBlock = block.lastDEnc  // <-- you must implement this in your block\n"]]},{"start1":4030,"length1":3,"diffs":[[1,""]]},{"start1":4142,"length1":55,"diffs":[[1,"                    let size = dEncAccum.data.length\n"]]},{"start1":4530,"length1":41,"diffs":[[1,"            let dEnc = dEncAccum\n"]]},{"start1":4802,"length1":62,"diffs":[[1,"            let dSrcPE = this.posEnc.backward(dEnc)\n"]]},{"start1":4916,"length1":61,"diffs":[[1,"            let dSrc = this.embed.backward(dSrcPE)\n"]]},{"start1":5037,"length1":62,"diffs":[[1,"            let dTgtPE = this.posEnc.backward(dDec)\n"]]},{"start1":5151,"length1":61,"diffs":[[1,"            let dTgt = this.embed.backward(dTgtPE)\n"]]}]}]},{"timestamp":1771473149570,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":177,"diffs":[[1,"let x = new TorchNew.Tensor([1, 2, 3], [1, 3])\r\nlet linear = new TorchNew.Linear(3, 2)\r\n"]]},{"start1":90,"length1":0,"diffs":[[1,"let y = linear.forward(x)\r\n"]]},{"start1":119,"length1":1321,"diffs":[[1,"console.log(\"Linear output:\")\r\nlet i = 0\r\nwhile (i < y.data.length) {\r\n    console.log(\"\" + y.data[i])\r\n    i++\r\n}"]]}]},{"type":"edited","filename":"multiheadattention.ts","patch":[{"start1":1352,"length1":46,"diffs":[[1,"        lastDEnc: Tensor\n"]]},{"start1":23796,"length1":58,"diffs":[[1,"            \n"]]},{"start1":23810,"length1":1,"diffs":[[1,""]]}]},{"type":"edited","filename":"tranformerdecoder.ts","patch":[{"start1":4967,"length1":321,"diffs":[[1,"    \n"]]},{"start1":4973,"length1":1,"diffs":[[1,""]]}]}]},{"timestamp":1771473704102,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":1108,"length1":156,"diffs":[[1,"    opt.step(model.parameters(),grads)\r\n"]]},{"start1":1150,"length1":4,"diffs":[[1,""]]}]}]},{"timestamp":1771524456638,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":865,"length1":54,"diffs":[[1,"        \"tranformermodel.ts\"\n"]]}]},{"type":"added","filename":"fasttensor.ts","value":""}]},{"timestamp":1771524927232,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"pxt.json","patch":[{"start1":895,"length1":76,"diffs":[[1,"        \"fasttensor.ts\"\n"]]}]},{"type":"edited","filename":"fasttensor.ts","patch":[{"start1":0,"length1":5406,"diffs":[[1,""]]}]},{"type":"added","filename":"FastLinear.ts","value":"namespace TorchNew {\r\n\r\n    export class FastLinear {\r\n        inFeatures: number\r\n        outFeatures: number\r\n        weight: TorchNew.FastTensor\r\n        bias: TorchNew.FastTensor\r\n\r\n        constructor(inFeatures: number, outFeatures: number) {\r\n            this.inFeatures = inFeatures\r\n            this.outFeatures = outFeatures\r\n\r\n            // Weight: [outFeatures][inFeatures]\r\n            let w: number[][] = []\r\n            for (let r = 0; r < outFeatures; r++) {\r\n                let row: number[] = []\r\n                for (let c = 0; c < inFeatures; c++) {\r\n                    // Xavier-like init\r\n                    row.push((Math.random() - 0.5) * 2 / Math.sqrt(inFeatures))\r\n                }\r\n                w.push(row)\r\n            }\r\n            this.weight = new TorchNew.FastTensor(w)\r\n\r\n            // Bias: [outFeatures][1] (or just a row vector)\r\n            let b: number[][] = []\r\n            let brow: number[] = []\r\n            for (let i = 0; i < outFeatures; i++) brow.push(0)\r\n            b.push(brow)\r\n            this.bias = new TorchNew.FastTensor(b)\r\n        }\r\n\r\n        // Forward: X (batch Ã— inFeatures) * W^T (inFeatures Ã— outFeatures) + b\r\n        forward(x: TorchNew.FastTensor): TorchNew.FastTensor {\r\n            // x.data: [batch][inFeatures]\r\n            // weight.data: [outFeatures][inFeatures]\r\n            // but matmul expects: A(rowsA Ã— colsA) * B(rowsB Ã— colsB)\r\n            // so we need weight^T: [inFeatures][outFeatures]\r\n\r\n            let Wt = this.transpose2D(this.weight.data)\r\n\r\n            // matmul: (batch Ã— inFeatures) * (inFeatures Ã— outFeatures)\r\n            let out = x.matmul(new TorchNew.FastTensor(Wt)) as TorchNew.FastTensor\r\n\r\n            // Add bias row-wise\r\n            let batch = out.data.length\r\n            let outF = this.outFeatures\r\n\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < outF; c++) {\r\n                    out.data[r][c] += this.bias.data[0][c]\r\n                }\r\n            }\r\n\r\n            return out\r\n        }\r\n\r\n        backward(dY: TorchNew.FastTensor, x: TorchNew.FastTensor) {\r\n            // dx = dY @ W\r\n            let dx = dY.matmul(this.weight) as TorchNew.FastTensor\r\n\r\n            // dW = dY^T @ x\r\n            let dYt = new TorchNew.FastTensor(this.transpose2D(dY.data))\r\n            let dW = dYt.matmul(x) as TorchNew.FastTensor\r\n\r\n            // db = sum over batch rows of dY\r\n            let dbRow: number[] = []\r\n            for (let i = 0; i < this.outFeatures; i++) dbRow.push(0)\r\n\r\n            let batch = dY.data.length\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < this.outFeatures; c++) {\r\n                    dbRow[c] += dY.data[r][c]\r\n                }\r\n            }\r\n\r\n            let db = new TorchNew.FastTensor([dbRow])\r\n\r\n            return {\r\n                dx: dx,\r\n                dW: dW,\r\n                db: db\r\n            }\r\n        }\r\n\r\n\r\n\r\n        // Simple 2D transpose helper\r\n        private transpose2D(m: number[][]): number[][] {\r\n            let rows = m.length\r\n            let cols = m[0].length\r\n            let out: number[][] = []\r\n\r\n            for (let c = 0; c < cols; c++) {\r\n                let row: number[] = []\r\n                for (let r = 0; r < rows; r++) {\r\n                    row.push(m[r][c])\r\n                }\r\n                out.push(row)\r\n            }\r\n            return out\r\n        }\r\n    }\r\n}"},{"type":"added","filename":"fastsequtial.ts","value":"namespace TorchNew {\n\n    export class FastSequential {\n        layers: any[]\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all fast layers\n        forward(x: FastTensor): FastTensor {\n            let out = x\n            for (let i = 0; i < this.layers.length; i++) {\n                out = this.layers[i].forward(out)\n            }\n            return out\n        }\n\n        // Backward pass (reverse order)\n        backward(grad: FastTensor, x: FastTensor): FastTensor {\n            // For FastSequential, we assume each layer stores its own input\n            // OR the user passes the correct x for the first layer.\n            let g = grad\n\n            // We need to track inputs for each layer\n            // So we require each layer to store lastInput during forward\n            for (let i = this.layers.length - 1; i >= 0; i--) {\n                let layer = this.layers[i]\n\n                if (!layer.lastInput) {\n                    // If a layer didn't store its input, we cannot backprop\n                    // Fast layers should ALWAYS store lastInput\n                    console.log(\"FastSequential WARNING: layer missing lastInput\")\n                    g = layer.backward(g)\n                } else {\n                    g = layer.backward(g, layer.lastInput)\n                }\n            }\n\n            return g\n        }\n\n        // Collect parameters from all fast layers\n        parameters(): FastTensor[] {\n            let params: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.weight) params.push(L.weight)\n                if (L.bias) params.push(L.bias)\n            }\n            return params\n        }\n\n        // Collect gradients from all fast layers\n        gradients(): FastTensor[] {\n            let grads: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.dW) grads.push(L.dW)\n                if (L.db) grads.push(L.db)\n            }\n            return grads\n        }\n    }\n}"}]},{"timestamp":1771525525017,"editorVersion":"4.0.5","changes":[{"type":"edited","filename":"main.ts","patch":[{"start1":0,"length1":0,"diffs":[[1,"// Build model using Sequential\r\nlet model = new TorchNew.Sequential([\r\n    new TorchNew.Linear(4, 8),\r\n    new TorchNew.Activations.ReLU(),\r\n    new TorchNew.Linear(8, 1)\r\n])\r\n\r\n\r\n// Adam optimizer for all parameters\r\nlet opt = new TorchNew.Adam(\r\n    0.001,   // learning rate\r\n    0.9,     // beta1\r\n    0.999,   // beta2\r\n    1e-8     // eps\r\n)\r\n\r\n// Training loop\r\nfor (let step = 0; step < 2000; step++) {\r\n\r\n    // ---- 1. Generate random training sample ----\r\n    let a = Math.randomRange(0, 10)\r\n    let b = Math.randomRange(0, 10)\r\n    let c = Math.randomRange(0, 10)\r\n    let d = Math.randomRange(0, 10)\r\n\r\n    let x = new TorchNew.Tensor([a, b, c, d], [1, 4])\r\n    let target = (a * b) + c - d\r\n    let yTrue = new TorchNew.Tensor([target], [1, 1])\r\n\r\n    // ---- 2. Forward ----\r\n    let yPred = model.forward(x)\r\n\r\n    // ---- 3. Loss (MSE) ----\r\n    let diff = yPred.data[0] - yTrue.data[0]\r\n    let loss = diff * diff\r\n\r\n    // dLoss/dPred\r\n    let dLoss = new TorchNew.Tensor([2 * diff], [1, 1])\r\n\r\n    // ---- 4. Backward ----\r\n    model.backward(dLoss)\r\n\r\n    // ---- 5. Adam update ----\r\n    opt.step(\r\n        model.parameters(),   // array of parameter tensors\r\n        model.gradients()     // array of gradient tensors (same order)\r\n    )\r\n\r\n\r\n\r\n    // Print occasionally\r\n    if (step % 200 == 0) {\r\n        console.log(\"step \" + step + \" loss=\" + loss)\r\n    }\r\n}\r\n\r\n// ---- Test the trained model ----\r\nlet test = new TorchNew.Tensor([3, 4, 2, 5], [1, 4])\r\nlet yTest = model.forward(test)\r\n\r\nconsole.log(\"Prediction for (3,4,2,5): \" + yTest.data[0])\r\nconsole.log(\"Expected: \" + ((3 * 4) + 2 - 5))"]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":945,"length1":98,"diffs":[[1,"        \"fastsequtial.ts\"\n"]]}]},{"type":"edited","filename":"FastLinear.ts","patch":[{"start1":184,"length1":31,"diffs":[[1,""]]},{"start1":1485,"length1":34,"diffs":[[1,""]]}]},{"type":"added","filename":"FastActivaons.ts","value":"namespace TorchNew.Activations {\n\n    export interface FastActivation {\n        forward(x: FastTensor): FastTensor\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class FastReLU implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            // Store input for backward\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : 0)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            return new FastTensor(\n                x.data.map((row, r) =>\n                    row.map((v, c) => v > 0 ? gradOut.data[r][c] : 0)\n                )\n            )\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class FastSigmoid implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => 1 / (1 + Math.exp(-v)))\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let s = 1 / (1 + Math.exp(-x.data[r][c]))\n                    row.push(gradOut.data[r][c] * s * (1 - s))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class FastTanh implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => {\n                let e2 = Math.exp(2 * v)\n                return (e2 - 1) / (e2 + 1)\n            })\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    let e2 = Math.exp(2 * v)\n                    let t = (e2 - 1) / (e2 + 1)\n                    row.push(gradOut.data[r][c] * (1 - t * t))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class FastLeakyReLU implements FastActivation {\n        alpha: number\n        lastInput: FastTensor\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : this.alpha * v)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    row.push(v > 0 ? gradOut.data[r][c] : this.alpha * gradOut.data[r][c])\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n    }\n}"},{"type":"added","filename":"tofast.ts","value":"namespace TorchNew {\n\n    export function ToFast(t: Tensor): FastTensor {\n        // Reject 0D tensors\n        if (t.shape.length === 0) {\n            console.log(\"ToFast ERROR: Cannot convert 0D tensor\")\n            return null\n        }\n\n        // If 1D: [N] â†’ [1][N]\n        if (t.shape.length === 1) {\n            let row: number[] = []\n            for (let i = 0; i < t.data.length; i++) {\n                row.push(t.data[i])\n            }\n            return new FastTensor([row])\n        }\n\n        // If 2D: [A, B] â†’ [A][B]\n        if (t.shape.length === 2) {\n            let rows = t.shape[0]\n            let cols = t.shape[1]\n            let out: number[][] = []\n            let idx = 0\n\n            for (let r = 0; r < rows; r++) {\n                let row: number[] = []\n                for (let c = 0; c < cols; c++) {\n                    row.push(t.data[idx])\n                    idx++\n                }\n                out.push(row)\n            }\n\n            return new FastTensor(out)\n        }\n\n        // ND case: flatten all dims except last\n        // shape [D1, D2, ..., Dk, F] â†’ [D1*D2*...*Dk][F]\n        let lastDim = t.shape[t.shape.length - 1]\n        let batch = 1\n\n        for (let i = 0; i < t.shape.length - 1; i++) {\n            batch *= t.shape[i]\n        }\n\n        let out: number[][] = []\n        let idx = 0\n\n        for (let r = 0; r < batch; r++) {\n            let row: number[] = []\n            for (let c = 0; c < lastDim; c++) {\n                row.push(t.data[idx])\n                idx++\n            }\n            out.push(row)\n        }\n\n        return new FastTensor(out)\n    }\n}"},{"type":"added","filename":"fromfast.ts","value":"namespace TorchNew {\n\n    // Convert FastTensor (2D) â†’ full Tensor (ND) with a given shape\n    export function FromFast(ft: FastTensor, shape: number[]): Tensor {\n        // Validate shape\n        if (shape.length === 0) {\n            console.log(\"FromFast ERROR: Cannot create 0D tensor\")\n            return null\n        }\n\n        // Flatten FastTensor.data (number[][]) into number[]\n        let flat: number[] = []\n        for (let r = 0; r < ft.data.length; r++) {\n            for (let c = 0; c < ft.data[0].length; c++) {\n                flat.push(ft.data[r][c])\n            }\n        }\n\n        // Validate total size\n        let expected = 1\n        for (let i = 0; i < shape.length; i++) {\n            expected *= shape[i]\n        }\n\n        if (expected !== flat.length) {\n            console.log(\"FromFast ERROR: Shape mismatch. Expected size \" + expected + \" but got \" + flat.length)\n            return null\n        }\n\n        // Create full Tensor\n        return new Tensor(flat, shape.slice(0))\n    }\n}"}]},{"timestamp":1771528296097,"editorVersion":"4.0.7","changes":[{"type":"edited","filename":"avgpoolnd.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]}]},{"timestamp":1771528320355,"editorVersion":"4.0.7","changes":[{"type":"edited","filename":"main.blocks","patch":[{"start1":0,"length1":84,"diffs":[[1,"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>"]]}]},{"type":"edited","filename":"pxt.json","patch":[{"start1":1050,"length1":44,"diffs":[[1,""]]}]},{"type":"edited","filename":"tensor.ts","patch":[{"start1":106,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"linear.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"cnn.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"adam.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"activactions.ts","patch":[{"start1":119,"length1":1,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"softmax.ts","patch":[{"start1":119,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"sequtial.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"embedding.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"flatten.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"layernorm.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"dropout.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"crossentropyloss.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"positalencoding.ts","patch":[{"start1":129,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"maxpoolnd.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"convtransposend.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"rnn.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"gru.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"lstm.ts","patch":[{"start1":106,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"residual.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"feedforward.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"multiheadattention.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"tranformerencoder.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"tranformerdecoder.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"edited","filename":"tranformermodel.ts","patch":[{"start1":107,"length1":0,"diffs":[[1,"        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n"]]}]},{"type":"added","filename":"test.ts","value":"// tests go here; this will not be compiled when this package is used as an extension.\n"}]}],"snapshots":[{"timestamp":1771458142142,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"http://www.w3.org/1999/xhtml\">\n  <variables></variables>\n  <block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block>\n</xml>","main.ts":" ","README.md":" ","assets.json":"","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\"\n    ],\n    \"additionalFilePaths\": []\n}\n"}},{"timestamp":1771468727252,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}},{"timestamp":1771470560560,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"let x = new TorchNew.Tensor([1, 2, 3], [1, 3])\r\nlet linear = new TorchNew.Linear(3, 2)\r\n\r\nlet y = linear.forward(x)\r\n\r\nconsole.log(\"Linear output:\")\r\nlet i = 0\r\nwhile (i < y.data.length) {\r\n    console.log(\"\" + y.data[i])\r\n    i++\r\n}","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","linear.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Linear {\n        inFeatures: number\n        outFeatures: number\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inFeatures: number, outFeatures: number) {\n            this.inFeatures = inFeatures\n            this.outFeatures = outFeatures\n\n            // Weight shape: [outFeatures, inFeatures]\n            let wsize = outFeatures * inFeatures\n            let wdata = alloc(wsize)\n\n            // Xavier-like init (simple version)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 / Math.sqrt(inFeatures)\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [outFeatures, inFeatures])\n\n            // Bias shape: [outFeatures]\n            let bdata = alloc(outFeatures)\n            i = 0\n            while (i < outFeatures) {\n                bdata[i] = 0\n                i++\n            }\n\n            this.bias = new Tensor(bdata, [outFeatures])\n        }\n\n        // Forward pass: x @ W^T + b\n        forward(x: Tensor): Tensor {\n            // x shape: [..., inFeatures]\n            // weight shape: [outFeatures, inFeatures]\n            // Need W^T shape: [inFeatures, outFeatures]\n\n            let Wt = this.weight.transpose(0, 1)\n\n            // Matmul: [..., inFeatures] Ã— [inFeatures, outFeatures]\n            let out = x.matmul(Wt)\n\n            // Add bias (broadcast over batch dims)\n            let size = out.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                out.data[i] += this.bias.data[idx]\n                i++\n            }\n\n            return out\n        }\n\n        backward(dY: Tensor, x: Tensor): { dx: Tensor, dW: Tensor, db: Tensor } {\n            // dY shape: [..., outFeatures]\n            // x shape:  [..., inFeatures]\n            // W shape:  [outFeatures, inFeatures]\n\n            // 1. dx = dY @ W\n            let dx = dY.matmul(this.weight)\n\n            // 2. dW = dY^T @ x\n            let dYt = dY.transpose(dY.shape.length - 2, dY.shape.length - 1)\n            let dW = dYt.matmul(x)\n\n            // 3. db = sum(dY over all batch dims)\n            let dbData = alloc(this.outFeatures)\n            let size = dY.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                dbData[idx] += dY.data[i]\n                i++\n            }\n\n            let db = new Tensor(dbData, [this.outFeatures])\n\n            return {\n                dx: dx,\n                dW: dW,\n                db: db\n            }\n        }\n    }\n}","cnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}","adam.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}","activactions.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","softmax.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","sequtial.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}","embedding.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}","avgpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","flatten.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}","layernorm.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","dropout.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","crossentropyloss.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","positalencoding.ts":"// Add your code here\nnamespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class PositionalEncoding {\n        maxLen: number\n        dim: number\n        encoding: Tensor\n\n        constructor(maxLen: number, dim: number) {\n            this.maxLen = maxLen\n            this.dim = dim\n\n            // Precompute positional encodings\n            let data = alloc(maxLen * dim)\n\n            let pos = 0\n            while (pos < maxLen) {\n                let i = 0\n                while (i < dim) {\n                    let angle = pos / Math.pow(10000, (2 * Math.idiv(i, 2)) / dim)\n\n                    if (i % 2 == 0) {\n                        data[pos * dim + i] = Math.sin(angle)\n                    } else {\n                        data[pos * dim + i] = Math.cos(angle)\n                    }\n\n                    i++\n                }\n                pos++\n            }\n\n            this.encoding = new Tensor(data, [maxLen, dim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward: add positional encoding to input\n        // x shape: [batch, seq, dim]\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let j = 0\n                while (j < seq) {\n                    let k = 0\n                    while (k < dim) {\n                        let idx = i * seq * dim + j * dim + k\n                        let pe = this.encoding.data[j * dim + k]\n                        outData[idx] = x.data[idx] + pe\n                        k++\n                    }\n                    j++\n                }\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: gradient passes through unchanged\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradData = alloc(gradOut.data.length)\n            let i = 0\n            while (i < gradOut.data.length) {\n                gradData[i] = gradOut.data[i]\n                i++\n            }\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\",\n        \"linear.ts\",\n        \"cnn.ts\",\n        \"adam.ts\",\n        \"activactions.ts\",\n        \"softmax.ts\",\n        \"sequtial.ts\",\n        \"embedding.ts\",\n        \"avgpoolnd.ts\",\n        \"flatten.ts\",\n        \"layernorm.ts\",\n        \"dropout.ts\",\n        \"crossentropyloss.ts\",\n        \"positalencoding.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}},{"timestamp":1771472550054,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"let x = new TorchNew.Tensor([1, 2, 3], [1, 3])\r\nlet linear = new TorchNew.Linear(3, 2)\r\n\r\nlet y = linear.forward(x)\r\n\r\nconsole.log(\"Linear output:\")\r\nlet i = 0\r\nwhile (i < y.data.length) {\r\n    console.log(\"\" + y.data[i])\r\n    i++\r\n}","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","linear.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Linear {\n        inFeatures: number\n        outFeatures: number\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inFeatures: number, outFeatures: number) {\n            this.inFeatures = inFeatures\n            this.outFeatures = outFeatures\n\n            // Weight shape: [outFeatures, inFeatures]\n            let wsize = outFeatures * inFeatures\n            let wdata = alloc(wsize)\n\n            // Xavier-like init (simple version)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 / Math.sqrt(inFeatures)\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [outFeatures, inFeatures])\n\n            // Bias shape: [outFeatures]\n            let bdata = alloc(outFeatures)\n            i = 0\n            while (i < outFeatures) {\n                bdata[i] = 0\n                i++\n            }\n\n            this.bias = new Tensor(bdata, [outFeatures])\n        }\n\n        // Forward pass: x @ W^T + b\n        forward(x: Tensor): Tensor {\n            // x shape: [..., inFeatures]\n            // weight shape: [outFeatures, inFeatures]\n            // Need W^T shape: [inFeatures, outFeatures]\n\n            let Wt = this.weight.transpose(0, 1)\n\n            // Matmul: [..., inFeatures] Ã— [inFeatures, outFeatures]\n            let out = x.matmul(Wt)\n\n            // Add bias (broadcast over batch dims)\n            let size = out.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                out.data[i] += this.bias.data[idx]\n                i++\n            }\n\n            return out\n        }\n\n        backward(dY: Tensor, x: Tensor): { dx: Tensor, dW: Tensor, db: Tensor } {\n            // dY shape: [..., outFeatures]\n            // x shape:  [..., inFeatures]\n            // W shape:  [outFeatures, inFeatures]\n\n            // 1. dx = dY @ W\n            let dx = dY.matmul(this.weight)\n\n            // 2. dW = dY^T @ x\n            let dYt = dY.transpose(dY.shape.length - 2, dY.shape.length - 1)\n            let dW = dYt.matmul(x)\n\n            // 3. db = sum(dY over all batch dims)\n            let dbData = alloc(this.outFeatures)\n            let size = dY.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                dbData[idx] += dY.data[i]\n                i++\n            }\n\n            let db = new Tensor(dbData, [this.outFeatures])\n\n            return {\n                dx: dx,\n                dW: dW,\n                db: db\n            }\n        }\n    }\n}","cnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}","adam.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}","activactions.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","softmax.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","sequtial.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}","embedding.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}","avgpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","flatten.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}","layernorm.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","dropout.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","crossentropyloss.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","positalencoding.ts":"// Add your code here\nnamespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class PositionalEncoding {\n        maxLen: number\n        dim: number\n        encoding: Tensor\n\n        constructor(maxLen: number, dim: number) {\n            this.maxLen = maxLen\n            this.dim = dim\n\n            // Precompute positional encodings\n            let data = alloc(maxLen * dim)\n\n            let pos = 0\n            while (pos < maxLen) {\n                let i = 0\n                while (i < dim) {\n                    let angle = pos / Math.pow(10000, (2 * Math.idiv(i, 2)) / dim)\n\n                    if (i % 2 == 0) {\n                        data[pos * dim + i] = Math.sin(angle)\n                    } else {\n                        data[pos * dim + i] = Math.cos(angle)\n                    }\n\n                    i++\n                }\n                pos++\n            }\n\n            this.encoding = new Tensor(data, [maxLen, dim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward: add positional encoding to input\n        // x shape: [batch, seq, dim]\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let j = 0\n                while (j < seq) {\n                    let k = 0\n                    while (k < dim) {\n                        let idx = i * seq * dim + j * dim + k\n                        let pe = this.encoding.data[j * dim + k]\n                        outData[idx] = x.data[idx] + pe\n                        k++\n                    }\n                    j++\n                }\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: gradient passes through unchanged\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradData = alloc(gradOut.data.length)\n            let i = 0\n            while (i < gradOut.data.length) {\n                gradData[i] = gradOut.data[i]\n                i++\n            }\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","maxpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class MaxPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        lastInput: Tensor\n        lastMaxIndex: number[]   // stores argmax positions for backward\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Store argmax indices for backward\n            this.lastMaxIndex = alloc(total)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                let maxVal = -1e30\n                let maxIndex = -1\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let flat = x.index(idxX)\n                            let v = x.data[flat]\n                            if (v > maxVal) {\n                                maxVal = v\n                                maxIndex = flat\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                outData[p] = maxVal\n                this.lastMaxIndex[p] = maxIndex\n\n                p++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                let idx = this.lastMaxIndex[p]\n                if (idx >= 0) {\n                    dXdata[idx] += gradOut.data[p]\n                }\n                p++\n            }\n\n            return dX\n        }\n    }\n}","convtransposend.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class ConvTransposeND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        weight: Tensor\n        bias: Tensor\n\n        dW: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], stride: number[], padding: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [inChannels, outChannels, ...kernelShape]\n            let wShape = alloc(2 + dims)\n            wShape[0] = inChannels\n            wShape[1] = outChannels\n            let i = 0\n            while (i < dims) {\n                wShape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            let wSize = 1\n            i = 0\n            while (i < wShape.length) {\n                wSize *= wShape[i]\n                i++\n            }\n\n            let wData = alloc(wSize)\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wSize) {\n                wData[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wData, wShape)\n            this.dW = new Tensor(alloc(wSize), wShape)\n\n            // Bias: [outChannels]\n            let bData = alloc(outChannels)\n            this.bias = new Tensor(bData, [outChannels])\n            this.dB = new Tensor(alloc(outChannels), [outChannels])\n        }\n\n        // ---------------------------------------------------------\n        // Forward (fractionally strided convolution)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let inC = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let s = this.stride[i]\n                let p = this.padding[i]\n\n                // Transposed conv output formula:\n                outSpatial[i] = (inSize - 1) * s - 2 * p + k\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Compute base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute output index\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= outSpatial[d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                // Weight index\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n\n                                let wVal = this.weight.get(idxW)\n                                let outIdx = out.index(idxOut)\n                                outData[outIdx] += x.data[p] * wVal\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            // Add bias\n            let q = 0\n            while (q < outData.length) {\n                let idx = Tensor.unravelIndex(q, outShape)\n                let oc = idx[1]\n                outData[q] += this.bias.data[oc]\n                q++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dW.data.length) {\n                this.dW.data[i] = 0\n                i++\n            }\n            i = 0\n            while (i < this.dB.data.length) {\n                this.dB.data[i] = 0\n                i++\n            }\n\n            // dB: sum over gradOut\n            let q = 0\n            while (q < gradOut.data.length) {\n                let idx = Tensor.unravelIndex(q, gradOut.shape)\n                let oc = idx[1]\n                this.dB.data[oc] += gradOut.data[q]\n                q++\n            }\n\n            // dX\n            let dXdata = alloc(x.data.length)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions for dW and dX\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= gradOut.shape[2 + d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                let go = gradOut.get(idxOut)\n\n                                // dW\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n                                let wIdx = this.weight.index(idxW)\n                                this.dW.data[wIdx] += x.data[p] * go\n\n                                // dX\n                                dXdata[p] += this.weight.data[wIdx] * go\n\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","rnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n    // tanh implementation (same as in activations.ts)\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class RNN {\n        inputDim: number\n        hiddenDim: number\n\n        Wxh: Tensor\n        Whh: Tensor\n        b: Tensor\n\n        dWxh: Tensor\n        dWhh: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n        lastH: number[]      // all hidden states (flattened)\n        lastA: number[]      // all pre-activations (Wxh x + Whh h + b)\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            // Weight shapes\n            this.Wxh = new Tensor(\n                this.initWeights(inputDim * hiddenDim),\n                [hiddenDim, inputDim]\n            )\n\n            this.Whh = new Tensor(\n                this.initWeights(hiddenDim * hiddenDim),\n                [hiddenDim, hiddenDim]\n            )\n\n            this.b = new Tensor(\n                alloc(hiddenDim),\n                [hiddenDim]\n            )\n\n            this.dWxh = new Tensor(alloc(inputDim * hiddenDim), [hiddenDim, inputDim])\n            this.dWhh = new Tensor(alloc(hiddenDim * hiddenDim), [hiddenDim, hiddenDim])\n            this.dB = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeights(size: number): number[] {\n            let arr = alloc(size)\n            let scale = 1 / Math.sqrt(size)\n            let i = 0\n            while (i < size) {\n                arr[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return arr\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            // Store hidden states and pre-activations\n            this.lastH = alloc(batch * (seq + 1) * hDim)  // includes h0 = 0\n            this.lastA = alloc(batch * seq * hDim)\n\n            // Initialize h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            // Forward through time\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    // Compute h_t\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.b.data[j]\n\n                        // Wxh * x_t\n                        let k = 0\n                        while (k < inDim) {\n                            let w = this.Wxh.data[j * inDim + k]\n                            let xv = x.data[baseX + k]\n                            sum += w * xv\n                            k++\n                        }\n\n                        // Whh * h_{t-1}\n                        k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[j * hDim + k]\n                            let hv = this.lastH[baseHprev + k]\n                            sum += w * hv\n                            k++\n                        }\n\n                        this.lastA[baseA + j] = sum\n                        this.lastH[baseHcur + j] = fastTanh(sum)\n\n                        // Output is h_t\n                        outData[(b * seq + t) * hDim + j] = this.lastH[baseHcur + j]\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWxh.data.length) { this.dWxh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWhh.data.length) { this.dWhh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dB.data.length) { this.dB.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)  // gradient from next time step\n\n            // BPTT\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    // dL/dh_t = gradOut + dHnext\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dA = dH * (1 - tanh(a)^2)\n                    let dA = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let h = this.lastH[baseHcur + j]\n                        dA[j] = dH[j] * (1 - h * h)\n                        j++\n                    }\n\n                    // Accumulate gradients\n                    j = 0\n                    while (j < hDim) {\n                        // dB\n                        this.dB.data[j] += dA[j]\n\n                        // dWxh\n                        let k = 0\n                        while (k < inDim) {\n                            let idx = j * inDim + k\n                            this.dWxh.data[idx] += dA[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        // dWhh\n                        k = 0\n                        while (k < hDim) {\n                            let idx = j * hDim + k\n                            this.dWhh.data[idx] += dA[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dX\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Wxh.data[k * inDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // dHnext = Whh^T * dA\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[k * hDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","gru.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class GRU {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wz: Tensor; Uz: Tensor; bz: Tensor\n        Wr: Tensor; Ur: Tensor; br: Tensor\n        Wh: Tensor; Uh: Tensor; bh: Tensor\n\n        // Gradients\n        dWz: Tensor; dUz: Tensor; dBz: Tensor\n        dWr: Tensor; dUr: Tensor; dBr: Tensor\n        dWh: Tensor; dUh: Tensor; dBh: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastZ: number[]\n        lastR: number[]\n        lastHtilde: number[]\n        lastH: number[]   // includes h0\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wz = this.initWeight(hiddenDim, inputDim)\n            this.Uz = this.initWeight(hiddenDim, hiddenDim)\n            this.bz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wr = this.initWeight(hiddenDim, inputDim)\n            this.Ur = this.initWeight(hiddenDim, hiddenDim)\n            this.br = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wh = this.initWeight(hiddenDim, inputDim)\n            this.Uh = this.initWeight(hiddenDim, hiddenDim)\n            this.bh = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWz = this.initZero(hiddenDim, inputDim)\n            this.dUz = this.initZero(hiddenDim, hiddenDim)\n            this.dBz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWr = this.initZero(hiddenDim, inputDim)\n            this.dUr = this.initZero(hiddenDim, hiddenDim)\n            this.dBr = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWh = this.initZero(hiddenDim, inputDim)\n            this.dUh = this.initZero(hiddenDim, hiddenDim)\n            this.dBh = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastZ = alloc(batch * seq * hDim)\n            this.lastR = alloc(batch * seq * hDim)\n            this.lastHtilde = alloc(batch * seq * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // ---- Compute z_t ----\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.bz.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wz.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Uz.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let z = sigmoid(sum)\n                        this.lastZ[baseZ + j] = z\n                        j++\n                    }\n\n                    // ---- Compute r_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.br.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wr.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Ur.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let r = sigmoid(sum)\n                        this.lastR[baseR + j] = r\n                        j++\n                    }\n\n                    // ---- Compute h~_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.bh.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wh.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            let r = this.lastR[baseR + k]\n                            sum += this.Uh.data[j * hDim + k] * (r * hprev)\n                            k++\n                        }\n\n                        let htilde = fastTanh(sum)\n                        this.lastHtilde[baseHtilde + j] = htilde\n                        j++\n                    }\n\n                    // ---- Final h_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        let h = (1 - z) * hprev + z * htilde\n                        this.lastH[baseHcur + j] = h\n\n                        outData[(b * seq + t) * hDim + j] = h\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWz.data.length) { this.dWz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUz.data.length) { this.dUz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBz.data.length) { this.dBz.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWr.data.length) { this.dWr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUr.data.length) { this.dUr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBr.data.length) { this.dBr.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWh.data.length) { this.dWh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUh.data.length) { this.dUh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBh.data.length) { this.dBh.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Compute gate derivatives\n                    let dZ = alloc(hDim)\n                    let dHtilde = alloc(hDim)\n                    let dHprev = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        dZ[j] = dH[j] * (htilde - hprev)\n                        dHtilde[j] = dH[j] * z\n                        dHprev[j] = dH[j] * (1 - z)\n                        j++\n                    }\n\n                    // dA_h = dHtilde * (1 - htilde^2)\n                    let dA_h = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let htilde = this.lastHtilde[baseHtilde + j]\n                        dA_h[j] = dHtilde[j] * (1 - htilde * htilde)\n                        j++\n                    }\n\n                    // dA_z = dZ * z * (1 - z)\n                    let dA_z = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        dA_z[j] = dZ[j] * z * (1 - z)\n                        j++\n                    }\n\n                    // dA_r = dHtilde * Uh * hprev * r*(1-r)\n                    let dA_r = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Uh.data[k * hDim + j]\n                            let d = dA_h[k]\n                            sum += d * w\n                            k++\n                        }\n                        let r = this.lastR[baseR + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        dA_r[j] = sum * hprev * r * (1 - r)\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWh, dUh, dBh\n                    j = 0\n                    while (j < hDim) {\n                        this.dBh.data[j] += dA_h[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWh.data[j * inDim + k] += dA_h[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let r = this.lastR[baseR + k]\n                            let hprev = this.lastH[baseHprev + k]\n                            this.dUh.data[j * hDim + k] += dA_h[j] * (r * hprev)\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWz, dUz, dBz\n                    j = 0\n                    while (j < hDim) {\n                        this.dBz.data[j] += dA_z[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWz.data[j * inDim + k] += dA_z[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUz.data[j * hDim + k] += dA_z[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWr, dUr, dBr\n                    j = 0\n                    while (j < hDim) {\n                        this.dBr.data[j] += dA_r[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWr.data[j * inDim + k] += dA_r[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUr.data[j * hDim + k] += dA_r[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Wz.data[k * inDim + j]\n                            sum += dA_r[k] * this.Wr.data[k * inDim + j]\n                            sum += dA_h[k] * this.Wh.data[k * inDim + j]\n                            k++\n                        }\n\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = dHprev[j]\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Uz.data[k * hDim + j]\n                            sum += dA_r[k] * this.Ur.data[k * hDim + j]\n                            sum += dA_h[k] * this.Uh.data[k * hDim + j] * this.lastR[baseR + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","lstm.ts":"namespace TorchNew {\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class LSTM {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wi: Tensor; Ui: Tensor; bi: Tensor\n        Wf: Tensor; Uf: Tensor; bf: Tensor\n        Wo: Tensor; Uo: Tensor; bo: Tensor\n        Wc: Tensor; Uc: Tensor; bc: Tensor\n\n        // Gradients\n        dWi: Tensor; dUi: Tensor; dBi: Tensor\n        dWf: Tensor; dUf: Tensor; dBf: Tensor\n        dWo: Tensor; dUo: Tensor; dBo: Tensor\n        dWc: Tensor; dUc: Tensor; dBc: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastI: number[]\n        lastF: number[]\n        lastO: number[]\n        lastCtilde: number[]\n        lastC: number[]\n        lastH: number[]\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wi = this.initWeight(hiddenDim, inputDim)\n            this.Ui = this.initWeight(hiddenDim, hiddenDim)\n            this.bi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wf = this.initWeight(hiddenDim, inputDim)\n            this.Uf = this.initWeight(hiddenDim, hiddenDim)\n            this.bf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wo = this.initWeight(hiddenDim, inputDim)\n            this.Uo = this.initWeight(hiddenDim, hiddenDim)\n            this.bo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wc = this.initWeight(hiddenDim, inputDim)\n            this.Uc = this.initWeight(hiddenDim, hiddenDim)\n            this.bc = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWi = this.initZero(hiddenDim, inputDim)\n            this.dUi = this.initZero(hiddenDim, hiddenDim)\n            this.dBi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWf = this.initZero(hiddenDim, inputDim)\n            this.dUf = this.initZero(hiddenDim, hiddenDim)\n            this.dBf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWo = this.initZero(hiddenDim, inputDim)\n            this.dUo = this.initZero(hiddenDim, hiddenDim)\n            this.dBo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWc = this.initZero(hiddenDim, inputDim)\n            this.dUc = this.initZero(hiddenDim, hiddenDim)\n            this.dBc = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastI = alloc(batch * seq * hDim)\n            this.lastF = alloc(batch * seq * hDim)\n            this.lastO = alloc(batch * seq * hDim)\n            this.lastCtilde = alloc(batch * seq * hDim)\n            this.lastC = alloc(batch * (seq + 1) * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // Initialize h0 = 0, c0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                this.lastC[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // ---- Compute gates ----\n                    let j = 0\n                    while (j < hDim) {\n                        // Input gate\n                        let sumI = this.bi.data[j]\n                        let sumF = this.bf.data[j]\n                        let sumO = this.bo.data[j]\n                        let sumC = this.bc.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            let xv = x.data[baseX + k]\n                            sumI += this.Wi.data[j * inDim + k] * xv\n                            sumF += this.Wf.data[j * inDim + k] * xv\n                            sumO += this.Wo.data[j * inDim + k] * xv\n                            sumC += this.Wc.data[j * inDim + k] * xv\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            sumI += this.Ui.data[j * hDim + k] * hprev\n                            sumF += this.Uf.data[j * hDim + k] * hprev\n                            sumO += this.Uo.data[j * hDim + k] * hprev\n                            sumC += this.Uc.data[j * hDim + k] * hprev\n                            k++\n                        }\n\n                        let iGate = sigmoid(sumI)\n                        let fGate = sigmoid(sumF)\n                        let oGate = sigmoid(sumO)\n                        let cTilde = fastTanh(sumC)\n\n                        this.lastI[baseI + j] = iGate\n                        this.lastF[baseF + j] = fGate\n                        this.lastO[baseO + j] = oGate\n                        this.lastCtilde[baseCtilde + j] = cTilde\n\n                        // Cell state\n                        let cprev = this.lastC[baseCprev + j]\n                        let ccur = fGate * cprev + iGate * cTilde\n                        this.lastC[baseCcur + j] = ccur\n\n                        // Hidden state\n                        let hcur = oGate * fastTanh(ccur)\n                        this.lastH[baseHcur + j] = hcur\n\n                        outData[(b * seq + t) * hDim + j] = hcur\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWi.data.length) { this.dWi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUi.data.length) { this.dUi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBi.data.length) { this.dBi.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWf.data.length) { this.dWf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUf.data.length) { this.dUf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBf.data.length) { this.dBf.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUo.data.length) { this.dUo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBo.data.length) { this.dBo.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWc.data.length) { this.dWc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUc.data.length) { this.dUc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBc.data.length) { this.dBc.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n            let dCnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dL/dc_t\n                    let dC = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let o = this.lastO[baseO + j]\n                        let ccur = this.lastC[baseCcur + j]\n                        dC[j] = dH[j] * o * (1 - fastTanh(ccur) * fastTanh(ccur)) + dCnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Gate derivatives\n                    let dI = alloc(hDim)\n                    let dF = alloc(hDim)\n                    let dO = alloc(hDim)\n                    let dCtilde = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let iGate = this.lastI[baseI + j]\n                        let fGate = this.lastF[baseF + j]\n                        let oGate = this.lastO[baseO + j]\n                        let cTilde = this.lastCtilde[baseCtilde + j]\n                        let cprev = this.lastC[baseCprev + j]\n\n                        dI[j] = dC[j] * cTilde * iGate * (1 - iGate)\n                        dF[j] = dC[j] * cprev * fGate * (1 - fGate)\n                        dO[j] = dH[j] * fastTanh(this.lastC[baseCcur + j]) * oGate * (1 - oGate)\n                        dCtilde[j] = dC[j] * iGate * (1 - cTilde * cTilde)\n\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWi, dUi, dBi\n                    j = 0\n                    while (j < hDim) {\n                        this.dBi.data[j] += dI[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWi.data[j * inDim + k] += dI[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUi.data[j * hDim + k] += dI[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWf, dUf, dBf\n                    j = 0\n                    while (j < hDim) {\n                        this.dBf.data[j] += dF[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWf.data[j * inDim + k] += dF[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUf.data[j * hDim + k] += dF[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWo, dUo, dBo\n                    j = 0\n                    while (j < hDim) {\n                        this.dBo.data[j] += dO[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWo.data[j * inDim + k] += dO[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUo.data[j * hDim + k] += dO[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWc, dUc, dBc\n                    j = 0\n                    while (j < hDim) {\n                        this.dBc.data[j] += dCtilde[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWc.data[j * inDim + k] += dCtilde[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUc.data[j * hDim + k] += dCtilde[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dI[k] * this.Wi.data[k * inDim + j]\n                            sum += dF[k] * this.Wf.data[k * inDim + j]\n                            sum += dO[k] * this.Wo.data[k * inDim + j]\n                            sum += dCtilde[k] * this.Wc.data[k * inDim + j]\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext and dCnext ----\n                    j = 0\n                    while (j < hDim) {\n                        // dCnext = dC * f_t\n                        let fGate = this.lastF[baseF + j]\n                        dCnext[b * hDim + j] = dC[j] * fGate\n\n                        // dHnext = contributions from all gates\n                        let sumH = 0\n\n                        // From input gate\n                        let k = 0\n                        while (k < hDim) {\n                            sumH += dI[k] * this.Ui.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From forget gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dF[k] * this.Uf.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From output gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dO[k] * this.Uo.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From candidate cell\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dCtilde[k] * this.Uc.data[k * hDim + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sumH\n\n                        j++\n                    }\n\n                    b++\n                    t--\n                }\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n\n}","residual.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Residual {\n        layer: any\n        lastInput: Tensor\n        lastLayerOut: Tensor\n\n        constructor(layer: any) {\n            this.layer = layer\n        }\n\n        // ---------------------------------------------------------\n        // Forward: y = x + layer(x)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let outLayer = this.layer.forward(x)\n            this.lastLayerOut = outLayer\n\n            let size = x.data.length\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                outData[i] = x.data[i] + outLayer.data[i]\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dX = gradOut + dLayer\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradLayer = this.layer.backward(gradOut)\n\n            let size = gradOut.data.length\n            let dXdata = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dXdata[i] = gradOut.data[i] + gradLayer.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","feedforward.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class FeedForward {\n        layer1: any\n        activation: any\n        layer2: any\n\n        lastInput: Tensor\n        lastHidden: Tensor\n\n        constructor(inputDim: number, hiddenDim: number, outputDim: number, activation: any) {\n            this.layer1 = new Linear(inputDim, hiddenDim)\n            this.activation = activation\n            this.layer2 = new Linear(hiddenDim, outputDim)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let h1 = this.layer1.forward(x)\n            let h2 = this.activation.forward(h1)\n            this.lastHidden = h2\n\n            let out = this.layer2.forward(h2)\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            // Backprop through second linear\n            let g2 = this.layer2.backward(gradOut)\n\n            // Backprop through activation\n            let gAct = this.activation.backward(g2)\n\n            // Backprop through first linear\n            let g1 = this.layer1.backward(gAct)\n\n            return g1\n        }\n    }\n}","multiheadattention.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function softmaxRow(data: number[], start: number, length: number): void {\n        let maxVal = data[start]\n        let i = 1\n        while (i < length) {\n            let v = data[start + i]\n            if (v > maxVal) maxVal = v\n            i++\n        }\n\n        let sum = 0\n        i = 0\n        while (i < length) {\n            let e = Math.exp(data[start + i] - maxVal)\n            data[start + i] = e\n            sum += e\n            i++\n        }\n\n        i = 0\n        while (i < length) {\n            data[start + i] /= sum\n            i++\n        }\n    }\n\n    export class MultiHeadAttention {\n        embedDim: number\n        numHeads: number\n        headDim: number\n\n        Wq: Tensor; bq: Tensor\n        Wk: Tensor; bk: Tensor\n        Wv: Tensor; bv: Tensor\n        Wo: Tensor; bo: Tensor\n\n        dWq: Tensor; dbq: Tensor\n        dWk: Tensor; dbk: Tensor\n        dWv: Tensor; dbv: Tensor\n        dWo: Tensor; dbo: Tensor\n\n        lastInput: Tensor\n        lastQ: Tensor\n        lastK: Tensor\n        lastV: Tensor\n        lastScores: number[]\n        lastSoftmax: number[]\n        lastAttention: number[]\n\n        constructor(embedDim: number, numHeads: number) {\n            this.embedDim = embedDim\n            this.numHeads = numHeads\n            this.headDim = Math.idiv(embedDim, numHeads)\n\n            this.Wq = new Linear(embedDim, embedDim).weight\n            this.bq = new Linear(embedDim, embedDim).bias\n\n            this.Wk = new Linear(embedDim, embedDim).weight\n            this.bk = new Linear(embedDim, embedDim).bias\n\n            this.Wv = new Linear(embedDim, embedDim).weight\n            this.bv = new Linear(embedDim, embedDim).bias\n\n            this.Wo = new Linear(embedDim, embedDim).weight\n            this.bo = new Linear(embedDim, embedDim).bias\n\n            this.dWq = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbq = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWk = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbk = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWv = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbv = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWo = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbo = new Tensor(alloc(embedDim), [embedDim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, embedDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(x, this.Wq, this.bq)\n            let K = this.linearForward(x, this.Wk, this.bk)\n            let V = this.linearForward(x, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores and softmax\n            let scores = alloc(batch * H * seq * seq)\n            let softmaxOut = alloc(batch * H * seq * seq)\n            let attention = alloc(batch * seq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            // Compute attention scores\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n\n                    let i = 0\n                    while (i < seq) {\n                        let j = 0\n                        while (j < seq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * seq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < seq) {\n                        softmaxRow(scores, sBase + i2 * seq, seq)\n                        let j2 = 0\n                        while (j2 < seq) {\n                            softmaxOut[sBase + i2 * seq + j2] = scores[sBase + i2 * seq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * seq * E\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < seq) {\n                                let w = softmaxOut[sBase + i3 * seq + j3]\n                                let vv = V.data[kBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, seq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection: attention -> gradAtt\n            let attTensor = new Tensor(this.lastAttention, [batch, seq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // 2) Allocate grads for Q, K, V and softmax/scores\n            let dQdata = alloc(batch * seq * E)\n            let dKdata = alloc(batch * seq * E)\n            let dVdata = alloc(batch * seq * E)\n\n            let dSoft = alloc(batch * H * seq * seq)\n            let dScores = alloc(batch * H * seq * seq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 3) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n                    let outBase = b * seq * E\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n                    let vBase = (b * seq * E) + h * D\n\n                    // dSoft and dV from gradAtt\n                    let i2 = 0\n                    while (i2 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < seq) {\n                                let w = this.lastSoftmax[sBase + i2 * seq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * seq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 4) Softmax backward: dScores from dSoft and probs\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dScores[sBase + i3 * seq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 5) Scores = (QK^T)/sqrt(D) -> dQ, dK\n                    let i4 = 0\n                    while (i4 < seq) {\n                        let j4 = 0\n                        while (j4 < seq) {\n                            let ds = dScores[sBase + i4 * seq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 6) Merge dQ, dK, dV back to [batch, seq, E] tensors\n            let dQ = new Tensor(dQdata, [batch, seq, E])\n            let dK = new Tensor(dKdata, [batch, seq, E])\n            let dV = new Tensor(dVdata, [batch, seq, E])\n\n            // 7) Back through Q, K, V linears to input x\n            let dXq = this.linearBackward(x, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(x, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(x, dV, this.Wv, this.dWv, this.dbv)\n\n            // 8) Sum contributions: dX = dXq + dXk + dXv\n            let dXdata = alloc(x.data.length)\n            i = 0\n            while (i < dXdata.length) {\n                dXdata[i] = dXq.data[i] + dXk.data[i] + dXv.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear forward\n        // ---------------------------------------------------------\n        linearForward(x: Tensor, W: Tensor, b: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = b.shape[0]\n\n            let outData = alloc(batch * seq * outDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseO = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let sum = b.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += W.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        outData[baseO + j] = sum\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(outData, [batch, seq, outDim])\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear backward (accumulates dW, db)\n        // ---------------------------------------------------------\n        linearBackward(x: Tensor, gradOut: Tensor, W: Tensor, dW: Tensor, db: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = gradOut.shape[2]\n\n            let gradInData = alloc(batch * seq * inDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseG = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let g = gradOut.data[baseG + j]\n\n                        db.data[j] += g\n\n                        let k = 0\n                        while (k < inDim) {\n                            dW.data[j * inDim + k] += g * x.data[baseX + k]\n                            gradInData[baseX + k] += g * W.data[j * inDim + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(gradInData, [batch, seq, inDim])\n        }\n\n        forwardKV(qInput: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            this.lastInput = qInput\n\n            let batch = qInput.shape[0]\n            let qSeq = qInput.shape[1]\n            let kvSeq = kInput.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(qInput, this.Wq, this.bq)\n            let K = this.linearForward(kInput, this.Wk, this.bk)\n            let V = this.linearForward(vInput, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores, softmax, attention\n            let scores = alloc(batch * H * qSeq * kvSeq)\n            let softmaxOut = alloc(batch * H * qSeq * kvSeq)\n            let attention = alloc(batch * qSeq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n\n                    // Compute scores = QK^T / sqrt(D)\n                    let i = 0\n                    while (i < qSeq) {\n                        let j = 0\n                        while (j < kvSeq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * kvSeq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        softmaxRow(scores, sBase + i2 * kvSeq, kvSeq)\n                        let j2 = 0\n                        while (j2 < kvSeq) {\n                            softmaxOut[sBase + i2 * kvSeq + j2] = scores[sBase + i2 * kvSeq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * qSeq * E\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < kvSeq) {\n                                let w = softmaxOut[sBase + i3 * kvSeq + j3]\n                                let vv = V.data[vBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, qSeq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        backwardKV(gradOut: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = Q.shape[0]\n            let qSeq = Q.shape[1]\n            let kvSeq = K.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection\n            let attTensor = new Tensor(this.lastAttention, [batch, qSeq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // Allocate grads for Q, K, V\n            let dQdata = alloc(batch * qSeq * E)\n            let dKdata = alloc(batch * kvSeq * E)\n            let dVdata = alloc(batch * kvSeq * E)\n\n            let dSoft = alloc(batch * H * qSeq * kvSeq)\n            let dScores = alloc(batch * H * qSeq * kvSeq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 2) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n                    let outBase = b * qSeq * E\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    // dSoft and dV\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < kvSeq) {\n                                let w = this.lastSoftmax[sBase + i2 * kvSeq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * kvSeq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 3) Softmax backward\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dScores[sBase + i3 * kvSeq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 4) Scores = QK^T / sqrt(D)\n                    let i4 = 0\n                    while (i4 < qSeq) {\n                        let j4 = 0\n                        while (j4 < kvSeq) {\n                            let ds = dScores[sBase + i4 * kvSeq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 5) Wrap dQ, dK, dV as tensors\n            let dQ = new Tensor(dQdata, [batch, qSeq, E])\n            let dK = new Tensor(dKdata, [batch, kvSeq, E])\n            let dV = new Tensor(dVdata, [batch, kvSeq, E])\n\n            // 6) Back through Q, K, V linears\n            let dXq = this.linearBackward(this.lastInput, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(kInput, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(vInput, dV, this.Wv, this.dWv, this.dbv)\n\n            // 7) Return gradients for decoder input (queries)\n            return dXq\n        }\n    }\n}","tranformerencoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerEncoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        mha: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterMHA: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            // LayerNorm expects an array of dims\n            this.ln1 = new LayerNorm([embedDim])\n            this.mha = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n\n            // FeedForward expects activation instance\n            this.ff = new FeedForward(\n                embedDim,\n                ffHiddenDim,\n                embedDim,\n                new TorchNew.Activations.ReLU()\n            )\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            // 1) LayerNorm â†’ MHA â†’ Residual\n            let xNorm1 = this.ln1.forward(x)\n            let mhaOut = this.mha.forward(xNorm1)\n\n            let size = x.data.length\n            let afterMHAdata = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterMHAdata[i] = x.data[i] + mhaOut.data[i]\n                i++\n            }\n            let afterMHA = new Tensor(afterMHAdata, x.shape.slice(0))\n            this.lastAfterMHA = afterMHA\n\n            // 2) LayerNorm â†’ FeedForward â†’ Residual\n            let xNorm2 = this.ln2.forward(afterMHA)\n            let ffOut = this.ff.forward(xNorm2)\n\n            let afterFFdata = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFdata[i] = afterMHA.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFdata, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // 1) Backprop through second residual: d(afterMHA) += gradOut\n            let dAfterMHAdata = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterMHAdata[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            // 2) Backprop through FeedForward\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n\n            // 3) Backprop through LayerNorm2\n            let dNorm2 = this.ln2.backward(dFF)\n\n            // Add to dAfterMHA\n            i = 0\n            while (i < size) {\n                dAfterMHAdata[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterMHA = new Tensor(dAfterMHAdata, gradOut.shape.slice(0))\n\n            // 4) Backprop through first residual: dX += dAfterMHA\n            let dXdata = alloc(size)\n            let dMHAout = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterMHA.data[i]\n                dMHAout[i] = dAfterMHA.data[i]\n                i++\n            }\n\n            // 5) Backprop through MHA\n            let dMHA = this.mha.backward(new Tensor(dMHAout, gradOut.shape.slice(0)))\n\n            // 6) Backprop through LayerNorm1\n            let dNorm1 = this.ln1.backward(dMHA)\n\n            // Add to dX\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","tranformerdecoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerDecoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        ln3: LayerNorm\n\n        selfAtt: MultiHeadAttention\n        crossAtt: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterSelf: Tensor\n        lastAfterCross: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm([embedDim])\n            this.selfAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n            this.crossAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln3 = new LayerNorm([embedDim])\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new TorchNew.Activations.ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor, encoderOut: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let embedDim = x.shape[2]\n\n            // -----------------------------\n            // 1) Masked Self-Attention\n            // -----------------------------\n            let xNorm1 = this.ln1.forward(x)\n\n            // Apply causal mask: disallow attending to future tokens\n            let masked = this.applyCausalMask(xNorm1)\n\n            let selfOut = this.selfAtt.forward(masked)\n\n            let size = x.data.length\n            let afterSelfData = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterSelfData[i] = x.data[i] + selfOut.data[i]\n                i++\n            }\n            let afterSelf = new Tensor(afterSelfData, x.shape.slice(0))\n            this.lastAfterSelf = afterSelf\n\n            // -----------------------------\n            // 2) Cross-Attention\n            // -----------------------------\n            let xNorm2 = this.ln2.forward(afterSelf)\n\n            // Cross-attention: query = decoder, key/value = encoder\n            let crossOut = this.crossAtt.forwardKV(xNorm2, encoderOut, encoderOut)\n\n            let afterCrossData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterCrossData[i] = afterSelf.data[i] + crossOut.data[i]\n                i++\n            }\n            let afterCross = new Tensor(afterCrossData, x.shape.slice(0))\n            this.lastAfterCross = afterCross\n\n            // -----------------------------\n            // 3) FeedForward\n            // -----------------------------\n            let xNorm3 = this.ln3.forward(afterCross)\n            let ffOut = this.ff.forward(xNorm3)\n\n            let afterFFData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFData[i] = afterCross.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFData, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor, encoderOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // -----------------------------\n            // 1) Backprop through FF residual\n            // -----------------------------\n            let dAfterCrossData = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterCrossData[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n            let dNorm3 = this.ln3.backward(dFF)\n\n            i = 0\n            while (i < size) {\n                dAfterCrossData[i] += dNorm3.data[i]\n                i++\n            }\n\n            let dAfterCross = new Tensor(dAfterCrossData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 2) Backprop through cross-attention residual\n            // -----------------------------\n            let dAfterSelfData = alloc(size)\n            let dCrossOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] = dAfterCross.data[i]\n                dCrossOut[i] = dAfterCross.data[i]\n                i++\n            }\n\n            let dCross = this.crossAtt.backwardKV(\n                new Tensor(dCrossOut, gradOut.shape.slice(0)),\n                encoderOut,\n                encoderOut\n            )\n\n            let dNorm2 = this.ln2.backward(dCross)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterSelf = new Tensor(dAfterSelfData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 3) Backprop through self-attention residual\n            // -----------------------------\n            let dXdata = alloc(size)\n            let dSelfOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterSelf.data[i]\n                dSelfOut[i] = dAfterSelf.data[i]\n                i++\n            }\n\n            let dSelf = this.selfAtt.backward(new Tensor(dSelfOut, gradOut.shape.slice(0)))\n            let dNorm1 = this.ln1.backward(dSelf)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Causal mask: zero out future positions\n        // ---------------------------------------------------------\n        applyCausalMask(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let b = 0\n            while (b < batch) {\n                let i = 0\n                while (i < seq) {\n                    let j = 0\n                    while (j < seq) {\n                        let k = 0\n                        while (k < dim) {\n                            let idx = b * seq * dim + i * dim + k\n                            let src = b * seq * dim + j * dim + k\n\n                            if (j <= i) {\n                                outData[idx] = x.data[src]\n                            } else {\n                                outData[idx] = 0\n                            }\n\n                            k++\n                        }\n                        j++\n                    }\n                    i++\n                }\n                b++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","tranformermodel.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerModel {\n        embed: Embedding\n        posEnc: PositionalEncoding\n\n        encoderBlocks: TransformerEncoder[]\n        decoderBlocks: TransformerDecoder[]\n\n        finalLinear: Linear\n\n        lastInput: Tensor\n        lastEncoderOut: Tensor\n        lastDecoderOut: Tensor\n\n        constructor(\n            vocabSize: number,\n            embedDim: number,\n            numHeads: number,\n            ffHiddenDim: number,\n            numEncoderLayers: number,\n            numDecoderLayers: number,\n            maxSeqLen: number\n        ) {\n            this.embed = new Embedding(vocabSize, embedDim)\n            this.posEnc = new PositionalEncoding(maxSeqLen, embedDim)\n\n            this.encoderBlocks = []\n            let i = 0\n            while (i < numEncoderLayers) {\n                this.encoderBlocks.push(\n                    new TransformerEncoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.decoderBlocks = []\n            i = 0\n            while (i < numDecoderLayers) {\n                this.decoderBlocks.push(\n                    new TransformerDecoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.finalLinear = new Linear(embedDim, vocabSize)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(srcTokens: Tensor, tgtTokens: Tensor): Tensor {\n            // srcTokens: [batch, srcSeq]\n            // tgtTokens: [batch, tgtSeq]\n\n            this.lastInput = srcTokens\n\n            // 1) Embed + positional encode source\n            let srcEmb = this.embed.forward(srcTokens)\n            let srcPE = this.posEnc.forward(srcEmb)\n\n            // 2) Pass through encoder stack\n            let encOut = srcPE\n            let i = 0\n            while (i < this.encoderBlocks.length) {\n                encOut = this.encoderBlocks[i].forward(encOut)\n                i++\n            }\n            this.lastEncoderOut = encOut\n\n            // 3) Embed + positional encode target\n            let tgtEmb = this.embed.forward(tgtTokens)\n            let tgtPE = this.posEnc.forward(tgtEmb)\n\n            // 4) Pass through decoder stack (with cross-attention)\n            let decOut = tgtPE\n            i = 0\n            while (i < this.decoderBlocks.length) {\n                decOut = this.decoderBlocks[i].forward(decOut, encOut)\n                i++\n            }\n            this.lastDecoderOut = decOut\n\n            // 5) Final projection to vocab logits\n            let logits = this.finalLinear.forward(decOut)\n            return logits\n        }\n\n        backward(gradOut: Tensor): { dSrc: Tensor, dTgt: Tensor } {\n            // gradOut: dL/dLogits, [batch, tgtSeq, vocabSize]\n\n            // 1) Backprop through final linear projection\n            let dDec = this.finalLinear.backward(gradOut) // [batch, tgtSeq, embedDim]\n\n            // 2) Backprop through decoder stack (right-to-left)\n            let i = this.decoderBlocks.length - 1\n\n            // Accumulate gradients w.r.t. encoder output from all decoder blocks\n            let dEncAccum: Tensor = null\n\n            while (i >= 0) {\n                let block = this.decoderBlocks[i]\n\n                // block.backward returns gradients for decoder input and encoder output\n                let res = block.backward(dDec, this.lastEncoderOut)\n                let dDecIn = res.dX\n                let dEncFromBlock = res.dEnc\n\n                if (dEncAccum == null) {\n                    dEncAccum = dEncFromBlock\n                } else {\n                    let size = dEncAccum.data.length\n                    let j = 0\n                    while (j < size) {\n                        dEncAccum.data[j] += dEncFromBlock.data[j]\n                        j++\n                    }\n                }\n\n                dDec = dDecIn\n                i--\n            }\n\n            // 3) Backprop through encoder stack (right-to-left)\n            let dEnc = dEncAccum\n            i = this.encoderBlocks.length - 1\n            while (i >= 0) {\n                dEnc = this.encoderBlocks[i].backward(dEnc)\n                i--\n            }\n\n            // 4) Backprop through positional encoding (source path)\n            let dSrcPE = this.posEnc.backward(dEnc)\n\n            // 5) Backprop through embedding (source tokens)\n            let dSrc = this.embed.backward(dSrcPE)\n\n            // 6) Backprop through positional encoding (target path)\n            let dTgtPE = this.posEnc.backward(dDec)\n\n            // 7) Backprop through embedding (target tokens)\n            let dTgt = this.embed.backward(dTgtPE)\n\n            // Return both, even though gradients on token indices themselves\n            // are usually not used directly.\n            return {\n                dSrc: dSrc,\n                dTgt: dTgt\n            }\n        }\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\",\n        \"linear.ts\",\n        \"cnn.ts\",\n        \"adam.ts\",\n        \"activactions.ts\",\n        \"softmax.ts\",\n        \"sequtial.ts\",\n        \"embedding.ts\",\n        \"avgpoolnd.ts\",\n        \"flatten.ts\",\n        \"layernorm.ts\",\n        \"dropout.ts\",\n        \"crossentropyloss.ts\",\n        \"positalencoding.ts\",\n        \"maxpoolnd.ts\",\n        \"convtransposend.ts\",\n        \"rnn.ts\",\n        \"gru.ts\",\n        \"lstm.ts\",\n        \"residual.ts\",\n        \"feedforward.ts\",\n        \"multiheadattention.ts\",\n        \"tranformerencoder.ts\",\n        \"tranformerdecoder.ts\",\n        \"tranformermodel.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}},{"timestamp":1771524352861,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"// Build model using Sequential\r\nlet model = new TorchNew.Sequential([\r\n    new TorchNew.Linear(4, 8),\r\n    new TorchNew.Activations.ReLU(),\r\n    new TorchNew.Linear(8, 1)\r\n])\r\n\r\n\r\n// Adam optimizer for all parameters\r\nlet opt = new TorchNew.Adam(\r\n    0.001,   // learning rate\r\n    0.9,     // beta1\r\n    0.999,   // beta2\r\n    1e-8     // eps\r\n)\r\n\r\n// Training loop\r\nfor (let step = 0; step < 2000; step++) {\r\n\r\n    // ---- 1. Generate random training sample ----\r\n    let a = Math.randomRange(0, 10)\r\n    let b = Math.randomRange(0, 10)\r\n    let c = Math.randomRange(0, 10)\r\n    let d = Math.randomRange(0, 10)\r\n\r\n    let x = new TorchNew.Tensor([a, b, c, d], [1, 4])\r\n    let target = (a * b) + c - d\r\n    let yTrue = new TorchNew.Tensor([target], [1, 1])\r\n\r\n    // ---- 2. Forward ----\r\n    let yPred = model.forward(x)\r\n\r\n    // ---- 3. Loss (MSE) ----\r\n    let diff = yPred.data[0] - yTrue.data[0]\r\n    let loss = diff * diff\r\n\r\n    // dLoss/dPred\r\n    let dLoss = new TorchNew.Tensor([2 * diff], [1, 1])\r\n\r\n    // ---- 4. Backward ----\r\n    model.backward(dLoss)\r\n\r\n    // ---- 5. Adam update ----\r\n    opt.step(\r\n        model.parameters(),   // array of parameter tensors\r\n        model.gradients()     // array of gradient tensors (same order)\r\n    )\r\n\r\n\r\n\r\n    // Print occasionally\r\n    if (step % 200 == 0) {\r\n        console.log(\"step \" + step + \" loss=\" + loss)\r\n    }\r\n}\r\n\r\n// ---- Test the trained model ----\r\nlet test = new TorchNew.Tensor([3, 4, 2, 5], [1, 4])\r\nlet yTest = model.forward(test)\r\n\r\nconsole.log(\"Prediction for (3,4,2,5): \" + yTest.data[0])\r\nconsole.log(\"Expected: \" + ((3 * 4) + 2 - 5))","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","linear.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Linear {\n        inFeatures: number\n        outFeatures: number\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inFeatures: number, outFeatures: number) {\n            this.inFeatures = inFeatures\n            this.outFeatures = outFeatures\n\n            // Weight shape: [outFeatures, inFeatures]\n            let wsize = outFeatures * inFeatures\n            let wdata = alloc(wsize)\n\n            // Xavier-like init (simple version)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 / Math.sqrt(inFeatures)\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [outFeatures, inFeatures])\n\n            // Bias shape: [outFeatures]\n            let bdata = alloc(outFeatures)\n            i = 0\n            while (i < outFeatures) {\n                bdata[i] = 0\n                i++\n            }\n\n            this.bias = new Tensor(bdata, [outFeatures])\n        }\n\n        // Forward pass: x @ W^T + b\n        forward(x: Tensor): Tensor {\n            // x shape: [..., inFeatures]\n            // weight shape: [outFeatures, inFeatures]\n            // Need W^T shape: [inFeatures, outFeatures]\n\n            let Wt = this.weight.transpose(0, 1)\n\n            // Matmul: [..., inFeatures] Ã— [inFeatures, outFeatures]\n            let out = x.matmul(Wt)\n\n            // Add bias (broadcast over batch dims)\n            let size = out.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                out.data[i] += this.bias.data[idx]\n                i++\n            }\n\n            return out\n        }\n\n        backward(dY: Tensor, x: Tensor): { dx: Tensor, dW: Tensor, db: Tensor } {\n            // dY shape: [..., outFeatures]\n            // x shape:  [..., inFeatures]\n            // W shape:  [outFeatures, inFeatures]\n\n            // 1. dx = dY @ W\n            let dx = dY.matmul(this.weight)\n\n            // 2. dW = dY^T @ x\n            let dYt = dY.transpose(dY.shape.length - 2, dY.shape.length - 1)\n            let dW = dYt.matmul(x)\n\n            // 3. db = sum(dY over all batch dims)\n            let dbData = alloc(this.outFeatures)\n            let size = dY.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                dbData[idx] += dY.data[i]\n                i++\n            }\n\n            let db = new Tensor(dbData, [this.outFeatures])\n\n            return {\n                dx: dx,\n                dW: dW,\n                db: db\n            }\n        }\n    }\n}","cnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}","adam.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}","activactions.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","softmax.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","sequtial.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}","embedding.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}","avgpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","flatten.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}","layernorm.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","dropout.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","crossentropyloss.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","positalencoding.ts":"// Add your code here\nnamespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class PositionalEncoding {\n        maxLen: number\n        dim: number\n        encoding: Tensor\n\n        constructor(maxLen: number, dim: number) {\n            this.maxLen = maxLen\n            this.dim = dim\n\n            // Precompute positional encodings\n            let data = alloc(maxLen * dim)\n\n            let pos = 0\n            while (pos < maxLen) {\n                let i = 0\n                while (i < dim) {\n                    let angle = pos / Math.pow(10000, (2 * Math.idiv(i, 2)) / dim)\n\n                    if (i % 2 == 0) {\n                        data[pos * dim + i] = Math.sin(angle)\n                    } else {\n                        data[pos * dim + i] = Math.cos(angle)\n                    }\n\n                    i++\n                }\n                pos++\n            }\n\n            this.encoding = new Tensor(data, [maxLen, dim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward: add positional encoding to input\n        // x shape: [batch, seq, dim]\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let j = 0\n                while (j < seq) {\n                    let k = 0\n                    while (k < dim) {\n                        let idx = i * seq * dim + j * dim + k\n                        let pe = this.encoding.data[j * dim + k]\n                        outData[idx] = x.data[idx] + pe\n                        k++\n                    }\n                    j++\n                }\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: gradient passes through unchanged\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradData = alloc(gradOut.data.length)\n            let i = 0\n            while (i < gradOut.data.length) {\n                gradData[i] = gradOut.data[i]\n                i++\n            }\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","maxpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class MaxPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        lastInput: Tensor\n        lastMaxIndex: number[]   // stores argmax positions for backward\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Store argmax indices for backward\n            this.lastMaxIndex = alloc(total)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                let maxVal = -1e30\n                let maxIndex = -1\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let flat = x.index(idxX)\n                            let v = x.data[flat]\n                            if (v > maxVal) {\n                                maxVal = v\n                                maxIndex = flat\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                outData[p] = maxVal\n                this.lastMaxIndex[p] = maxIndex\n\n                p++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                let idx = this.lastMaxIndex[p]\n                if (idx >= 0) {\n                    dXdata[idx] += gradOut.data[p]\n                }\n                p++\n            }\n\n            return dX\n        }\n    }\n}","convtransposend.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class ConvTransposeND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        weight: Tensor\n        bias: Tensor\n\n        dW: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], stride: number[], padding: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [inChannels, outChannels, ...kernelShape]\n            let wShape = alloc(2 + dims)\n            wShape[0] = inChannels\n            wShape[1] = outChannels\n            let i = 0\n            while (i < dims) {\n                wShape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            let wSize = 1\n            i = 0\n            while (i < wShape.length) {\n                wSize *= wShape[i]\n                i++\n            }\n\n            let wData = alloc(wSize)\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wSize) {\n                wData[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wData, wShape)\n            this.dW = new Tensor(alloc(wSize), wShape)\n\n            // Bias: [outChannels]\n            let bData = alloc(outChannels)\n            this.bias = new Tensor(bData, [outChannels])\n            this.dB = new Tensor(alloc(outChannels), [outChannels])\n        }\n\n        // ---------------------------------------------------------\n        // Forward (fractionally strided convolution)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let inC = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let s = this.stride[i]\n                let p = this.padding[i]\n\n                // Transposed conv output formula:\n                outSpatial[i] = (inSize - 1) * s - 2 * p + k\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Compute base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute output index\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= outSpatial[d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                // Weight index\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n\n                                let wVal = this.weight.get(idxW)\n                                let outIdx = out.index(idxOut)\n                                outData[outIdx] += x.data[p] * wVal\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            // Add bias\n            let q = 0\n            while (q < outData.length) {\n                let idx = Tensor.unravelIndex(q, outShape)\n                let oc = idx[1]\n                outData[q] += this.bias.data[oc]\n                q++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dW.data.length) {\n                this.dW.data[i] = 0\n                i++\n            }\n            i = 0\n            while (i < this.dB.data.length) {\n                this.dB.data[i] = 0\n                i++\n            }\n\n            // dB: sum over gradOut\n            let q = 0\n            while (q < gradOut.data.length) {\n                let idx = Tensor.unravelIndex(q, gradOut.shape)\n                let oc = idx[1]\n                this.dB.data[oc] += gradOut.data[q]\n                q++\n            }\n\n            // dX\n            let dXdata = alloc(x.data.length)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions for dW and dX\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= gradOut.shape[2 + d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                let go = gradOut.get(idxOut)\n\n                                // dW\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n                                let wIdx = this.weight.index(idxW)\n                                this.dW.data[wIdx] += x.data[p] * go\n\n                                // dX\n                                dXdata[p] += this.weight.data[wIdx] * go\n\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","rnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n    // tanh implementation (same as in activations.ts)\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class RNN {\n        inputDim: number\n        hiddenDim: number\n\n        Wxh: Tensor\n        Whh: Tensor\n        b: Tensor\n\n        dWxh: Tensor\n        dWhh: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n        lastH: number[]      // all hidden states (flattened)\n        lastA: number[]      // all pre-activations (Wxh x + Whh h + b)\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            // Weight shapes\n            this.Wxh = new Tensor(\n                this.initWeights(inputDim * hiddenDim),\n                [hiddenDim, inputDim]\n            )\n\n            this.Whh = new Tensor(\n                this.initWeights(hiddenDim * hiddenDim),\n                [hiddenDim, hiddenDim]\n            )\n\n            this.b = new Tensor(\n                alloc(hiddenDim),\n                [hiddenDim]\n            )\n\n            this.dWxh = new Tensor(alloc(inputDim * hiddenDim), [hiddenDim, inputDim])\n            this.dWhh = new Tensor(alloc(hiddenDim * hiddenDim), [hiddenDim, hiddenDim])\n            this.dB = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeights(size: number): number[] {\n            let arr = alloc(size)\n            let scale = 1 / Math.sqrt(size)\n            let i = 0\n            while (i < size) {\n                arr[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return arr\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            // Store hidden states and pre-activations\n            this.lastH = alloc(batch * (seq + 1) * hDim)  // includes h0 = 0\n            this.lastA = alloc(batch * seq * hDim)\n\n            // Initialize h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            // Forward through time\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    // Compute h_t\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.b.data[j]\n\n                        // Wxh * x_t\n                        let k = 0\n                        while (k < inDim) {\n                            let w = this.Wxh.data[j * inDim + k]\n                            let xv = x.data[baseX + k]\n                            sum += w * xv\n                            k++\n                        }\n\n                        // Whh * h_{t-1}\n                        k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[j * hDim + k]\n                            let hv = this.lastH[baseHprev + k]\n                            sum += w * hv\n                            k++\n                        }\n\n                        this.lastA[baseA + j] = sum\n                        this.lastH[baseHcur + j] = fastTanh(sum)\n\n                        // Output is h_t\n                        outData[(b * seq + t) * hDim + j] = this.lastH[baseHcur + j]\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWxh.data.length) { this.dWxh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWhh.data.length) { this.dWhh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dB.data.length) { this.dB.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)  // gradient from next time step\n\n            // BPTT\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    // dL/dh_t = gradOut + dHnext\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dA = dH * (1 - tanh(a)^2)\n                    let dA = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let h = this.lastH[baseHcur + j]\n                        dA[j] = dH[j] * (1 - h * h)\n                        j++\n                    }\n\n                    // Accumulate gradients\n                    j = 0\n                    while (j < hDim) {\n                        // dB\n                        this.dB.data[j] += dA[j]\n\n                        // dWxh\n                        let k = 0\n                        while (k < inDim) {\n                            let idx = j * inDim + k\n                            this.dWxh.data[idx] += dA[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        // dWhh\n                        k = 0\n                        while (k < hDim) {\n                            let idx = j * hDim + k\n                            this.dWhh.data[idx] += dA[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dX\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Wxh.data[k * inDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // dHnext = Whh^T * dA\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[k * hDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","gru.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class GRU {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wz: Tensor; Uz: Tensor; bz: Tensor\n        Wr: Tensor; Ur: Tensor; br: Tensor\n        Wh: Tensor; Uh: Tensor; bh: Tensor\n\n        // Gradients\n        dWz: Tensor; dUz: Tensor; dBz: Tensor\n        dWr: Tensor; dUr: Tensor; dBr: Tensor\n        dWh: Tensor; dUh: Tensor; dBh: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastZ: number[]\n        lastR: number[]\n        lastHtilde: number[]\n        lastH: number[]   // includes h0\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wz = this.initWeight(hiddenDim, inputDim)\n            this.Uz = this.initWeight(hiddenDim, hiddenDim)\n            this.bz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wr = this.initWeight(hiddenDim, inputDim)\n            this.Ur = this.initWeight(hiddenDim, hiddenDim)\n            this.br = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wh = this.initWeight(hiddenDim, inputDim)\n            this.Uh = this.initWeight(hiddenDim, hiddenDim)\n            this.bh = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWz = this.initZero(hiddenDim, inputDim)\n            this.dUz = this.initZero(hiddenDim, hiddenDim)\n            this.dBz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWr = this.initZero(hiddenDim, inputDim)\n            this.dUr = this.initZero(hiddenDim, hiddenDim)\n            this.dBr = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWh = this.initZero(hiddenDim, inputDim)\n            this.dUh = this.initZero(hiddenDim, hiddenDim)\n            this.dBh = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastZ = alloc(batch * seq * hDim)\n            this.lastR = alloc(batch * seq * hDim)\n            this.lastHtilde = alloc(batch * seq * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // ---- Compute z_t ----\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.bz.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wz.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Uz.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let z = sigmoid(sum)\n                        this.lastZ[baseZ + j] = z\n                        j++\n                    }\n\n                    // ---- Compute r_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.br.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wr.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Ur.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let r = sigmoid(sum)\n                        this.lastR[baseR + j] = r\n                        j++\n                    }\n\n                    // ---- Compute h~_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.bh.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wh.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            let r = this.lastR[baseR + k]\n                            sum += this.Uh.data[j * hDim + k] * (r * hprev)\n                            k++\n                        }\n\n                        let htilde = fastTanh(sum)\n                        this.lastHtilde[baseHtilde + j] = htilde\n                        j++\n                    }\n\n                    // ---- Final h_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        let h = (1 - z) * hprev + z * htilde\n                        this.lastH[baseHcur + j] = h\n\n                        outData[(b * seq + t) * hDim + j] = h\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWz.data.length) { this.dWz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUz.data.length) { this.dUz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBz.data.length) { this.dBz.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWr.data.length) { this.dWr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUr.data.length) { this.dUr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBr.data.length) { this.dBr.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWh.data.length) { this.dWh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUh.data.length) { this.dUh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBh.data.length) { this.dBh.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Compute gate derivatives\n                    let dZ = alloc(hDim)\n                    let dHtilde = alloc(hDim)\n                    let dHprev = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        dZ[j] = dH[j] * (htilde - hprev)\n                        dHtilde[j] = dH[j] * z\n                        dHprev[j] = dH[j] * (1 - z)\n                        j++\n                    }\n\n                    // dA_h = dHtilde * (1 - htilde^2)\n                    let dA_h = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let htilde = this.lastHtilde[baseHtilde + j]\n                        dA_h[j] = dHtilde[j] * (1 - htilde * htilde)\n                        j++\n                    }\n\n                    // dA_z = dZ * z * (1 - z)\n                    let dA_z = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        dA_z[j] = dZ[j] * z * (1 - z)\n                        j++\n                    }\n\n                    // dA_r = dHtilde * Uh * hprev * r*(1-r)\n                    let dA_r = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Uh.data[k * hDim + j]\n                            let d = dA_h[k]\n                            sum += d * w\n                            k++\n                        }\n                        let r = this.lastR[baseR + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        dA_r[j] = sum * hprev * r * (1 - r)\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWh, dUh, dBh\n                    j = 0\n                    while (j < hDim) {\n                        this.dBh.data[j] += dA_h[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWh.data[j * inDim + k] += dA_h[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let r = this.lastR[baseR + k]\n                            let hprev = this.lastH[baseHprev + k]\n                            this.dUh.data[j * hDim + k] += dA_h[j] * (r * hprev)\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWz, dUz, dBz\n                    j = 0\n                    while (j < hDim) {\n                        this.dBz.data[j] += dA_z[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWz.data[j * inDim + k] += dA_z[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUz.data[j * hDim + k] += dA_z[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWr, dUr, dBr\n                    j = 0\n                    while (j < hDim) {\n                        this.dBr.data[j] += dA_r[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWr.data[j * inDim + k] += dA_r[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUr.data[j * hDim + k] += dA_r[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Wz.data[k * inDim + j]\n                            sum += dA_r[k] * this.Wr.data[k * inDim + j]\n                            sum += dA_h[k] * this.Wh.data[k * inDim + j]\n                            k++\n                        }\n\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = dHprev[j]\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Uz.data[k * hDim + j]\n                            sum += dA_r[k] * this.Ur.data[k * hDim + j]\n                            sum += dA_h[k] * this.Uh.data[k * hDim + j] * this.lastR[baseR + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","lstm.ts":"namespace TorchNew {\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class LSTM {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wi: Tensor; Ui: Tensor; bi: Tensor\n        Wf: Tensor; Uf: Tensor; bf: Tensor\n        Wo: Tensor; Uo: Tensor; bo: Tensor\n        Wc: Tensor; Uc: Tensor; bc: Tensor\n\n        // Gradients\n        dWi: Tensor; dUi: Tensor; dBi: Tensor\n        dWf: Tensor; dUf: Tensor; dBf: Tensor\n        dWo: Tensor; dUo: Tensor; dBo: Tensor\n        dWc: Tensor; dUc: Tensor; dBc: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastI: number[]\n        lastF: number[]\n        lastO: number[]\n        lastCtilde: number[]\n        lastC: number[]\n        lastH: number[]\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wi = this.initWeight(hiddenDim, inputDim)\n            this.Ui = this.initWeight(hiddenDim, hiddenDim)\n            this.bi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wf = this.initWeight(hiddenDim, inputDim)\n            this.Uf = this.initWeight(hiddenDim, hiddenDim)\n            this.bf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wo = this.initWeight(hiddenDim, inputDim)\n            this.Uo = this.initWeight(hiddenDim, hiddenDim)\n            this.bo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wc = this.initWeight(hiddenDim, inputDim)\n            this.Uc = this.initWeight(hiddenDim, hiddenDim)\n            this.bc = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWi = this.initZero(hiddenDim, inputDim)\n            this.dUi = this.initZero(hiddenDim, hiddenDim)\n            this.dBi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWf = this.initZero(hiddenDim, inputDim)\n            this.dUf = this.initZero(hiddenDim, hiddenDim)\n            this.dBf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWo = this.initZero(hiddenDim, inputDim)\n            this.dUo = this.initZero(hiddenDim, hiddenDim)\n            this.dBo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWc = this.initZero(hiddenDim, inputDim)\n            this.dUc = this.initZero(hiddenDim, hiddenDim)\n            this.dBc = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastI = alloc(batch * seq * hDim)\n            this.lastF = alloc(batch * seq * hDim)\n            this.lastO = alloc(batch * seq * hDim)\n            this.lastCtilde = alloc(batch * seq * hDim)\n            this.lastC = alloc(batch * (seq + 1) * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // Initialize h0 = 0, c0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                this.lastC[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // ---- Compute gates ----\n                    let j = 0\n                    while (j < hDim) {\n                        // Input gate\n                        let sumI = this.bi.data[j]\n                        let sumF = this.bf.data[j]\n                        let sumO = this.bo.data[j]\n                        let sumC = this.bc.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            let xv = x.data[baseX + k]\n                            sumI += this.Wi.data[j * inDim + k] * xv\n                            sumF += this.Wf.data[j * inDim + k] * xv\n                            sumO += this.Wo.data[j * inDim + k] * xv\n                            sumC += this.Wc.data[j * inDim + k] * xv\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            sumI += this.Ui.data[j * hDim + k] * hprev\n                            sumF += this.Uf.data[j * hDim + k] * hprev\n                            sumO += this.Uo.data[j * hDim + k] * hprev\n                            sumC += this.Uc.data[j * hDim + k] * hprev\n                            k++\n                        }\n\n                        let iGate = sigmoid(sumI)\n                        let fGate = sigmoid(sumF)\n                        let oGate = sigmoid(sumO)\n                        let cTilde = fastTanh(sumC)\n\n                        this.lastI[baseI + j] = iGate\n                        this.lastF[baseF + j] = fGate\n                        this.lastO[baseO + j] = oGate\n                        this.lastCtilde[baseCtilde + j] = cTilde\n\n                        // Cell state\n                        let cprev = this.lastC[baseCprev + j]\n                        let ccur = fGate * cprev + iGate * cTilde\n                        this.lastC[baseCcur + j] = ccur\n\n                        // Hidden state\n                        let hcur = oGate * fastTanh(ccur)\n                        this.lastH[baseHcur + j] = hcur\n\n                        outData[(b * seq + t) * hDim + j] = hcur\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWi.data.length) { this.dWi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUi.data.length) { this.dUi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBi.data.length) { this.dBi.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWf.data.length) { this.dWf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUf.data.length) { this.dUf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBf.data.length) { this.dBf.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUo.data.length) { this.dUo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBo.data.length) { this.dBo.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWc.data.length) { this.dWc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUc.data.length) { this.dUc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBc.data.length) { this.dBc.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n            let dCnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dL/dc_t\n                    let dC = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let o = this.lastO[baseO + j]\n                        let ccur = this.lastC[baseCcur + j]\n                        dC[j] = dH[j] * o * (1 - fastTanh(ccur) * fastTanh(ccur)) + dCnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Gate derivatives\n                    let dI = alloc(hDim)\n                    let dF = alloc(hDim)\n                    let dO = alloc(hDim)\n                    let dCtilde = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let iGate = this.lastI[baseI + j]\n                        let fGate = this.lastF[baseF + j]\n                        let oGate = this.lastO[baseO + j]\n                        let cTilde = this.lastCtilde[baseCtilde + j]\n                        let cprev = this.lastC[baseCprev + j]\n\n                        dI[j] = dC[j] * cTilde * iGate * (1 - iGate)\n                        dF[j] = dC[j] * cprev * fGate * (1 - fGate)\n                        dO[j] = dH[j] * fastTanh(this.lastC[baseCcur + j]) * oGate * (1 - oGate)\n                        dCtilde[j] = dC[j] * iGate * (1 - cTilde * cTilde)\n\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWi, dUi, dBi\n                    j = 0\n                    while (j < hDim) {\n                        this.dBi.data[j] += dI[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWi.data[j * inDim + k] += dI[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUi.data[j * hDim + k] += dI[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWf, dUf, dBf\n                    j = 0\n                    while (j < hDim) {\n                        this.dBf.data[j] += dF[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWf.data[j * inDim + k] += dF[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUf.data[j * hDim + k] += dF[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWo, dUo, dBo\n                    j = 0\n                    while (j < hDim) {\n                        this.dBo.data[j] += dO[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWo.data[j * inDim + k] += dO[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUo.data[j * hDim + k] += dO[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWc, dUc, dBc\n                    j = 0\n                    while (j < hDim) {\n                        this.dBc.data[j] += dCtilde[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWc.data[j * inDim + k] += dCtilde[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUc.data[j * hDim + k] += dCtilde[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dI[k] * this.Wi.data[k * inDim + j]\n                            sum += dF[k] * this.Wf.data[k * inDim + j]\n                            sum += dO[k] * this.Wo.data[k * inDim + j]\n                            sum += dCtilde[k] * this.Wc.data[k * inDim + j]\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext and dCnext ----\n                    j = 0\n                    while (j < hDim) {\n                        // dCnext = dC * f_t\n                        let fGate = this.lastF[baseF + j]\n                        dCnext[b * hDim + j] = dC[j] * fGate\n\n                        // dHnext = contributions from all gates\n                        let sumH = 0\n\n                        // From input gate\n                        let k = 0\n                        while (k < hDim) {\n                            sumH += dI[k] * this.Ui.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From forget gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dF[k] * this.Uf.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From output gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dO[k] * this.Uo.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From candidate cell\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dCtilde[k] * this.Uc.data[k * hDim + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sumH\n\n                        j++\n                    }\n\n                    b++\n                    t--\n                }\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n\n}","residual.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Residual {\n        layer: any\n        lastInput: Tensor\n        lastLayerOut: Tensor\n\n        constructor(layer: any) {\n            this.layer = layer\n        }\n\n        // ---------------------------------------------------------\n        // Forward: y = x + layer(x)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let outLayer = this.layer.forward(x)\n            this.lastLayerOut = outLayer\n\n            let size = x.data.length\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                outData[i] = x.data[i] + outLayer.data[i]\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dX = gradOut + dLayer\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradLayer = this.layer.backward(gradOut)\n\n            let size = gradOut.data.length\n            let dXdata = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dXdata[i] = gradOut.data[i] + gradLayer.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","feedforward.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class FeedForward {\n        layer1: any\n        activation: any\n        layer2: any\n\n        lastInput: Tensor\n        lastHidden: Tensor\n\n        constructor(inputDim: number, hiddenDim: number, outputDim: number, activation: any) {\n            this.layer1 = new Linear(inputDim, hiddenDim)\n            this.activation = activation\n            this.layer2 = new Linear(hiddenDim, outputDim)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let h1 = this.layer1.forward(x)\n            let h2 = this.activation.forward(h1)\n            this.lastHidden = h2\n\n            let out = this.layer2.forward(h2)\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            // Backprop through second linear\n            let g2 = this.layer2.backward(gradOut)\n\n            // Backprop through activation\n            let gAct = this.activation.backward(g2)\n\n            // Backprop through first linear\n            let g1 = this.layer1.backward(gAct)\n\n            return g1\n        }\n    }\n}","multiheadattention.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function softmaxRow(data: number[], start: number, length: number): void {\n        let maxVal = data[start]\n        let i = 1\n        while (i < length) {\n            let v = data[start + i]\n            if (v > maxVal) maxVal = v\n            i++\n        }\n\n        let sum = 0\n        i = 0\n        while (i < length) {\n            let e = Math.exp(data[start + i] - maxVal)\n            data[start + i] = e\n            sum += e\n            i++\n        }\n\n        i = 0\n        while (i < length) {\n            data[start + i] /= sum\n            i++\n        }\n    }\n\n    export class MultiHeadAttention {\n        embedDim: number\n        numHeads: number\n        headDim: number\n\n        Wq: Tensor; bq: Tensor\n        Wk: Tensor; bk: Tensor\n        Wv: Tensor; bv: Tensor\n        Wo: Tensor; bo: Tensor\n\n        dWq: Tensor; dbq: Tensor\n        dWk: Tensor; dbk: Tensor\n        dWv: Tensor; dbv: Tensor\n        dWo: Tensor; dbo: Tensor\n\n        lastInput: Tensor\n        lastQ: Tensor\n        lastK: Tensor\n        lastV: Tensor\n        lastScores: number[]\n        lastSoftmax: number[]\n        lastAttention: number[]\n\n        lastDK: Tensor\n        lastDV: Tensor\n\n        constructor(embedDim: number, numHeads: number) {\n            this.embedDim = embedDim\n            this.numHeads = numHeads\n            this.headDim = Math.idiv(embedDim, numHeads)\n\n            this.Wq = new Linear(embedDim, embedDim).weight\n            this.bq = new Linear(embedDim, embedDim).bias\n\n            this.Wk = new Linear(embedDim, embedDim).weight\n            this.bk = new Linear(embedDim, embedDim).bias\n\n            this.Wv = new Linear(embedDim, embedDim).weight\n            this.bv = new Linear(embedDim, embedDim).bias\n\n            this.Wo = new Linear(embedDim, embedDim).weight\n            this.bo = new Linear(embedDim, embedDim).bias\n\n            this.dWq = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbq = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWk = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbk = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWv = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbv = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWo = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbo = new Tensor(alloc(embedDim), [embedDim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, embedDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(x, this.Wq, this.bq)\n            let K = this.linearForward(x, this.Wk, this.bk)\n            let V = this.linearForward(x, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores and softmax\n            let scores = alloc(batch * H * seq * seq)\n            let softmaxOut = alloc(batch * H * seq * seq)\n            let attention = alloc(batch * seq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            // Compute attention scores\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n\n                    let i = 0\n                    while (i < seq) {\n                        let j = 0\n                        while (j < seq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * seq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < seq) {\n                        softmaxRow(scores, sBase + i2 * seq, seq)\n                        let j2 = 0\n                        while (j2 < seq) {\n                            softmaxOut[sBase + i2 * seq + j2] = scores[sBase + i2 * seq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * seq * E\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < seq) {\n                                let w = softmaxOut[sBase + i3 * seq + j3]\n                                let vv = V.data[kBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, seq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection: attention -> gradAtt\n            let attTensor = new Tensor(this.lastAttention, [batch, seq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // 2) Allocate grads for Q, K, V and softmax/scores\n            let dQdata = alloc(batch * seq * E)\n            let dKdata = alloc(batch * seq * E)\n            let dVdata = alloc(batch * seq * E)\n\n            let dSoft = alloc(batch * H * seq * seq)\n            let dScores = alloc(batch * H * seq * seq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 3) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n                    let outBase = b * seq * E\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n                    let vBase = (b * seq * E) + h * D\n\n                    // dSoft and dV from gradAtt\n                    let i2 = 0\n                    while (i2 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < seq) {\n                                let w = this.lastSoftmax[sBase + i2 * seq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * seq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 4) Softmax backward: dScores from dSoft and probs\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dScores[sBase + i3 * seq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 5) Scores = (QK^T)/sqrt(D) -> dQ, dK\n                    let i4 = 0\n                    while (i4 < seq) {\n                        let j4 = 0\n                        while (j4 < seq) {\n                            let ds = dScores[sBase + i4 * seq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 6) Merge dQ, dK, dV back to [batch, seq, E] tensors\n            let dQ = new Tensor(dQdata, [batch, seq, E])\n            let dK = new Tensor(dKdata, [batch, seq, E])\n            let dV = new Tensor(dVdata, [batch, seq, E])\n\n            // 7) Back through Q, K, V linears to input x\n            let dXq = this.linearBackward(x, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(x, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(x, dV, this.Wv, this.dWv, this.dbv)\n\n            // 8) Sum contributions: dX = dXq + dXk + dXv\n            let dXdata = alloc(x.data.length)\n            i = 0\n            while (i < dXdata.length) {\n                dXdata[i] = dXq.data[i] + dXk.data[i] + dXv.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear forward\n        // ---------------------------------------------------------\n        linearForward(x: Tensor, W: Tensor, b: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = b.shape[0]\n\n            let outData = alloc(batch * seq * outDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseO = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let sum = b.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += W.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        outData[baseO + j] = sum\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(outData, [batch, seq, outDim])\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear backward (accumulates dW, db)\n        // ---------------------------------------------------------\n        linearBackward(x: Tensor, gradOut: Tensor, W: Tensor, dW: Tensor, db: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = gradOut.shape[2]\n\n            let gradInData = alloc(batch * seq * inDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseG = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let g = gradOut.data[baseG + j]\n\n                        db.data[j] += g\n\n                        let k = 0\n                        while (k < inDim) {\n                            dW.data[j * inDim + k] += g * x.data[baseX + k]\n                            gradInData[baseX + k] += g * W.data[j * inDim + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(gradInData, [batch, seq, inDim])\n        }\n\n        forwardKV(qInput: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            this.lastInput = qInput\n\n            let batch = qInput.shape[0]\n            let qSeq = qInput.shape[1]\n            let kvSeq = kInput.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(qInput, this.Wq, this.bq)\n            let K = this.linearForward(kInput, this.Wk, this.bk)\n            let V = this.linearForward(vInput, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores, softmax, attention\n            let scores = alloc(batch * H * qSeq * kvSeq)\n            let softmaxOut = alloc(batch * H * qSeq * kvSeq)\n            let attention = alloc(batch * qSeq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n\n                    // Compute scores = QK^T / sqrt(D)\n                    let i = 0\n                    while (i < qSeq) {\n                        let j = 0\n                        while (j < kvSeq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * kvSeq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        softmaxRow(scores, sBase + i2 * kvSeq, kvSeq)\n                        let j2 = 0\n                        while (j2 < kvSeq) {\n                            softmaxOut[sBase + i2 * kvSeq + j2] = scores[sBase + i2 * kvSeq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * qSeq * E\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < kvSeq) {\n                                let w = softmaxOut[sBase + i3 * kvSeq + j3]\n                                let vv = V.data[vBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, qSeq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        backwardKV(gradOut: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = Q.shape[0]\n            let qSeq = Q.shape[1]\n            let kvSeq = K.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection\n            let attTensor = new Tensor(this.lastAttention, [batch, qSeq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // Allocate grads for Q, K, V\n            let dQdata = alloc(batch * qSeq * E)\n            let dKdata = alloc(batch * kvSeq * E)\n            let dVdata = alloc(batch * kvSeq * E)\n\n            let dSoft = alloc(batch * H * qSeq * kvSeq)\n            let dScores = alloc(batch * H * qSeq * kvSeq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 2) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n                    let outBase = b * qSeq * E\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    // dSoft and dV\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < kvSeq) {\n                                let w = this.lastSoftmax[sBase + i2 * kvSeq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * kvSeq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 3) Softmax backward\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dScores[sBase + i3 * kvSeq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 4) Scores = QK^T / sqrt(D)\n                    let i4 = 0\n                    while (i4 < qSeq) {\n                        let j4 = 0\n                        while (j4 < kvSeq) {\n                            let ds = dScores[sBase + i4 * kvSeq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 5) Wrap dQ, dK, dV as tensors\n            let dQ = new Tensor(dQdata, [batch, qSeq, E])\n            let dK = new Tensor(dKdata, [batch, kvSeq, E])\n            let dV = new Tensor(dVdata, [batch, kvSeq, E])\n\n            this.lastDK = dK\n            this.lastDV = dV\n\n\n            // 6) Back through Q, K, V linears\n            let dXq = this.linearBackward(this.lastInput, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(kInput, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(vInput, dV, this.Wv, this.dWv, this.dbv)\n\n            // 7) Return gradients for decoder input (queries)\n            return dXq\n        }\n    }\n}","tranformerencoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerEncoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        mha: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterMHA: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            // LayerNorm expects an array of dims\n            this.ln1 = new LayerNorm([embedDim])\n            this.mha = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n\n            // FeedForward expects activation instance\n            this.ff = new FeedForward(\n                embedDim,\n                ffHiddenDim,\n                embedDim,\n                new TorchNew.Activations.ReLU()\n            )\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            // 1) LayerNorm â†’ MHA â†’ Residual\n            let xNorm1 = this.ln1.forward(x)\n            let mhaOut = this.mha.forward(xNorm1)\n\n            let size = x.data.length\n            let afterMHAdata = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterMHAdata[i] = x.data[i] + mhaOut.data[i]\n                i++\n            }\n            let afterMHA = new Tensor(afterMHAdata, x.shape.slice(0))\n            this.lastAfterMHA = afterMHA\n\n            // 2) LayerNorm â†’ FeedForward â†’ Residual\n            let xNorm2 = this.ln2.forward(afterMHA)\n            let ffOut = this.ff.forward(xNorm2)\n\n            let afterFFdata = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFdata[i] = afterMHA.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFdata, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // 1) Backprop through second residual: d(afterMHA) += gradOut\n            let dAfterMHAdata = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterMHAdata[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            // 2) Backprop through FeedForward\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n\n            // 3) Backprop through LayerNorm2\n            let dNorm2 = this.ln2.backward(dFF)\n\n            // Add to dAfterMHA\n            i = 0\n            while (i < size) {\n                dAfterMHAdata[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterMHA = new Tensor(dAfterMHAdata, gradOut.shape.slice(0))\n\n            // 4) Backprop through first residual: dX += dAfterMHA\n            let dXdata = alloc(size)\n            let dMHAout = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterMHA.data[i]\n                dMHAout[i] = dAfterMHA.data[i]\n                i++\n            }\n\n            // 5) Backprop through MHA\n            let dMHA = this.mha.backward(new Tensor(dMHAout, gradOut.shape.slice(0)))\n\n            // 6) Backprop through LayerNorm1\n            let dNorm1 = this.ln1.backward(dMHA)\n\n            // Add to dX\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","tranformerdecoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerDecoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        ln3: LayerNorm\n\n        selfAtt: MultiHeadAttention\n        crossAtt: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterSelf: Tensor\n        lastAfterCross: Tensor\n        lastAfterFF: Tensor\n\n        lastDEnc: Tensor\n\n\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm([embedDim])\n            this.selfAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n            this.crossAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln3 = new LayerNorm([embedDim])\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new TorchNew.Activations.ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor, encoderOut: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let embedDim = x.shape[2]\n\n            // -----------------------------\n            // 1) Masked Self-Attention\n            // -----------------------------\n            let xNorm1 = this.ln1.forward(x)\n\n            // Apply causal mask: disallow attending to future tokens\n            let masked = this.applyCausalMask(xNorm1)\n\n            let selfOut = this.selfAtt.forward(masked)\n\n            let size = x.data.length\n            let afterSelfData = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterSelfData[i] = x.data[i] + selfOut.data[i]\n                i++\n            }\n            let afterSelf = new Tensor(afterSelfData, x.shape.slice(0))\n            this.lastAfterSelf = afterSelf\n\n            // -----------------------------\n            // 2) Cross-Attention\n            // -----------------------------\n            let xNorm2 = this.ln2.forward(afterSelf)\n\n            // Cross-attention: query = decoder, key/value = encoder\n            let crossOut = this.crossAtt.forwardKV(xNorm2, encoderOut, encoderOut)\n\n            let afterCrossData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterCrossData[i] = afterSelf.data[i] + crossOut.data[i]\n                i++\n            }\n            let afterCross = new Tensor(afterCrossData, x.shape.slice(0))\n            this.lastAfterCross = afterCross\n\n            // -----------------------------\n            // 3) FeedForward\n            // -----------------------------\n            let xNorm3 = this.ln3.forward(afterCross)\n            let ffOut = this.ff.forward(xNorm3)\n\n            let afterFFData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFData[i] = afterCross.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFData, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor, encoderOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // -----------------------------\n            // 1) Backprop through FF residual\n            // -----------------------------\n            let dAfterCrossData = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterCrossData[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n            let dNorm3 = this.ln3.backward(dFF)\n\n            i = 0\n            while (i < size) {\n                dAfterCrossData[i] += dNorm3.data[i]\n                i++\n            }\n\n            let dAfterCross = new Tensor(dAfterCrossData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 2) Backprop through cross-attention residual\n            // -----------------------------\n            let dAfterSelfData = alloc(size)\n            let dCrossOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] = dAfterCross.data[i]\n                dCrossOut[i] = dAfterCross.data[i]\n                i++\n            }\n\n            let dCross = this.crossAtt.backwardKV(\n                new Tensor(dCrossOut, gradOut.shape.slice(0)),\n                encoderOut,\n                encoderOut\n            )\n\n            let dEncData = alloc(encoderOut.data.length)\n            let j = 0\n            while (j < dEncData.length) {\n                dEncData[j] = this.crossAtt.lastDK.data[j] + this.crossAtt.lastDV.data[j]\n                j++\n            }\n            this.lastDEnc = new Tensor(dEncData, encoderOut.shape.slice(0))\n\n\n            let dNorm2 = this.ln2.backward(dCross)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterSelf = new Tensor(dAfterSelfData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 3) Backprop through self-attention residual\n            // -----------------------------\n            let dXdata = alloc(size)\n            let dSelfOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterSelf.data[i]\n                dSelfOut[i] = dAfterSelf.data[i]\n                i++\n            }\n\n            let dSelf = this.selfAtt.backward(new Tensor(dSelfOut, gradOut.shape.slice(0)))\n            let dNorm1 = this.ln1.backward(dSelf)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Causal mask: zero out future positions\n        // ---------------------------------------------------------\n        applyCausalMask(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let b = 0\n            while (b < batch) {\n                let i = 0\n                while (i < seq) {\n                    let j = 0\n                    while (j < seq) {\n                        let k = 0\n                        while (k < dim) {\n                            let idx = b * seq * dim + i * dim + k\n                            let src = b * seq * dim + j * dim + k\n\n                            if (j <= i) {\n                                outData[idx] = x.data[src]\n                            } else {\n                                outData[idx] = 0\n                            }\n\n                            k++\n                        }\n                        j++\n                    }\n                    i++\n                }\n                b++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","tranformermodel.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerModel {\n        embed: Embedding\n        posEnc: PositionalEncoding\n\n        encoderBlocks: TransformerEncoder[]\n        decoderBlocks: TransformerDecoder[]\n\n        finalLinear: Linear\n\n        lastInput: Tensor\n        lastEncoderOut: Tensor\n        lastDecoderOut: Tensor\n\n        constructor(\n            vocabSize: number,\n            embedDim: number,\n            numHeads: number,\n            ffHiddenDim: number,\n            numEncoderLayers: number,\n            numDecoderLayers: number,\n            maxSeqLen: number\n        ) {\n            this.embed = new Embedding(vocabSize, embedDim)\n            this.posEnc = new PositionalEncoding(maxSeqLen, embedDim)\n\n            this.encoderBlocks = []\n            let i = 0\n            while (i < numEncoderLayers) {\n                this.encoderBlocks.push(\n                    new TransformerEncoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.decoderBlocks = []\n            i = 0\n            while (i < numDecoderLayers) {\n                this.decoderBlocks.push(\n                    new TransformerDecoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.finalLinear = new Linear(embedDim, vocabSize)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(srcTokens: Tensor, tgtTokens: Tensor): Tensor {\n            // srcTokens: [batch, srcSeq]\n            // tgtTokens: [batch, tgtSeq]\n\n            this.lastInput = srcTokens\n\n            // 1) Embed + positional encode source\n            let srcEmb = this.embed.forward(srcTokens)\n            let srcPE = this.posEnc.forward(srcEmb)\n\n            // 2) Pass through encoder stack\n            let encOut = srcPE\n            let i = 0\n            while (i < this.encoderBlocks.length) {\n                encOut = this.encoderBlocks[i].forward(encOut)\n                i++\n            }\n            this.lastEncoderOut = encOut\n\n            // 3) Embed + positional encode target\n            let tgtEmb = this.embed.forward(tgtTokens)\n            let tgtPE = this.posEnc.forward(tgtEmb)\n\n            // 4) Pass through decoder stack (with cross-attention)\n            let decOut = tgtPE\n            i = 0\n            while (i < this.decoderBlocks.length) {\n                decOut = this.decoderBlocks[i].forward(decOut, encOut)\n                i++\n            }\n            this.lastDecoderOut = decOut\n\n            // 5) Final projection to vocab logits\n            let logits = this.finalLinear.forward(decOut)\n            return logits\n        }\n\n        backward(gradOut: Tensor): { dSrc: Tensor, dTgt: Tensor } {\n            // 1) Backprop through final linear projection\n            // Use the same input x that was used in forward (adjust name if different)\n            const linGrad = this.finalLinear.backward(gradOut, this.lastDecoderOut)\n            let dDec: Tensor = linGrad.dx  // [batch, tgtSeq, embedDim]\n\n            // 2) Backprop through decoder stack (right-to-left)\n            let i = this.decoderBlocks.length - 1\n\n            // Accumulate gradients w.r.t. encoder output from all decoder blocks\n            let dEncAccum: Tensor = null\n\n            while (i >= 0) {\n                const block = this.decoderBlocks[i]\n\n                // block.backward returns { dX, dEnc }\n                // block.backward returns only dX (Tensor)\n                const dDecIn: Tensor = block.backward(dDec, this.lastEncoderOut)\n\n                // encoder gradient is stored internally\n                const dEncFromBlock: Tensor = block.lastDEnc\n\n\n\n\n                if (dEncAccum == null) {\n                    dEncAccum = dEncFromBlock\n                } else {\n                    const size = dEncAccum.data.length\n                    let j = 0\n                    while (j < size) {\n                        dEncAccum.data[j] += dEncFromBlock.data[j]\n                        j++\n                    }\n                }\n\n                dDec = dDecIn\n                i--\n            }\n\n            // 3) Backprop through encoder stack (right-to-left)\n            let dEnc: Tensor = dEncAccum\n            i = this.encoderBlocks.length - 1\n            while (i >= 0) {\n                dEnc = this.encoderBlocks[i].backward(dEnc)\n                i--\n            }\n\n            // 4) Backprop through positional encoding (source path)\n            const dSrcPE: Tensor = this.posEnc.backward(dEnc)\n\n            // 5) Backprop through embedding (source tokens)\n            const dSrc: Tensor = this.embed.backward(dSrcPE)\n\n            // 6) Backprop through positional encoding (target path)\n            const dTgtPE: Tensor = this.posEnc.backward(dDec)\n\n            // 7) Backprop through embedding (target tokens)\n            const dTgt: Tensor = this.embed.backward(dTgtPE)\n\n            return {\n                dSrc: dSrc,\n                dTgt: dTgt\n            }\n        }\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\",\n        \"linear.ts\",\n        \"cnn.ts\",\n        \"adam.ts\",\n        \"activactions.ts\",\n        \"softmax.ts\",\n        \"sequtial.ts\",\n        \"embedding.ts\",\n        \"avgpoolnd.ts\",\n        \"flatten.ts\",\n        \"layernorm.ts\",\n        \"dropout.ts\",\n        \"crossentropyloss.ts\",\n        \"positalencoding.ts\",\n        \"maxpoolnd.ts\",\n        \"convtransposend.ts\",\n        \"rnn.ts\",\n        \"gru.ts\",\n        \"lstm.ts\",\n        \"residual.ts\",\n        \"feedforward.ts\",\n        \"multiheadattention.ts\",\n        \"tranformerencoder.ts\",\n        \"tranformerdecoder.ts\",\n        \"tranformermodel.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}},{"timestamp":1771526529645,"editorVersion":"4.0.5","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","linear.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Linear {\n        inFeatures: number\n        outFeatures: number\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inFeatures: number, outFeatures: number) {\n            this.inFeatures = inFeatures\n            this.outFeatures = outFeatures\n\n            // Weight shape: [outFeatures, inFeatures]\n            let wsize = outFeatures * inFeatures\n            let wdata = alloc(wsize)\n\n            // Xavier-like init (simple version)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 / Math.sqrt(inFeatures)\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [outFeatures, inFeatures])\n\n            // Bias shape: [outFeatures]\n            let bdata = alloc(outFeatures)\n            i = 0\n            while (i < outFeatures) {\n                bdata[i] = 0\n                i++\n            }\n\n            this.bias = new Tensor(bdata, [outFeatures])\n        }\n\n        // Forward pass: x @ W^T + b\n        forward(x: Tensor): Tensor {\n            // x shape: [..., inFeatures]\n            // weight shape: [outFeatures, inFeatures]\n            // Need W^T shape: [inFeatures, outFeatures]\n\n            let Wt = this.weight.transpose(0, 1)\n\n            // Matmul: [..., inFeatures] Ã— [inFeatures, outFeatures]\n            let out = x.matmul(Wt)\n\n            // Add bias (broadcast over batch dims)\n            let size = out.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                out.data[i] += this.bias.data[idx]\n                i++\n            }\n\n            return out\n        }\n\n        backward(dY: Tensor, x: Tensor): { dx: Tensor, dW: Tensor, db: Tensor } {\n            // dY shape: [..., outFeatures]\n            // x shape:  [..., inFeatures]\n            // W shape:  [outFeatures, inFeatures]\n\n            // 1. dx = dY @ W\n            let dx = dY.matmul(this.weight)\n\n            // 2. dW = dY^T @ x\n            let dYt = dY.transpose(dY.shape.length - 2, dY.shape.length - 1)\n            let dW = dYt.matmul(x)\n\n            // 3. db = sum(dY over all batch dims)\n            let dbData = alloc(this.outFeatures)\n            let size = dY.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                dbData[idx] += dY.data[i]\n                i++\n            }\n\n            let db = new Tensor(dbData, [this.outFeatures])\n\n            return {\n                dx: dx,\n                dW: dW,\n                db: db\n            }\n        }\n    }\n}","cnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}","adam.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}","activactions.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","softmax.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","sequtial.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}","embedding.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}","avgpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","flatten.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}","layernorm.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","dropout.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","crossentropyloss.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","positalencoding.ts":"// Add your code here\nnamespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class PositionalEncoding {\n        maxLen: number\n        dim: number\n        encoding: Tensor\n\n        constructor(maxLen: number, dim: number) {\n            this.maxLen = maxLen\n            this.dim = dim\n\n            // Precompute positional encodings\n            let data = alloc(maxLen * dim)\n\n            let pos = 0\n            while (pos < maxLen) {\n                let i = 0\n                while (i < dim) {\n                    let angle = pos / Math.pow(10000, (2 * Math.idiv(i, 2)) / dim)\n\n                    if (i % 2 == 0) {\n                        data[pos * dim + i] = Math.sin(angle)\n                    } else {\n                        data[pos * dim + i] = Math.cos(angle)\n                    }\n\n                    i++\n                }\n                pos++\n            }\n\n            this.encoding = new Tensor(data, [maxLen, dim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward: add positional encoding to input\n        // x shape: [batch, seq, dim]\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let j = 0\n                while (j < seq) {\n                    let k = 0\n                    while (k < dim) {\n                        let idx = i * seq * dim + j * dim + k\n                        let pe = this.encoding.data[j * dim + k]\n                        outData[idx] = x.data[idx] + pe\n                        k++\n                    }\n                    j++\n                }\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: gradient passes through unchanged\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradData = alloc(gradOut.data.length)\n            let i = 0\n            while (i < gradOut.data.length) {\n                gradData[i] = gradOut.data[i]\n                i++\n            }\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","maxpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class MaxPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        lastInput: Tensor\n        lastMaxIndex: number[]   // stores argmax positions for backward\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Store argmax indices for backward\n            this.lastMaxIndex = alloc(total)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                let maxVal = -1e30\n                let maxIndex = -1\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let flat = x.index(idxX)\n                            let v = x.data[flat]\n                            if (v > maxVal) {\n                                maxVal = v\n                                maxIndex = flat\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                outData[p] = maxVal\n                this.lastMaxIndex[p] = maxIndex\n\n                p++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                let idx = this.lastMaxIndex[p]\n                if (idx >= 0) {\n                    dXdata[idx] += gradOut.data[p]\n                }\n                p++\n            }\n\n            return dX\n        }\n    }\n}","convtransposend.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class ConvTransposeND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        weight: Tensor\n        bias: Tensor\n\n        dW: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], stride: number[], padding: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [inChannels, outChannels, ...kernelShape]\n            let wShape = alloc(2 + dims)\n            wShape[0] = inChannels\n            wShape[1] = outChannels\n            let i = 0\n            while (i < dims) {\n                wShape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            let wSize = 1\n            i = 0\n            while (i < wShape.length) {\n                wSize *= wShape[i]\n                i++\n            }\n\n            let wData = alloc(wSize)\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wSize) {\n                wData[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wData, wShape)\n            this.dW = new Tensor(alloc(wSize), wShape)\n\n            // Bias: [outChannels]\n            let bData = alloc(outChannels)\n            this.bias = new Tensor(bData, [outChannels])\n            this.dB = new Tensor(alloc(outChannels), [outChannels])\n        }\n\n        // ---------------------------------------------------------\n        // Forward (fractionally strided convolution)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let inC = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let s = this.stride[i]\n                let p = this.padding[i]\n\n                // Transposed conv output formula:\n                outSpatial[i] = (inSize - 1) * s - 2 * p + k\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Compute base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute output index\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= outSpatial[d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                // Weight index\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n\n                                let wVal = this.weight.get(idxW)\n                                let outIdx = out.index(idxOut)\n                                outData[outIdx] += x.data[p] * wVal\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            // Add bias\n            let q = 0\n            while (q < outData.length) {\n                let idx = Tensor.unravelIndex(q, outShape)\n                let oc = idx[1]\n                outData[q] += this.bias.data[oc]\n                q++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dW.data.length) {\n                this.dW.data[i] = 0\n                i++\n            }\n            i = 0\n            while (i < this.dB.data.length) {\n                this.dB.data[i] = 0\n                i++\n            }\n\n            // dB: sum over gradOut\n            let q = 0\n            while (q < gradOut.data.length) {\n                let idx = Tensor.unravelIndex(q, gradOut.shape)\n                let oc = idx[1]\n                this.dB.data[oc] += gradOut.data[q]\n                q++\n            }\n\n            // dX\n            let dXdata = alloc(x.data.length)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions for dW and dX\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= gradOut.shape[2 + d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                let go = gradOut.get(idxOut)\n\n                                // dW\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n                                let wIdx = this.weight.index(idxW)\n                                this.dW.data[wIdx] += x.data[p] * go\n\n                                // dX\n                                dXdata[p] += this.weight.data[wIdx] * go\n\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","rnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n    // tanh implementation (same as in activations.ts)\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class RNN {\n        inputDim: number\n        hiddenDim: number\n\n        Wxh: Tensor\n        Whh: Tensor\n        b: Tensor\n\n        dWxh: Tensor\n        dWhh: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n        lastH: number[]      // all hidden states (flattened)\n        lastA: number[]      // all pre-activations (Wxh x + Whh h + b)\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            // Weight shapes\n            this.Wxh = new Tensor(\n                this.initWeights(inputDim * hiddenDim),\n                [hiddenDim, inputDim]\n            )\n\n            this.Whh = new Tensor(\n                this.initWeights(hiddenDim * hiddenDim),\n                [hiddenDim, hiddenDim]\n            )\n\n            this.b = new Tensor(\n                alloc(hiddenDim),\n                [hiddenDim]\n            )\n\n            this.dWxh = new Tensor(alloc(inputDim * hiddenDim), [hiddenDim, inputDim])\n            this.dWhh = new Tensor(alloc(hiddenDim * hiddenDim), [hiddenDim, hiddenDim])\n            this.dB = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeights(size: number): number[] {\n            let arr = alloc(size)\n            let scale = 1 / Math.sqrt(size)\n            let i = 0\n            while (i < size) {\n                arr[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return arr\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            // Store hidden states and pre-activations\n            this.lastH = alloc(batch * (seq + 1) * hDim)  // includes h0 = 0\n            this.lastA = alloc(batch * seq * hDim)\n\n            // Initialize h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            // Forward through time\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    // Compute h_t\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.b.data[j]\n\n                        // Wxh * x_t\n                        let k = 0\n                        while (k < inDim) {\n                            let w = this.Wxh.data[j * inDim + k]\n                            let xv = x.data[baseX + k]\n                            sum += w * xv\n                            k++\n                        }\n\n                        // Whh * h_{t-1}\n                        k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[j * hDim + k]\n                            let hv = this.lastH[baseHprev + k]\n                            sum += w * hv\n                            k++\n                        }\n\n                        this.lastA[baseA + j] = sum\n                        this.lastH[baseHcur + j] = fastTanh(sum)\n\n                        // Output is h_t\n                        outData[(b * seq + t) * hDim + j] = this.lastH[baseHcur + j]\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWxh.data.length) { this.dWxh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWhh.data.length) { this.dWhh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dB.data.length) { this.dB.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)  // gradient from next time step\n\n            // BPTT\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    // dL/dh_t = gradOut + dHnext\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dA = dH * (1 - tanh(a)^2)\n                    let dA = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let h = this.lastH[baseHcur + j]\n                        dA[j] = dH[j] * (1 - h * h)\n                        j++\n                    }\n\n                    // Accumulate gradients\n                    j = 0\n                    while (j < hDim) {\n                        // dB\n                        this.dB.data[j] += dA[j]\n\n                        // dWxh\n                        let k = 0\n                        while (k < inDim) {\n                            let idx = j * inDim + k\n                            this.dWxh.data[idx] += dA[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        // dWhh\n                        k = 0\n                        while (k < hDim) {\n                            let idx = j * hDim + k\n                            this.dWhh.data[idx] += dA[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dX\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Wxh.data[k * inDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // dHnext = Whh^T * dA\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[k * hDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","gru.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class GRU {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wz: Tensor; Uz: Tensor; bz: Tensor\n        Wr: Tensor; Ur: Tensor; br: Tensor\n        Wh: Tensor; Uh: Tensor; bh: Tensor\n\n        // Gradients\n        dWz: Tensor; dUz: Tensor; dBz: Tensor\n        dWr: Tensor; dUr: Tensor; dBr: Tensor\n        dWh: Tensor; dUh: Tensor; dBh: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastZ: number[]\n        lastR: number[]\n        lastHtilde: number[]\n        lastH: number[]   // includes h0\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wz = this.initWeight(hiddenDim, inputDim)\n            this.Uz = this.initWeight(hiddenDim, hiddenDim)\n            this.bz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wr = this.initWeight(hiddenDim, inputDim)\n            this.Ur = this.initWeight(hiddenDim, hiddenDim)\n            this.br = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wh = this.initWeight(hiddenDim, inputDim)\n            this.Uh = this.initWeight(hiddenDim, hiddenDim)\n            this.bh = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWz = this.initZero(hiddenDim, inputDim)\n            this.dUz = this.initZero(hiddenDim, hiddenDim)\n            this.dBz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWr = this.initZero(hiddenDim, inputDim)\n            this.dUr = this.initZero(hiddenDim, hiddenDim)\n            this.dBr = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWh = this.initZero(hiddenDim, inputDim)\n            this.dUh = this.initZero(hiddenDim, hiddenDim)\n            this.dBh = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastZ = alloc(batch * seq * hDim)\n            this.lastR = alloc(batch * seq * hDim)\n            this.lastHtilde = alloc(batch * seq * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // ---- Compute z_t ----\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.bz.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wz.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Uz.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let z = sigmoid(sum)\n                        this.lastZ[baseZ + j] = z\n                        j++\n                    }\n\n                    // ---- Compute r_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.br.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wr.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Ur.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let r = sigmoid(sum)\n                        this.lastR[baseR + j] = r\n                        j++\n                    }\n\n                    // ---- Compute h~_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.bh.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wh.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            let r = this.lastR[baseR + k]\n                            sum += this.Uh.data[j * hDim + k] * (r * hprev)\n                            k++\n                        }\n\n                        let htilde = fastTanh(sum)\n                        this.lastHtilde[baseHtilde + j] = htilde\n                        j++\n                    }\n\n                    // ---- Final h_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        let h = (1 - z) * hprev + z * htilde\n                        this.lastH[baseHcur + j] = h\n\n                        outData[(b * seq + t) * hDim + j] = h\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWz.data.length) { this.dWz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUz.data.length) { this.dUz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBz.data.length) { this.dBz.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWr.data.length) { this.dWr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUr.data.length) { this.dUr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBr.data.length) { this.dBr.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWh.data.length) { this.dWh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUh.data.length) { this.dUh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBh.data.length) { this.dBh.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Compute gate derivatives\n                    let dZ = alloc(hDim)\n                    let dHtilde = alloc(hDim)\n                    let dHprev = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        dZ[j] = dH[j] * (htilde - hprev)\n                        dHtilde[j] = dH[j] * z\n                        dHprev[j] = dH[j] * (1 - z)\n                        j++\n                    }\n\n                    // dA_h = dHtilde * (1 - htilde^2)\n                    let dA_h = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let htilde = this.lastHtilde[baseHtilde + j]\n                        dA_h[j] = dHtilde[j] * (1 - htilde * htilde)\n                        j++\n                    }\n\n                    // dA_z = dZ * z * (1 - z)\n                    let dA_z = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        dA_z[j] = dZ[j] * z * (1 - z)\n                        j++\n                    }\n\n                    // dA_r = dHtilde * Uh * hprev * r*(1-r)\n                    let dA_r = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Uh.data[k * hDim + j]\n                            let d = dA_h[k]\n                            sum += d * w\n                            k++\n                        }\n                        let r = this.lastR[baseR + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        dA_r[j] = sum * hprev * r * (1 - r)\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWh, dUh, dBh\n                    j = 0\n                    while (j < hDim) {\n                        this.dBh.data[j] += dA_h[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWh.data[j * inDim + k] += dA_h[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let r = this.lastR[baseR + k]\n                            let hprev = this.lastH[baseHprev + k]\n                            this.dUh.data[j * hDim + k] += dA_h[j] * (r * hprev)\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWz, dUz, dBz\n                    j = 0\n                    while (j < hDim) {\n                        this.dBz.data[j] += dA_z[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWz.data[j * inDim + k] += dA_z[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUz.data[j * hDim + k] += dA_z[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWr, dUr, dBr\n                    j = 0\n                    while (j < hDim) {\n                        this.dBr.data[j] += dA_r[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWr.data[j * inDim + k] += dA_r[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUr.data[j * hDim + k] += dA_r[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Wz.data[k * inDim + j]\n                            sum += dA_r[k] * this.Wr.data[k * inDim + j]\n                            sum += dA_h[k] * this.Wh.data[k * inDim + j]\n                            k++\n                        }\n\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = dHprev[j]\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Uz.data[k * hDim + j]\n                            sum += dA_r[k] * this.Ur.data[k * hDim + j]\n                            sum += dA_h[k] * this.Uh.data[k * hDim + j] * this.lastR[baseR + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","lstm.ts":"namespace TorchNew {\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class LSTM {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wi: Tensor; Ui: Tensor; bi: Tensor\n        Wf: Tensor; Uf: Tensor; bf: Tensor\n        Wo: Tensor; Uo: Tensor; bo: Tensor\n        Wc: Tensor; Uc: Tensor; bc: Tensor\n\n        // Gradients\n        dWi: Tensor; dUi: Tensor; dBi: Tensor\n        dWf: Tensor; dUf: Tensor; dBf: Tensor\n        dWo: Tensor; dUo: Tensor; dBo: Tensor\n        dWc: Tensor; dUc: Tensor; dBc: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastI: number[]\n        lastF: number[]\n        lastO: number[]\n        lastCtilde: number[]\n        lastC: number[]\n        lastH: number[]\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wi = this.initWeight(hiddenDim, inputDim)\n            this.Ui = this.initWeight(hiddenDim, hiddenDim)\n            this.bi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wf = this.initWeight(hiddenDim, inputDim)\n            this.Uf = this.initWeight(hiddenDim, hiddenDim)\n            this.bf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wo = this.initWeight(hiddenDim, inputDim)\n            this.Uo = this.initWeight(hiddenDim, hiddenDim)\n            this.bo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wc = this.initWeight(hiddenDim, inputDim)\n            this.Uc = this.initWeight(hiddenDim, hiddenDim)\n            this.bc = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWi = this.initZero(hiddenDim, inputDim)\n            this.dUi = this.initZero(hiddenDim, hiddenDim)\n            this.dBi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWf = this.initZero(hiddenDim, inputDim)\n            this.dUf = this.initZero(hiddenDim, hiddenDim)\n            this.dBf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWo = this.initZero(hiddenDim, inputDim)\n            this.dUo = this.initZero(hiddenDim, hiddenDim)\n            this.dBo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWc = this.initZero(hiddenDim, inputDim)\n            this.dUc = this.initZero(hiddenDim, hiddenDim)\n            this.dBc = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastI = alloc(batch * seq * hDim)\n            this.lastF = alloc(batch * seq * hDim)\n            this.lastO = alloc(batch * seq * hDim)\n            this.lastCtilde = alloc(batch * seq * hDim)\n            this.lastC = alloc(batch * (seq + 1) * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // Initialize h0 = 0, c0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                this.lastC[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // ---- Compute gates ----\n                    let j = 0\n                    while (j < hDim) {\n                        // Input gate\n                        let sumI = this.bi.data[j]\n                        let sumF = this.bf.data[j]\n                        let sumO = this.bo.data[j]\n                        let sumC = this.bc.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            let xv = x.data[baseX + k]\n                            sumI += this.Wi.data[j * inDim + k] * xv\n                            sumF += this.Wf.data[j * inDim + k] * xv\n                            sumO += this.Wo.data[j * inDim + k] * xv\n                            sumC += this.Wc.data[j * inDim + k] * xv\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            sumI += this.Ui.data[j * hDim + k] * hprev\n                            sumF += this.Uf.data[j * hDim + k] * hprev\n                            sumO += this.Uo.data[j * hDim + k] * hprev\n                            sumC += this.Uc.data[j * hDim + k] * hprev\n                            k++\n                        }\n\n                        let iGate = sigmoid(sumI)\n                        let fGate = sigmoid(sumF)\n                        let oGate = sigmoid(sumO)\n                        let cTilde = fastTanh(sumC)\n\n                        this.lastI[baseI + j] = iGate\n                        this.lastF[baseF + j] = fGate\n                        this.lastO[baseO + j] = oGate\n                        this.lastCtilde[baseCtilde + j] = cTilde\n\n                        // Cell state\n                        let cprev = this.lastC[baseCprev + j]\n                        let ccur = fGate * cprev + iGate * cTilde\n                        this.lastC[baseCcur + j] = ccur\n\n                        // Hidden state\n                        let hcur = oGate * fastTanh(ccur)\n                        this.lastH[baseHcur + j] = hcur\n\n                        outData[(b * seq + t) * hDim + j] = hcur\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWi.data.length) { this.dWi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUi.data.length) { this.dUi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBi.data.length) { this.dBi.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWf.data.length) { this.dWf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUf.data.length) { this.dUf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBf.data.length) { this.dBf.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUo.data.length) { this.dUo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBo.data.length) { this.dBo.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWc.data.length) { this.dWc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUc.data.length) { this.dUc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBc.data.length) { this.dBc.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n            let dCnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dL/dc_t\n                    let dC = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let o = this.lastO[baseO + j]\n                        let ccur = this.lastC[baseCcur + j]\n                        dC[j] = dH[j] * o * (1 - fastTanh(ccur) * fastTanh(ccur)) + dCnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Gate derivatives\n                    let dI = alloc(hDim)\n                    let dF = alloc(hDim)\n                    let dO = alloc(hDim)\n                    let dCtilde = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let iGate = this.lastI[baseI + j]\n                        let fGate = this.lastF[baseF + j]\n                        let oGate = this.lastO[baseO + j]\n                        let cTilde = this.lastCtilde[baseCtilde + j]\n                        let cprev = this.lastC[baseCprev + j]\n\n                        dI[j] = dC[j] * cTilde * iGate * (1 - iGate)\n                        dF[j] = dC[j] * cprev * fGate * (1 - fGate)\n                        dO[j] = dH[j] * fastTanh(this.lastC[baseCcur + j]) * oGate * (1 - oGate)\n                        dCtilde[j] = dC[j] * iGate * (1 - cTilde * cTilde)\n\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWi, dUi, dBi\n                    j = 0\n                    while (j < hDim) {\n                        this.dBi.data[j] += dI[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWi.data[j * inDim + k] += dI[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUi.data[j * hDim + k] += dI[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWf, dUf, dBf\n                    j = 0\n                    while (j < hDim) {\n                        this.dBf.data[j] += dF[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWf.data[j * inDim + k] += dF[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUf.data[j * hDim + k] += dF[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWo, dUo, dBo\n                    j = 0\n                    while (j < hDim) {\n                        this.dBo.data[j] += dO[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWo.data[j * inDim + k] += dO[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUo.data[j * hDim + k] += dO[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWc, dUc, dBc\n                    j = 0\n                    while (j < hDim) {\n                        this.dBc.data[j] += dCtilde[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWc.data[j * inDim + k] += dCtilde[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUc.data[j * hDim + k] += dCtilde[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dI[k] * this.Wi.data[k * inDim + j]\n                            sum += dF[k] * this.Wf.data[k * inDim + j]\n                            sum += dO[k] * this.Wo.data[k * inDim + j]\n                            sum += dCtilde[k] * this.Wc.data[k * inDim + j]\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext and dCnext ----\n                    j = 0\n                    while (j < hDim) {\n                        // dCnext = dC * f_t\n                        let fGate = this.lastF[baseF + j]\n                        dCnext[b * hDim + j] = dC[j] * fGate\n\n                        // dHnext = contributions from all gates\n                        let sumH = 0\n\n                        // From input gate\n                        let k = 0\n                        while (k < hDim) {\n                            sumH += dI[k] * this.Ui.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From forget gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dF[k] * this.Uf.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From output gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dO[k] * this.Uo.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From candidate cell\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dCtilde[k] * this.Uc.data[k * hDim + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sumH\n\n                        j++\n                    }\n\n                    b++\n                    t--\n                }\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n\n}","residual.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Residual {\n        layer: any\n        lastInput: Tensor\n        lastLayerOut: Tensor\n\n        constructor(layer: any) {\n            this.layer = layer\n        }\n\n        // ---------------------------------------------------------\n        // Forward: y = x + layer(x)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let outLayer = this.layer.forward(x)\n            this.lastLayerOut = outLayer\n\n            let size = x.data.length\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                outData[i] = x.data[i] + outLayer.data[i]\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dX = gradOut + dLayer\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradLayer = this.layer.backward(gradOut)\n\n            let size = gradOut.data.length\n            let dXdata = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dXdata[i] = gradOut.data[i] + gradLayer.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","feedforward.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class FeedForward {\n        layer1: any\n        activation: any\n        layer2: any\n\n        lastInput: Tensor\n        lastHidden: Tensor\n\n        constructor(inputDim: number, hiddenDim: number, outputDim: number, activation: any) {\n            this.layer1 = new Linear(inputDim, hiddenDim)\n            this.activation = activation\n            this.layer2 = new Linear(hiddenDim, outputDim)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let h1 = this.layer1.forward(x)\n            let h2 = this.activation.forward(h1)\n            this.lastHidden = h2\n\n            let out = this.layer2.forward(h2)\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            // Backprop through second linear\n            let g2 = this.layer2.backward(gradOut)\n\n            // Backprop through activation\n            let gAct = this.activation.backward(g2)\n\n            // Backprop through first linear\n            let g1 = this.layer1.backward(gAct)\n\n            return g1\n        }\n    }\n}","multiheadattention.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function softmaxRow(data: number[], start: number, length: number): void {\n        let maxVal = data[start]\n        let i = 1\n        while (i < length) {\n            let v = data[start + i]\n            if (v > maxVal) maxVal = v\n            i++\n        }\n\n        let sum = 0\n        i = 0\n        while (i < length) {\n            let e = Math.exp(data[start + i] - maxVal)\n            data[start + i] = e\n            sum += e\n            i++\n        }\n\n        i = 0\n        while (i < length) {\n            data[start + i] /= sum\n            i++\n        }\n    }\n\n    export class MultiHeadAttention {\n        embedDim: number\n        numHeads: number\n        headDim: number\n\n        Wq: Tensor; bq: Tensor\n        Wk: Tensor; bk: Tensor\n        Wv: Tensor; bv: Tensor\n        Wo: Tensor; bo: Tensor\n\n        dWq: Tensor; dbq: Tensor\n        dWk: Tensor; dbk: Tensor\n        dWv: Tensor; dbv: Tensor\n        dWo: Tensor; dbo: Tensor\n\n        lastInput: Tensor\n        lastQ: Tensor\n        lastK: Tensor\n        lastV: Tensor\n        lastScores: number[]\n        lastSoftmax: number[]\n        lastAttention: number[]\n\n        lastDK: Tensor\n        lastDV: Tensor\n\n        constructor(embedDim: number, numHeads: number) {\n            this.embedDim = embedDim\n            this.numHeads = numHeads\n            this.headDim = Math.idiv(embedDim, numHeads)\n\n            this.Wq = new Linear(embedDim, embedDim).weight\n            this.bq = new Linear(embedDim, embedDim).bias\n\n            this.Wk = new Linear(embedDim, embedDim).weight\n            this.bk = new Linear(embedDim, embedDim).bias\n\n            this.Wv = new Linear(embedDim, embedDim).weight\n            this.bv = new Linear(embedDim, embedDim).bias\n\n            this.Wo = new Linear(embedDim, embedDim).weight\n            this.bo = new Linear(embedDim, embedDim).bias\n\n            this.dWq = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbq = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWk = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbk = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWv = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbv = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWo = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbo = new Tensor(alloc(embedDim), [embedDim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, embedDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(x, this.Wq, this.bq)\n            let K = this.linearForward(x, this.Wk, this.bk)\n            let V = this.linearForward(x, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores and softmax\n            let scores = alloc(batch * H * seq * seq)\n            let softmaxOut = alloc(batch * H * seq * seq)\n            let attention = alloc(batch * seq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            // Compute attention scores\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n\n                    let i = 0\n                    while (i < seq) {\n                        let j = 0\n                        while (j < seq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * seq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < seq) {\n                        softmaxRow(scores, sBase + i2 * seq, seq)\n                        let j2 = 0\n                        while (j2 < seq) {\n                            softmaxOut[sBase + i2 * seq + j2] = scores[sBase + i2 * seq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * seq * E\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < seq) {\n                                let w = softmaxOut[sBase + i3 * seq + j3]\n                                let vv = V.data[kBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, seq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection: attention -> gradAtt\n            let attTensor = new Tensor(this.lastAttention, [batch, seq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // 2) Allocate grads for Q, K, V and softmax/scores\n            let dQdata = alloc(batch * seq * E)\n            let dKdata = alloc(batch * seq * E)\n            let dVdata = alloc(batch * seq * E)\n\n            let dSoft = alloc(batch * H * seq * seq)\n            let dScores = alloc(batch * H * seq * seq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 3) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n                    let outBase = b * seq * E\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n                    let vBase = (b * seq * E) + h * D\n\n                    // dSoft and dV from gradAtt\n                    let i2 = 0\n                    while (i2 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < seq) {\n                                let w = this.lastSoftmax[sBase + i2 * seq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * seq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 4) Softmax backward: dScores from dSoft and probs\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dScores[sBase + i3 * seq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 5) Scores = (QK^T)/sqrt(D) -> dQ, dK\n                    let i4 = 0\n                    while (i4 < seq) {\n                        let j4 = 0\n                        while (j4 < seq) {\n                            let ds = dScores[sBase + i4 * seq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 6) Merge dQ, dK, dV back to [batch, seq, E] tensors\n            let dQ = new Tensor(dQdata, [batch, seq, E])\n            let dK = new Tensor(dKdata, [batch, seq, E])\n            let dV = new Tensor(dVdata, [batch, seq, E])\n\n            // 7) Back through Q, K, V linears to input x\n            let dXq = this.linearBackward(x, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(x, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(x, dV, this.Wv, this.dWv, this.dbv)\n\n            // 8) Sum contributions: dX = dXq + dXk + dXv\n            let dXdata = alloc(x.data.length)\n            i = 0\n            while (i < dXdata.length) {\n                dXdata[i] = dXq.data[i] + dXk.data[i] + dXv.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear forward\n        // ---------------------------------------------------------\n        linearForward(x: Tensor, W: Tensor, b: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = b.shape[0]\n\n            let outData = alloc(batch * seq * outDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseO = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let sum = b.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += W.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        outData[baseO + j] = sum\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(outData, [batch, seq, outDim])\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear backward (accumulates dW, db)\n        // ---------------------------------------------------------\n        linearBackward(x: Tensor, gradOut: Tensor, W: Tensor, dW: Tensor, db: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = gradOut.shape[2]\n\n            let gradInData = alloc(batch * seq * inDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseG = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let g = gradOut.data[baseG + j]\n\n                        db.data[j] += g\n\n                        let k = 0\n                        while (k < inDim) {\n                            dW.data[j * inDim + k] += g * x.data[baseX + k]\n                            gradInData[baseX + k] += g * W.data[j * inDim + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(gradInData, [batch, seq, inDim])\n        }\n\n        forwardKV(qInput: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            this.lastInput = qInput\n\n            let batch = qInput.shape[0]\n            let qSeq = qInput.shape[1]\n            let kvSeq = kInput.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(qInput, this.Wq, this.bq)\n            let K = this.linearForward(kInput, this.Wk, this.bk)\n            let V = this.linearForward(vInput, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores, softmax, attention\n            let scores = alloc(batch * H * qSeq * kvSeq)\n            let softmaxOut = alloc(batch * H * qSeq * kvSeq)\n            let attention = alloc(batch * qSeq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n\n                    // Compute scores = QK^T / sqrt(D)\n                    let i = 0\n                    while (i < qSeq) {\n                        let j = 0\n                        while (j < kvSeq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * kvSeq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        softmaxRow(scores, sBase + i2 * kvSeq, kvSeq)\n                        let j2 = 0\n                        while (j2 < kvSeq) {\n                            softmaxOut[sBase + i2 * kvSeq + j2] = scores[sBase + i2 * kvSeq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * qSeq * E\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < kvSeq) {\n                                let w = softmaxOut[sBase + i3 * kvSeq + j3]\n                                let vv = V.data[vBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, qSeq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        backwardKV(gradOut: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = Q.shape[0]\n            let qSeq = Q.shape[1]\n            let kvSeq = K.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection\n            let attTensor = new Tensor(this.lastAttention, [batch, qSeq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // Allocate grads for Q, K, V\n            let dQdata = alloc(batch * qSeq * E)\n            let dKdata = alloc(batch * kvSeq * E)\n            let dVdata = alloc(batch * kvSeq * E)\n\n            let dSoft = alloc(batch * H * qSeq * kvSeq)\n            let dScores = alloc(batch * H * qSeq * kvSeq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 2) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n                    let outBase = b * qSeq * E\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    // dSoft and dV\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < kvSeq) {\n                                let w = this.lastSoftmax[sBase + i2 * kvSeq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * kvSeq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 3) Softmax backward\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dScores[sBase + i3 * kvSeq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 4) Scores = QK^T / sqrt(D)\n                    let i4 = 0\n                    while (i4 < qSeq) {\n                        let j4 = 0\n                        while (j4 < kvSeq) {\n                            let ds = dScores[sBase + i4 * kvSeq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 5) Wrap dQ, dK, dV as tensors\n            let dQ = new Tensor(dQdata, [batch, qSeq, E])\n            let dK = new Tensor(dKdata, [batch, kvSeq, E])\n            let dV = new Tensor(dVdata, [batch, kvSeq, E])\n\n            this.lastDK = dK\n            this.lastDV = dV\n\n\n            // 6) Back through Q, K, V linears\n            let dXq = this.linearBackward(this.lastInput, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(kInput, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(vInput, dV, this.Wv, this.dWv, this.dbv)\n\n            // 7) Return gradients for decoder input (queries)\n            return dXq\n        }\n    }\n}","tranformerencoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerEncoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        mha: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterMHA: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            // LayerNorm expects an array of dims\n            this.ln1 = new LayerNorm([embedDim])\n            this.mha = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n\n            // FeedForward expects activation instance\n            this.ff = new FeedForward(\n                embedDim,\n                ffHiddenDim,\n                embedDim,\n                new TorchNew.Activations.ReLU()\n            )\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            // 1) LayerNorm â†’ MHA â†’ Residual\n            let xNorm1 = this.ln1.forward(x)\n            let mhaOut = this.mha.forward(xNorm1)\n\n            let size = x.data.length\n            let afterMHAdata = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterMHAdata[i] = x.data[i] + mhaOut.data[i]\n                i++\n            }\n            let afterMHA = new Tensor(afterMHAdata, x.shape.slice(0))\n            this.lastAfterMHA = afterMHA\n\n            // 2) LayerNorm â†’ FeedForward â†’ Residual\n            let xNorm2 = this.ln2.forward(afterMHA)\n            let ffOut = this.ff.forward(xNorm2)\n\n            let afterFFdata = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFdata[i] = afterMHA.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFdata, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // 1) Backprop through second residual: d(afterMHA) += gradOut\n            let dAfterMHAdata = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterMHAdata[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            // 2) Backprop through FeedForward\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n\n            // 3) Backprop through LayerNorm2\n            let dNorm2 = this.ln2.backward(dFF)\n\n            // Add to dAfterMHA\n            i = 0\n            while (i < size) {\n                dAfterMHAdata[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterMHA = new Tensor(dAfterMHAdata, gradOut.shape.slice(0))\n\n            // 4) Backprop through first residual: dX += dAfterMHA\n            let dXdata = alloc(size)\n            let dMHAout = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterMHA.data[i]\n                dMHAout[i] = dAfterMHA.data[i]\n                i++\n            }\n\n            // 5) Backprop through MHA\n            let dMHA = this.mha.backward(new Tensor(dMHAout, gradOut.shape.slice(0)))\n\n            // 6) Backprop through LayerNorm1\n            let dNorm1 = this.ln1.backward(dMHA)\n\n            // Add to dX\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","tranformerdecoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerDecoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        ln3: LayerNorm\n\n        selfAtt: MultiHeadAttention\n        crossAtt: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterSelf: Tensor\n        lastAfterCross: Tensor\n        lastAfterFF: Tensor\n\n        lastDEnc: Tensor\n\n\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm([embedDim])\n            this.selfAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n            this.crossAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln3 = new LayerNorm([embedDim])\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new TorchNew.Activations.ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor, encoderOut: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let embedDim = x.shape[2]\n\n            // -----------------------------\n            // 1) Masked Self-Attention\n            // -----------------------------\n            let xNorm1 = this.ln1.forward(x)\n\n            // Apply causal mask: disallow attending to future tokens\n            let masked = this.applyCausalMask(xNorm1)\n\n            let selfOut = this.selfAtt.forward(masked)\n\n            let size = x.data.length\n            let afterSelfData = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterSelfData[i] = x.data[i] + selfOut.data[i]\n                i++\n            }\n            let afterSelf = new Tensor(afterSelfData, x.shape.slice(0))\n            this.lastAfterSelf = afterSelf\n\n            // -----------------------------\n            // 2) Cross-Attention\n            // -----------------------------\n            let xNorm2 = this.ln2.forward(afterSelf)\n\n            // Cross-attention: query = decoder, key/value = encoder\n            let crossOut = this.crossAtt.forwardKV(xNorm2, encoderOut, encoderOut)\n\n            let afterCrossData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterCrossData[i] = afterSelf.data[i] + crossOut.data[i]\n                i++\n            }\n            let afterCross = new Tensor(afterCrossData, x.shape.slice(0))\n            this.lastAfterCross = afterCross\n\n            // -----------------------------\n            // 3) FeedForward\n            // -----------------------------\n            let xNorm3 = this.ln3.forward(afterCross)\n            let ffOut = this.ff.forward(xNorm3)\n\n            let afterFFData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFData[i] = afterCross.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFData, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor, encoderOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // -----------------------------\n            // 1) Backprop through FF residual\n            // -----------------------------\n            let dAfterCrossData = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterCrossData[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n            let dNorm3 = this.ln3.backward(dFF)\n\n            i = 0\n            while (i < size) {\n                dAfterCrossData[i] += dNorm3.data[i]\n                i++\n            }\n\n            let dAfterCross = new Tensor(dAfterCrossData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 2) Backprop through cross-attention residual\n            // -----------------------------\n            let dAfterSelfData = alloc(size)\n            let dCrossOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] = dAfterCross.data[i]\n                dCrossOut[i] = dAfterCross.data[i]\n                i++\n            }\n\n            let dCross = this.crossAtt.backwardKV(\n                new Tensor(dCrossOut, gradOut.shape.slice(0)),\n                encoderOut,\n                encoderOut\n            )\n\n            let dEncData = alloc(encoderOut.data.length)\n            let j = 0\n            while (j < dEncData.length) {\n                dEncData[j] = this.crossAtt.lastDK.data[j] + this.crossAtt.lastDV.data[j]\n                j++\n            }\n            this.lastDEnc = new Tensor(dEncData, encoderOut.shape.slice(0))\n\n\n            let dNorm2 = this.ln2.backward(dCross)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterSelf = new Tensor(dAfterSelfData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 3) Backprop through self-attention residual\n            // -----------------------------\n            let dXdata = alloc(size)\n            let dSelfOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterSelf.data[i]\n                dSelfOut[i] = dAfterSelf.data[i]\n                i++\n            }\n\n            let dSelf = this.selfAtt.backward(new Tensor(dSelfOut, gradOut.shape.slice(0)))\n            let dNorm1 = this.ln1.backward(dSelf)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Causal mask: zero out future positions\n        // ---------------------------------------------------------\n        applyCausalMask(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let b = 0\n            while (b < batch) {\n                let i = 0\n                while (i < seq) {\n                    let j = 0\n                    while (j < seq) {\n                        let k = 0\n                        while (k < dim) {\n                            let idx = b * seq * dim + i * dim + k\n                            let src = b * seq * dim + j * dim + k\n\n                            if (j <= i) {\n                                outData[idx] = x.data[src]\n                            } else {\n                                outData[idx] = 0\n                            }\n\n                            k++\n                        }\n                        j++\n                    }\n                    i++\n                }\n                b++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","tranformermodel.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerModel {\n        embed: Embedding\n        posEnc: PositionalEncoding\n\n        encoderBlocks: TransformerEncoder[]\n        decoderBlocks: TransformerDecoder[]\n\n        finalLinear: Linear\n\n        lastInput: Tensor\n        lastEncoderOut: Tensor\n        lastDecoderOut: Tensor\n\n        constructor(\n            vocabSize: number,\n            embedDim: number,\n            numHeads: number,\n            ffHiddenDim: number,\n            numEncoderLayers: number,\n            numDecoderLayers: number,\n            maxSeqLen: number\n        ) {\n            this.embed = new Embedding(vocabSize, embedDim)\n            this.posEnc = new PositionalEncoding(maxSeqLen, embedDim)\n\n            this.encoderBlocks = []\n            let i = 0\n            while (i < numEncoderLayers) {\n                this.encoderBlocks.push(\n                    new TransformerEncoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.decoderBlocks = []\n            i = 0\n            while (i < numDecoderLayers) {\n                this.decoderBlocks.push(\n                    new TransformerDecoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.finalLinear = new Linear(embedDim, vocabSize)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(srcTokens: Tensor, tgtTokens: Tensor): Tensor {\n            // srcTokens: [batch, srcSeq]\n            // tgtTokens: [batch, tgtSeq]\n\n            this.lastInput = srcTokens\n\n            // 1) Embed + positional encode source\n            let srcEmb = this.embed.forward(srcTokens)\n            let srcPE = this.posEnc.forward(srcEmb)\n\n            // 2) Pass through encoder stack\n            let encOut = srcPE\n            let i = 0\n            while (i < this.encoderBlocks.length) {\n                encOut = this.encoderBlocks[i].forward(encOut)\n                i++\n            }\n            this.lastEncoderOut = encOut\n\n            // 3) Embed + positional encode target\n            let tgtEmb = this.embed.forward(tgtTokens)\n            let tgtPE = this.posEnc.forward(tgtEmb)\n\n            // 4) Pass through decoder stack (with cross-attention)\n            let decOut = tgtPE\n            i = 0\n            while (i < this.decoderBlocks.length) {\n                decOut = this.decoderBlocks[i].forward(decOut, encOut)\n                i++\n            }\n            this.lastDecoderOut = decOut\n\n            // 5) Final projection to vocab logits\n            let logits = this.finalLinear.forward(decOut)\n            return logits\n        }\n\n        backward(gradOut: Tensor): { dSrc: Tensor, dTgt: Tensor } {\n            // 1) Backprop through final linear projection\n            // Use the same input x that was used in forward (adjust name if different)\n            const linGrad = this.finalLinear.backward(gradOut, this.lastDecoderOut)\n            let dDec: Tensor = linGrad.dx  // [batch, tgtSeq, embedDim]\n\n            // 2) Backprop through decoder stack (right-to-left)\n            let i = this.decoderBlocks.length - 1\n\n            // Accumulate gradients w.r.t. encoder output from all decoder blocks\n            let dEncAccum: Tensor = null\n\n            while (i >= 0) {\n                const block = this.decoderBlocks[i]\n\n                // block.backward returns { dX, dEnc }\n                // block.backward returns only dX (Tensor)\n                const dDecIn: Tensor = block.backward(dDec, this.lastEncoderOut)\n\n                // encoder gradient is stored internally\n                const dEncFromBlock: Tensor = block.lastDEnc\n\n\n\n\n                if (dEncAccum == null) {\n                    dEncAccum = dEncFromBlock\n                } else {\n                    const size = dEncAccum.data.length\n                    let j = 0\n                    while (j < size) {\n                        dEncAccum.data[j] += dEncFromBlock.data[j]\n                        j++\n                    }\n                }\n\n                dDec = dDecIn\n                i--\n            }\n\n            // 3) Backprop through encoder stack (right-to-left)\n            let dEnc: Tensor = dEncAccum\n            i = this.encoderBlocks.length - 1\n            while (i >= 0) {\n                dEnc = this.encoderBlocks[i].backward(dEnc)\n                i--\n            }\n\n            // 4) Backprop through positional encoding (source path)\n            const dSrcPE: Tensor = this.posEnc.backward(dEnc)\n\n            // 5) Backprop through embedding (source tokens)\n            const dSrc: Tensor = this.embed.backward(dSrcPE)\n\n            // 6) Backprop through positional encoding (target path)\n            const dTgtPE: Tensor = this.posEnc.backward(dDec)\n\n            // 7) Backprop through embedding (target tokens)\n            const dTgt: Tensor = this.embed.backward(dTgtPE)\n\n            return {\n                dSrc: dSrc,\n                dTgt: dTgt\n            }\n        }\n    }\n}","fasttensor.ts":"namespace TorchNew {\n    export interface FastShape {\n        rows: number;\n        columns: number;\n    }\n\n    type Function = (x: number) => number\n\n    /**\n        * Represents a multi-dimensional tensor for number[][] computations.\n        */\n    export class FastTensor {\n        shape: FastShape\n\n        /** \n         * The Data Of the Tensor\n        */\n        data: number[][];\n        /**\n        * Creates a new tensor from a 2D `Matrix`.\n        * @param data A 2D `Matrix` representing the tensor values.\n        */\n        constructor(data: number[][]) {\n            this.data = data;\n            this.shape = { rows: data.length, columns: data[0].length }\n        }\n\n        /**\n         * Returns a new tensor with each element rounded to the nearest integer.\n         *\n         * @returns {FastTensor} A tensor with rounded values.\n         */\n        round(): FastTensor {\n            return this.applyFunction((x:number) => Math.round(x))\n        }\n\n        /**\n         * Returns a new tensor with each element rounded down to the nearest whole number.\n         *\n         * @returns {FastTensor} A tensor with floored values.\n         */\n        floor(): FastTensor {\n            return this.applyFunction((x:number) => Math.floor(x))\n        }\n\n        /**\n         * Flattens the 2D tensor into a 1D array in row-major order.\n         *\n         * @returns {number[]} A flat array containing all elements of the tensor.\n         */\n        flat(): number[] {\n            let data: number[] = []\n            this.data.forEach((a) => a.forEach((b) => data.push(b)))\n            return data\n        }\n\n        /**\n        * Performs matrix multiplication (A * B) and returns the resulting tensor.\n        * @param other The tensor to multiply with.\n        * @returns The resulting tensor, or `null` if dimensions do not match.\n        */\n        matmul(other: FastTensor): FastTensor | null {\n            let temp1 = this.data; // Ensure a true copy\n            let temp2 = other.data; // Prevent referencing original tensor\n            let rowsA = temp1.length;\n            let colsA = temp1[0].length;\n            let rowsB = temp2.length;\n            let colsB = temp2[0].length;\n\n            if (colsA !== rowsB) {\n                return null; // Dimension mismatch\n            }\n\n            let result: number[][] = [];\n\n            // Optimized number[] multiplication\n            for (let r = 0; r < rowsA; r++) { // Process row-wise first\n                for (let i = 0; i < colsA; i++) {\n                    let temp3 = temp1[r][i]; // Store lookup value for row\n                    for (let c = 0; c < colsB; c++) {\n                        result[r][c] += temp3 * temp2[i][c]; // Perform multiplication\n                    }\n                }\n            }\n            return new FastTensor(result);\n        }\n        /**\n        * Applies a function to every element in the tensor and returns a new transformed tensor.\n        * @param func The function to apply to each tensor element.\n        * @returns A new tensor with transformed values.\n        */\n        applyFunction(func: Function): FastTensor {\n            let data = this.data;\n            let result = data.map(row => row.map(func)); // Direct transformation without extra storage\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        * Adds another tensor element-wise and returns the resulting tensor.\n        * @param other The tensor to add.\n        * @returns The resulting tensor after addition.\n        */\n        add(other: FastTensor): FastTensor {\n            let rows = Math.min(this.data.length, other.data.length);\n            let cols = Math.min(this.data[0].length, other.data[0].length);\n\n            // Manual preallocation to prevent dynamic resizing overhead\n            let result: number[][] = [];\n            let data1 = this.data;\n            let data2 = other.data;\n            // Optimized addition loop\n            for (let r = 0; r < rows; r++) {\n                for (let c = 0; c < cols; c++) {\n                    result[r][c] = data1[r][c] + data2[r][c]; // Direct assignment avoids push overhead\n                }\n            }\n\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        *Subtracts another tensor element-wise and returns the resulting tensor.\n        * @param other The tensor to subtract.\n        * @returns The resulting tensor after subtraction.\n        */\n        sub(other: FastTensor): FastTensor {\n            let rows = Math.min(this.data.length, other.data.length);\n            let cols = Math.min(this.data[0].length, other.data[0].length);\n\n            // Manual array allocation without `new Array()`\n            let result: number[][] = [];\n            for (let r = 0; r < rows; r++) {\n                result[r] = [];  // No `new Array()`, just an empty array\n                for (let c = 0; c < cols; c++) {\n                    result[r][c] = this.data[r][c] - other.data[r][c];\n                }\n            }\n\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        * Computes the sum of all elements in the tensor.\n        * @returns The sum of all tensor elements.\n        */\n        sum(): number {\n            return this.data.reduce((acc: number, row: number[]) => acc + row.reduce((rowAcc: number, value: number) => rowAcc + value, 0), 0);\n        }\n    }\n}","FastLinear.ts":"namespace TorchNew {\r\n\r\n    export class FastLinear {\r\n        inFeatures: number\r\n        outFeatures: number\r\n        weight: TorchNew.FastTensor\r\n        bias: TorchNew.FastTensor\r\n        lastInput:FastTensor;\r\n\r\n        constructor(inFeatures: number, outFeatures: number) {\r\n            this.inFeatures = inFeatures\r\n            this.outFeatures = outFeatures\r\n\r\n            // Weight: [outFeatures][inFeatures]\r\n            let w: number[][] = []\r\n            for (let r = 0; r < outFeatures; r++) {\r\n                let row: number[] = []\r\n                for (let c = 0; c < inFeatures; c++) {\r\n                    // Xavier-like init\r\n                    row.push((Math.random() - 0.5) * 2 / Math.sqrt(inFeatures))\r\n                }\r\n                w.push(row)\r\n            }\r\n            this.weight = new TorchNew.FastTensor(w)\r\n\r\n            // Bias: [outFeatures][1] (or just a row vector)\r\n            let b: number[][] = []\r\n            let brow: number[] = []\r\n            for (let i = 0; i < outFeatures; i++) brow.push(0)\r\n            b.push(brow)\r\n            this.bias = new TorchNew.FastTensor(b)\r\n        }\r\n\r\n        // Forward: X (batch Ã— inFeatures) * W^T (inFeatures Ã— outFeatures) + b\r\n        forward(x: TorchNew.FastTensor): TorchNew.FastTensor {\r\n            // x.data: [batch][inFeatures]\r\n            // weight.data: [outFeatures][inFeatures]\r\n            // but matmul expects: A(rowsA Ã— colsA) * B(rowsB Ã— colsB)\r\n            // so we need weight^T: [inFeatures][outFeatures]\r\n\r\n            this.lastInput = x\r\n\r\n            let Wt = this.transpose2D(this.weight.data)\r\n\r\n            // matmul: (batch Ã— inFeatures) * (inFeatures Ã— outFeatures)\r\n            let out = x.matmul(new TorchNew.FastTensor(Wt)) as TorchNew.FastTensor\r\n\r\n            // Add bias row-wise\r\n            let batch = out.data.length\r\n            let outF = this.outFeatures\r\n\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < outF; c++) {\r\n                    out.data[r][c] += this.bias.data[0][c]\r\n                }\r\n            }\r\n\r\n            return out\r\n        }\r\n\r\n        backward(dY: TorchNew.FastTensor, x: TorchNew.FastTensor) {\r\n            // dx = dY @ W\r\n            let dx = dY.matmul(this.weight) as TorchNew.FastTensor\r\n\r\n            // dW = dY^T @ x\r\n            let dYt = new TorchNew.FastTensor(this.transpose2D(dY.data))\r\n            let dW = dYt.matmul(x) as TorchNew.FastTensor\r\n\r\n            // db = sum over batch rows of dY\r\n            let dbRow: number[] = []\r\n            for (let i = 0; i < this.outFeatures; i++) dbRow.push(0)\r\n\r\n            let batch = dY.data.length\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < this.outFeatures; c++) {\r\n                    dbRow[c] += dY.data[r][c]\r\n                }\r\n            }\r\n\r\n            let db = new TorchNew.FastTensor([dbRow])\r\n\r\n            return {\r\n                dx: dx,\r\n                dW: dW,\r\n                db: db\r\n            }\r\n        }\r\n\r\n\r\n\r\n        // Simple 2D transpose helper\r\n        private transpose2D(m: number[][]): number[][] {\r\n            let rows = m.length\r\n            let cols = m[0].length\r\n            let out: number[][] = []\r\n\r\n            for (let c = 0; c < cols; c++) {\r\n                let row: number[] = []\r\n                for (let r = 0; r < rows; r++) {\r\n                    row.push(m[r][c])\r\n                }\r\n                out.push(row)\r\n            }\r\n            return out\r\n        }\r\n    }\r\n}","fastsequtial.ts":"namespace TorchNew {\n\n    export class FastSequential {\n        layers: any[]\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all fast layers\n        forward(x: FastTensor): FastTensor {\n            let out = x\n            for (let i = 0; i < this.layers.length; i++) {\n                out = this.layers[i].forward(out)\n            }\n            return out\n        }\n\n        // Backward pass (reverse order)\n        backward(grad: FastTensor, x: FastTensor): FastTensor {\n            // For FastSequential, we assume each layer stores its own input\n            // OR the user passes the correct x for the first layer.\n            let g = grad\n\n            // We need to track inputs for each layer\n            // So we require each layer to store lastInput during forward\n            for (let i = this.layers.length - 1; i >= 0; i--) {\n                let layer = this.layers[i]\n\n                if (!layer.lastInput) {\n                    // If a layer didn't store its input, we cannot backprop\n                    // Fast layers should ALWAYS store lastInput\n                    console.log(\"FastSequential WARNING: layer missing lastInput\")\n                    g = layer.backward(g)\n                } else {\n                    g = layer.backward(g, layer.lastInput)\n                }\n            }\n\n            return g\n        }\n\n        // Collect parameters from all fast layers\n        parameters(): FastTensor[] {\n            let params: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.weight) params.push(L.weight)\n                if (L.bias) params.push(L.bias)\n            }\n            return params\n        }\n\n        // Collect gradients from all fast layers\n        gradients(): FastTensor[] {\n            let grads: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.dW) grads.push(L.dW)\n                if (L.db) grads.push(L.db)\n            }\n            return grads\n        }\n    }\n}","FastActivaons.ts":"namespace TorchNew.Activations {\n\n    export interface FastActivation {\n        forward(x: FastTensor): FastTensor\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class FastReLU implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            // Store input for backward\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : 0)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            return new FastTensor(\n                x.data.map((row, r) =>\n                    row.map((v, c) => v > 0 ? gradOut.data[r][c] : 0)\n                )\n            )\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class FastSigmoid implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => 1 / (1 + Math.exp(-v)))\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let s = 1 / (1 + Math.exp(-x.data[r][c]))\n                    row.push(gradOut.data[r][c] * s * (1 - s))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class FastTanh implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => {\n                let e2 = Math.exp(2 * v)\n                return (e2 - 1) / (e2 + 1)\n            })\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    let e2 = Math.exp(2 * v)\n                    let t = (e2 - 1) / (e2 + 1)\n                    row.push(gradOut.data[r][c] * (1 - t * t))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class FastLeakyReLU implements FastActivation {\n        alpha: number\n        lastInput: FastTensor\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : this.alpha * v)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    row.push(v > 0 ? gradOut.data[r][c] : this.alpha * gradOut.data[r][c])\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n    }\n}","tofast.ts":"namespace TorchNew {\n\n    export function ToFast(t: Tensor): FastTensor {\n        // Reject 0D tensors\n        if (t.shape.length === 0) {\n            console.log(\"ToFast ERROR: Cannot convert 0D tensor\")\n            return null\n        }\n\n        // If 1D: [N] â†’ [1][N]\n        if (t.shape.length === 1) {\n            let row: number[] = []\n            for (let i = 0; i < t.data.length; i++) {\n                row.push(t.data[i])\n            }\n            return new FastTensor([row])\n        }\n\n        // If 2D: [A, B] â†’ [A][B]\n        if (t.shape.length === 2) {\n            let rows = t.shape[0]\n            let cols = t.shape[1]\n            let out: number[][] = []\n            let idx = 0\n\n            for (let r = 0; r < rows; r++) {\n                let row: number[] = []\n                for (let c = 0; c < cols; c++) {\n                    row.push(t.data[idx])\n                    idx++\n                }\n                out.push(row)\n            }\n\n            return new FastTensor(out)\n        }\n\n        // ND case: flatten all dims except last\n        // shape [D1, D2, ..., Dk, F] â†’ [D1*D2*...*Dk][F]\n        let lastDim = t.shape[t.shape.length - 1]\n        let batch = 1\n\n        for (let i = 0; i < t.shape.length - 1; i++) {\n            batch *= t.shape[i]\n        }\n\n        let out: number[][] = []\n        let idx = 0\n\n        for (let r = 0; r < batch; r++) {\n            let row: number[] = []\n            for (let c = 0; c < lastDim; c++) {\n                row.push(t.data[idx])\n                idx++\n            }\n            out.push(row)\n        }\n\n        return new FastTensor(out)\n    }\n}","fromfast.ts":"namespace TorchNew {\n\n    // Convert FastTensor (2D) â†’ full Tensor (ND) with a given shape\n    export function FromFast(ft: FastTensor, shape: number[]): Tensor {\n        // Validate shape\n        if (shape.length === 0) {\n            console.log(\"FromFast ERROR: Cannot create 0D tensor\")\n            return null\n        }\n\n        // Flatten FastTensor.data (number[][]) into number[]\n        let flat: number[] = []\n        for (let r = 0; r < ft.data.length; r++) {\n            for (let c = 0; c < ft.data[0].length; c++) {\n                flat.push(ft.data[r][c])\n            }\n        }\n\n        // Validate total size\n        let expected = 1\n        for (let i = 0; i < shape.length; i++) {\n            expected *= shape[i]\n        }\n\n        if (expected !== flat.length) {\n            console.log(\"FromFast ERROR: Shape mismatch. Expected size \" + expected + \" but got \" + flat.length)\n            return null\n        }\n\n        // Create full Tensor\n        return new Tensor(flat, shape.slice(0))\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\",\n        \"linear.ts\",\n        \"cnn.ts\",\n        \"adam.ts\",\n        \"activactions.ts\",\n        \"softmax.ts\",\n        \"sequtial.ts\",\n        \"embedding.ts\",\n        \"avgpoolnd.ts\",\n        \"flatten.ts\",\n        \"layernorm.ts\",\n        \"dropout.ts\",\n        \"crossentropyloss.ts\",\n        \"positalencoding.ts\",\n        \"maxpoolnd.ts\",\n        \"convtransposend.ts\",\n        \"rnn.ts\",\n        \"gru.ts\",\n        \"lstm.ts\",\n        \"residual.ts\",\n        \"feedforward.ts\",\n        \"multiheadattention.ts\",\n        \"tranformerencoder.ts\",\n        \"tranformerdecoder.ts\",\n        \"tranformermodel.ts\",\n        \"fasttensor.ts\",\n        \"FastLinear.ts\",\n        \"fastsequtial.ts\",\n        \"FastActivaons.ts\",\n        \"tofast.ts\",\n        \"fromfast.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}},{"timestamp":1771528330927,"editorVersion":"4.0.7","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"></block></xml>","main.ts":"","README.md":" ","assets.json":"","tensor.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0,size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Tensor {\n        data: number[]\n        shape: number[]\n        strides: number[]\n\n        constructor(data: number[], shape: number[]) {\n            this.data = data\n            this.shape = shape\n            this.strides = Tensor.computeStrides(shape)\n        }\n\n        // Compute strides for ND indexing\n        static computeStrides(shape: number[]): number[] {\n            let n = shape.length\n            let strides = alloc(n)\n            let acc = 1\n            let i = n - 1\n\n            while (i >= 0) {\n                strides[i] = acc\n                acc *= shape[i]\n                i--\n            }\n\n            return strides\n        }\n\n        // Convert ND index â†’ flat index\n        index(indices: number[]): number {\n            let offset = 0\n            let i = 0\n\n            while (i < indices.length) {\n                offset += indices[i] * this.strides[i]\n                i++\n            }\n\n            return offset\n        }\n\n        get(indices: number[]): number {\n            return this.data[this.index(indices)]\n        }\n\n        set(indices: number[], value: number): void {\n            this.data[this.index(indices)] = value\n        }\n\n        reshape(newShape: number[]): Tensor {\n            let newSize = 1\n            let i = 0\n\n            while (i < newShape.length) {\n                newSize *= newShape[i]\n                i++\n            }\n\n            if (newSize != this.data.length) {\n                return null\n            }\n\n            return new Tensor(this.data.slice(0), newShape.slice(0))\n        }\n\n        static unravelIndex(flat: number, shape: number[]): number[] {\n            let rank = shape.length\n            let out = alloc(rank)\n            let i = rank - 1\n\n            while (i >= 0) {\n                let dim = shape[i]\n                out[i] = flat % dim\n                flat = Math.idiv(flat, dim)\n                i--\n            }\n\n            return out\n        }\n\n        transpose(a: number, b: number): Tensor {\n            let newShape = this.shape.slice(0)\n            let t = newShape[a]\n            newShape[a] = newShape[b]\n            newShape[b] = t\n\n            let out = alloc(this.data.length)\n            let outTensor = new Tensor(out, newShape)\n\n            let total = this.data.length\n            let flat = 0\n\n            while (flat < total) {\n                let idx = Tensor.unravelIndex(flat, this.shape)\n                let tmp = idx[a]\n                idx[a] = idx[b]\n                idx[b] = tmp\n\n                outTensor.set(idx, this.data[flat])\n                flat++\n            }\n\n            return outTensor\n        }\n\n        add(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] + other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        sub(other: Tensor): Tensor {\n            let size = this.data.length\n            let out = alloc(size)\n            let i = 0\n\n            while (i < size) {\n                out[i] = this.data[i] - other.data[i]\n                i++\n            }\n\n            return new Tensor(out, this.shape.slice(0))\n        }\n\n        extract2D(prefix: number[], rows: number, cols: number): number[] {\n            let out = alloc(rows * cols)\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    out[r * cols + c] = this.get(idx)\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n        write2D(prefix: number[], rows: number, cols: number, src: number[]): void {\n            let r = 0\n\n            while (r < rows) {\n                let c = 0\n                while (c < cols) {\n                    let idx = prefix.slice(0)\n                    idx.push(r)\n                    idx.push(c)\n                    this.set(idx, src[r * cols + c])\n                    c++\n                }\n                r++\n            }\n        }\n\n        static matmul2D(a: number[], aRows: number, aCols: number,\n            b: number[], bRows: number, bCols: number): number[] {\n\n            let out = alloc(aRows * bCols)\n            let r = 0\n\n            while (r < aRows) {\n                let c = 0\n                while (c < bCols) {\n                    let sum = 0\n                    let k = 0\n\n                    while (k < aCols) {\n                        sum += a[r * aCols + k] * b[k * bCols + c]\n                        k++\n                    }\n\n                    out[r * bCols + c] = sum\n                    c++\n                }\n                r++\n            }\n\n            return out\n        }\n\n\n\n        matmul(other: Tensor): Tensor {\n            let aShape = this.shape\n            let bShape = other.shape\n\n            let aRank = aShape.length\n            let bRank = bShape.length\n\n            // Last two dims must be matrix dims\n            let aRows = aShape[aRank - 2]\n            let aCols = aShape[aRank - 1]\n            let bRows = bShape[bRank - 2]\n            let bCols = bShape[bRank - 1]\n\n            if (aCols != bRows) {\n                return null\n            }\n\n            // Determine batch shape (everything except last 2 dims)\n            let batchRank = aRank - 2\n            let batchShape = Array.repeat(0,batchRank)\n            let i = 0\n\n            while (i < batchRank) {\n                batchShape[i] = aShape[i]\n                i++\n            }\n\n            // Output shape = batch + [aRows, bCols]\n            let outShape = Array.repeat(0,batchRank + 2)\n            i = 0\n\n            while (i < batchRank) {\n                outShape[i] = batchShape[i]\n                i++\n            }\n\n            outShape[batchRank] = aRows\n            outShape[batchRank + 1] = bCols\n\n            // Allocate output tensor\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Compute number of batches\n            let batchSize = 1\n            i = 0\n            while (i < batchRank) {\n                batchSize *= batchShape[i]\n                i++\n            }\n\n            // Loop over all batches\n            let bIndex = 0\n\n            while (bIndex < batchSize) {\n                // Convert flat batch index â†’ ND prefix\n                let prefix = Tensor.unravelIndex(bIndex, batchShape)\n\n                // Extract 2D slices\n                let a2 = this.extract2D(prefix, aRows, aCols)\n                let b2 = other.extract2D(prefix, bRows, bCols)\n\n                // Multiply 2D slices\n                let result2 = Tensor.matmul2D(a2, aRows, aCols, b2, bRows, bCols)\n\n                // Write result back into output tensor\n                out.write2D(prefix, aRows, bCols, result2)\n\n                bIndex++\n            }\n\n            return out\n        }\n    }\n}","linear.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Linear {\n        inFeatures: number\n        outFeatures: number\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inFeatures: number, outFeatures: number) {\n            this.inFeatures = inFeatures\n            this.outFeatures = outFeatures\n\n            // Weight shape: [outFeatures, inFeatures]\n            let wsize = outFeatures * inFeatures\n            let wdata = alloc(wsize)\n\n            // Xavier-like init (simple version)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 / Math.sqrt(inFeatures)\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [outFeatures, inFeatures])\n\n            // Bias shape: [outFeatures]\n            let bdata = alloc(outFeatures)\n            i = 0\n            while (i < outFeatures) {\n                bdata[i] = 0\n                i++\n            }\n\n            this.bias = new Tensor(bdata, [outFeatures])\n        }\n\n        // Forward pass: x @ W^T + b\n        forward(x: Tensor): Tensor {\n            // x shape: [..., inFeatures]\n            // weight shape: [outFeatures, inFeatures]\n            // Need W^T shape: [inFeatures, outFeatures]\n\n            let Wt = this.weight.transpose(0, 1)\n\n            // Matmul: [..., inFeatures] Ã— [inFeatures, outFeatures]\n            let out = x.matmul(Wt)\n\n            // Add bias (broadcast over batch dims)\n            let size = out.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                out.data[i] += this.bias.data[idx]\n                i++\n            }\n\n            return out\n        }\n\n        backward(dY: Tensor, x: Tensor): { dx: Tensor, dW: Tensor, db: Tensor } {\n            // dY shape: [..., outFeatures]\n            // x shape:  [..., inFeatures]\n            // W shape:  [outFeatures, inFeatures]\n\n            // 1. dx = dY @ W\n            let dx = dY.matmul(this.weight)\n\n            // 2. dW = dY^T @ x\n            let dYt = dY.transpose(dY.shape.length - 2, dY.shape.length - 1)\n            let dW = dYt.matmul(x)\n\n            // 3. db = sum(dY over all batch dims)\n            let dbData = alloc(this.outFeatures)\n            let size = dY.data.length\n            let lastDim = this.outFeatures\n            let i = 0\n\n            while (i < size) {\n                let idx = i % lastDim\n                dbData[idx] += dY.data[i]\n                i++\n            }\n\n            let db = new Tensor(dbData, [this.outFeatures])\n\n            return {\n                dx: dx,\n                dW: dW,\n                db: db\n            }\n        }\n    }\n}","cnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n\n    export class ConvND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        padding: number[]\n        stride: number[]\n        weight: Tensor\n        bias: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], padding: number[], stride: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.padding = padding.slice(0)\n            this.stride = stride.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [outChannels, inChannels, ...kernelShape]\n            let wsize = outChannels * inChannels\n            let i = 0\n            while (i < dims) {\n                wsize *= kernelShape[i]\n                i++\n            }\n\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            let wshape = alloc(2 + dims)\n            wshape[0] = outChannels\n            wshape[1] = inChannels\n            i = 0\n            while (i < dims) {\n                wshape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            this.weight = new Tensor(wdata, wshape)\n\n            // Bias shape: [outChannels]\n            let bdata = alloc(outChannels)\n            this.bias = new Tensor(bdata, [outChannels])\n        }\n\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, inChannels, ...spatial]\n            let batch = x.shape[0]\n            let cIn = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temporary index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n\n            // Convolution loops\n            let b = 0\n            while (b < batch) {\n                idxOut[0] = b\n                idxX[0] = b\n\n                let oc = 0\n                while (oc < this.outChannels) {\n                    idxOut[1] = oc\n                    idxW[0] = oc\n\n                    // Iterate over output spatial dims\n                    this._loopOutputDims(\n                        0, dims,\n                        idxOut, outSpatial,\n                        x, out,\n                        idxX, idxW,\n                        oc\n                    )\n\n                    oc++\n                }\n                b++\n            }\n\n            return out\n        }\n\n        // Iterates over output spatial dims (non-recursive wrapper)\n        _loopOutputDims(dim: number, dims: number,\n            idxOut: number[], outSpatial: number[],\n            x: Tensor, out: Tensor,\n            idxX: number[], idxW: number[],\n            oc: number): void {\n\n            // We simulate recursion manually using a stack\n            let stackDim = alloc(dims)\n            let stackPos = alloc(dims)\n\n            let d = 0\n            while (d < dims) {\n                stackDim[d] = 0\n                stackPos[d] = 0\n                d++\n            }\n\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute convolution at this output position\n                    let sum = this.bias.data[oc]\n\n                    let ic = 0\n                    while (ic < this.inChannels) {\n                        idxX[1] = ic\n                        idxW[1] = ic\n\n                        sum = this._applyKernel(\n                            dims, idxOut, idxX, idxW,\n                            x, sum\n                        )\n\n                        ic++\n                    }\n\n                    out.set(idxOut, sum)\n\n                    level--\n                    continue\n                }\n\n                if (stackPos[level] < outSpatial[level]) {\n                    idxOut[2 + level] = stackPos[level]\n                    stackPos[level]++\n                    level++\n                } else {\n                    stackPos[level] = 0\n                    level--\n                }\n            }\n        }\n\n        // Applies kernel at a single output position\n        _applyKernel(dims: number,\n            idxOut: number[], idxX: number[], idxW: number[],\n            x: Tensor, sum: number): number {\n\n            // Manual ND kernel loops\n            let kpos = alloc(dims)\n            let level = 0\n\n            while (level >= 0) {\n                if (level == dims) {\n                    // Compute input index\n                    let inside = true\n                    let d = 0\n                    while (d < dims) {\n                        let outPos = idxOut[2 + d]\n                        let k = kpos[d]\n                        let pad = this.padding[d]\n                        let s = this.stride[d]\n\n                        let inPos = outPos * s + k - pad\n                        idxX[2 + d] = inPos\n\n                        if (inPos < 0 || inPos >= x.shape[2 + d]) {\n                            inside = false\n                        }\n\n                        idxW[2 + d] = k\n                        d++\n                    }\n\n                    if (inside) {\n                        let xVal = x.get(idxX)\n                        let wVal = this.weight.get(idxW)\n                        sum += xVal * wVal\n                    }\n\n                    level--\n                    continue\n                }\n\n                if (kpos[level] < this.kernelShape[level]) {\n                    kpos[level]++\n                    level++\n                } else {\n                    kpos[level] = 0\n                    level--\n                }\n            }\n\n            return sum\n        }\n\n        backward(dY: Tensor, x: Tensor): { dX: Tensor, dW: Tensor, dB: Tensor } {\n            // Shapes:\n            // x:  [batch, inChannels, ...inSpatial]\n            // dY: [batch, outChannels, ...outSpatial]\n            // W:  [outChannels, inChannels, ...kernelShape]\n\n            let batch = x.shape[0]\n            let cIn = this.inChannels\n            let cOut = this.outChannels\n            let dims = this.kernelShape.length\n\n            // Allocate dX (same shape as x)\n            let sizeDX = 1\n            let i = 0\n            while (i < x.shape.length) {\n                sizeDX *= x.shape[i]\n                i++\n            }\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Allocate dW (same shape as weight)\n            let sizeDW = 1\n            i = 0\n            while (i < this.weight.shape.length) {\n                sizeDW *= this.weight.shape[i]\n                i++\n            }\n            let dWdata = alloc(sizeDW)\n            let dW = new Tensor(dWdata, this.weight.shape.slice(0))\n\n            // Allocate dB (same shape as bias)\n            let dBdata = alloc(cOut)\n            let dB = new Tensor(dBdata, [cOut])\n\n            // Temp index arrays\n            let idxY = alloc(2 + dims)   // [b, oc, o...]\n            let idxX = alloc(2 + dims)   // [b, ic, i...]\n            let idxW = alloc(2 + dims)   // [oc, ic, k...]\n            let kpos = alloc(dims)       // kernel position per dim\n\n            // Total elements in dY\n            let sizeDY = dY.data.length\n            let p = 0\n\n            while (p < sizeDY) {\n                // Unravel flat index p into dY indices\n                idxY = Tensor.unravelIndex(p, dY.shape)\n                let b = idxY[0]\n                let oc = idxY[1]\n\n                let grad = dY.data[p]\n\n                // 1) dB[oc] += dY[b, oc, ...]\n                dB.data[oc] += grad\n\n                // 2) Loop over input channels and kernel positions\n                let ic = 0\n                while (ic < cIn) {\n                    idxX[0] = b\n                    idxX[1] = ic\n                    idxW[0] = oc\n                    idxW[1] = ic\n\n                    // ND kernel loop using manual stack\n                    let level = 0\n                    let kmax = this.kernelShape\n                    let kcur = kpos\n                    let reset = 0\n                    while (reset < dims) {\n                        kcur[reset] = 0\n                        reset++\n                    }\n\n                    while (level >= 0) {\n                        if (level == dims) {\n                            // Compute input spatial index for this kernel position\n                            let inside = true\n                            let d = 0\n                            while (d < dims) {\n                                let oPos = idxY[2 + d]\n                                let k = kcur[d]\n                                let pad = this.padding[d]\n                                let s = this.stride[d]\n\n                                let iPos = oPos * s + k - pad\n                                idxX[2 + d] = iPos\n                                idxW[2 + d] = k\n\n                                if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                    inside = false\n                                }\n                                d++\n                            }\n\n                            if (inside) {\n                                // dW[oc, ic, k...] += dY * x[b, ic, i...]\n                                let xVal = x.get(idxX)\n                                let wGradIndex = dW.index(idxW)\n                                dW.data[wGradIndex] += grad * xVal\n\n                                // dX[b, ic, i...] += dY * W[oc, ic, k...]\n                                let wVal = this.weight.get(idxW)\n                                let xGradIndex = dX.index(idxX)\n                                dX.data[xGradIndex] += grad * wVal\n                            }\n\n                            level--\n                            continue\n                        }\n\n                        if (kcur[level] < kmax[level]) {\n                            kcur[level]++\n                            level++\n                        } else {\n                            kcur[level] = 0\n                            level--\n                        }\n                    }\n\n                    ic++\n                }\n\n                p++\n            }\n\n            return {\n                dX: dX,\n                dW: dW,\n                dB: dB\n            }\n        }\n    }\n\n\n\n}","adam.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Adam {\n        lr: number\n        beta1: number\n        beta2: number\n        eps: number\n        t: number\n\n        // Moment buffers: arrays of Tensors\n        m: Tensor[]\n        v: Tensor[]\n\n        constructor(lr: number, beta1: number, beta2: number, eps: number) {\n            this.lr = lr\n            this.beta1 = beta1\n            this.beta2 = beta2\n            this.eps = eps\n            this.t = 0\n\n            this.m = []\n            this.v = []\n        }\n\n        // Initialize moment buffers for a parameter list\n        init(params: Tensor[]): void {\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n\n                let size = p.data.length\n                let mdata = alloc(size)\n                let vdata = alloc(size)\n\n                this.m.push(new Tensor(mdata, p.shape.slice(0)))\n                this.v.push(new Tensor(vdata, p.shape.slice(0)))\n\n                i++\n            }\n        }\n\n        // Apply Adam update to a list of (param, grad) pairs\n        step(params: Tensor[], grads: Tensor[]): void {\n            this.t++\n\n            let lr = this.lr\n            let b1 = this.beta1\n            let b2 = this.beta2\n            let eps = this.eps\n\n            let t = this.t\n\n            let i = 0\n            while (i < params.length) {\n                let p = params[i]\n                let g = grads[i]\n                let m = this.m[i]\n                let v = this.v[i]\n\n                let size = p.data.length\n                let j = 0\n\n                // Update each element\n                while (j < size) {\n                    let grad = g.data[j]\n\n                    // m_t = b1*m + (1-b1)*g\n                    m.data[j] = b1 * m.data[j] + (1 - b1) * grad\n\n                    // v_t = b2*v + (1-b2)*g^2\n                    v.data[j] = b2 * v.data[j] + (1 - b2) * grad * grad\n\n                    // Bias correction\n                    let mHat = m.data[j] / (1 - Math.pow(b1, t))\n                    let vHat = v.data[j] / (1 - Math.pow(b2, t))\n\n                    // Parameter update\n                    p.data[j] -= lr * mHat / (Math.sqrt(vHat) + eps)\n\n                    j++\n                }\n\n                i++\n            }\n        }\n    }\n}","activactions.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n\n        return arr\n    }\n\n    // Base interface (not required, but helpful)\n    export interface Activation {\n        forward(x: Tensor): Tensor\n        backward(x: Tensor, gradOut: Tensor): Tensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class ReLU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : 0\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : 0\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class Sigmoid implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = 1 / (1 + Math.exp(-v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let s = 1 / (1 + Math.exp(-x.data[i]))\n                grad[i] = gradOut.data[i] * s * (1 - s)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class Tanh implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                outData[i] = fastTanh(x.data[i])\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let t = fastTanh(x.data[i])\n                grad[i] = gradOut.data[i] * (1 - t * t)\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // GELU (approximate version)\n    // ---------------------------------------------------------\n    export class GELU implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                outData[i] = 0.5 * v * (1 + fastTanh(inner))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let c = 0.044715\n                let k = Math.sqrt(2 / Math.PI)\n                let inner = k * (v + c * v * v * v)\n                let t = fastTanh(inner)\n                let sech2 = 1 - t * t\n                let innerDeriv = k * (1 + 3 * c * v * v)\n                let geluDeriv = 0.5 * (1 + t) + 0.5 * v * sech2 * innerDeriv\n\n                grad[i] = gradOut.data[i] * geluDeriv\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class LeakyReLU implements Activation {\n        alpha: number\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = v > 0 ? v : this.alpha * v\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                grad[i] = x.data[i] > 0 ? gradOut.data[i] : this.alpha * gradOut.data[i]\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n\n    // ---------------------------------------------------------\n    // Softplus\n    // ---------------------------------------------------------\n    export class Softplus implements Activation {\n\n        forward(x: Tensor): Tensor {\n            let outData = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                outData[i] = Math.log(1 + Math.exp(v))\n                i++\n            }\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let grad = alloc(x.data.length)\n            let i = 0\n            while (i < x.data.length) {\n                let v = x.data[i]\n                let s = 1 / (1 + Math.exp(-v)) // sigmoid\n                grad[i] = gradOut.data[i] * s\n                i++\n            }\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","softmax.ts":"namespace TorchNew.Activations {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Softmax {\n\n        // Forward: softmax along last dimension\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                // Compute index of start of this row\n                let rowStart = i - (i % lastDim)\n\n                // 1. Find max for numerical stability\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < lastDim) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // 2. Compute exp(x - max)\n                let sumExp = 0\n                j = 0\n                while (j < lastDim) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // 3. Normalize\n                j = 0\n                while (j < lastDim) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                // Move to next row\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // Backward: dSoftmax = softmax * (gradOut - sum(gradOut * softmax))\n        backward(x: Tensor, gradOut: Tensor): Tensor {\n            let size = x.data.length\n            let lastDim = x.shape[x.shape.length - 1]\n\n            let grad = alloc(size)\n\n            // First compute softmax(x)\n            let soft = this.forward(x)\n\n            let i = 0\n            while (i < size) {\n                let rowStart = i - (i % lastDim)\n\n                // Compute dot(gradOut, softmax)\n                let dot = 0\n                let j = 0\n                while (j < lastDim) {\n                    dot += gradOut.data[rowStart + j] * soft.data[rowStart + j]\n                    j++\n                }\n\n                // Compute gradient\n                j = 0\n                while (j < lastDim) {\n                    let s = soft.data[rowStart + j]\n                    grad[rowStart + j] = s * (gradOut.data[rowStart + j] - dot)\n                    j++\n                }\n\n                i = rowStart + lastDim\n            }\n\n            return new Tensor(grad, x.shape.slice(0))\n        }\n    }\n}","sequtial.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Sequential {\n        layers: any[]   // Each layer must have forward(), backward(), and optionally parameters()\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all layers\n        forward(x: Tensor): Tensor {\n            let out = x\n            let i = 0\n            while (i < this.layers.length) {\n                out = this.layers[i].forward(out)\n                i++\n            }\n            return out\n        }\n\n        // Backward pass through all layers (reverse order)\n        backward(grad: Tensor): Tensor {\n            let g = grad\n            let i = this.layers.length - 1\n            while (i >= 0) {\n                // Some layers need the original input for backward\n                // So we store last input inside each layer during forward\n                if (this.layers[i].lastInput) {\n                    g = this.layers[i].backward(this.layers[i].lastInput, g)\n                } else {\n                    // Layers like Linear or ConvND already store their own input\n                    g = this.layers[i].backward(g)\n                }\n                i--\n            }\n            return g\n        }\n\n        // Collect all parameters from all layers\n        parameters(): Tensor[] {\n            let params: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.weight) params.push(layer.weight)\n                if (layer.bias) params.push(layer.bias)\n                i++\n            }\n            return params\n        }\n\n        // Collect all gradients from all layers\n        gradients(): Tensor[] {\n            let grads: Tensor[] = []\n            let i = 0\n            while (i < this.layers.length) {\n                let layer = this.layers[i]\n                if (layer.dW) grads.push(layer.dW)\n                if (layer.dB) grads.push(layer.dB)\n                i++\n            }\n            return grads\n        }\n    }\n}","embedding.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Embedding {\n        vocabSize: number\n        embedDim: number\n        weight: Tensor\n        dW: Tensor\n        lastIndices:Tensor;\n\n        constructor(vocabSize: number, embedDim: number) {\n            this.vocabSize = vocabSize\n            this.embedDim = embedDim\n\n            // Weight shape: [vocabSize, embedDim]\n            let wsize = vocabSize * embedDim\n            let wdata = alloc(wsize)\n\n            // Xavier-like init\n            let scale = 1 / Math.sqrt(vocabSize)\n            let i = 0\n            while (i < wsize) {\n                wdata[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wdata, [vocabSize, embedDim])\n\n            // Gradient buffer\n            let dwdata = alloc(wsize)\n            this.dW = new Tensor(dwdata, [vocabSize, embedDim])\n        }\n\n        // Forward: indices -> embedding vectors\n        forward(indices: Tensor): Tensor {\n            // indices is an integer tensor (flat or ND)\n            // output shape = [..., embedDim]\n\n            let outShape = alloc(indices.shape.length + 1)\n            let i = 0\n            while (i < indices.shape.length) {\n                outShape[i] = indices.shape[i]\n                i++\n            }\n            outShape[indices.shape.length] = this.embedDim\n\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Save indices for backward\n            this.lastIndices = indices\n\n            // Fill output\n            let idxCount = indices.data.length\n            let j = 0\n            while (j < idxCount) {\n                let token = indices.data[j]\n\n                // Copy weight[token] into out[j]\n                let baseOut = j * this.embedDim\n                let baseW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    outData[baseOut + k] = this.weight.data[baseW + k]\n                    k++\n                }\n\n                j++\n            }\n\n            return out\n        }\n\n        // Backward: accumulate gradients into dW\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [..., embedDim]\n            // dW[token] += gradOut[i]\n\n            // Zero dW\n            let sizeDW = this.dW.data.length\n            let i = 0\n            while (i < sizeDW) {\n                this.dW.data[i] = 0\n                i++\n            }\n\n            let indices = this.lastIndices\n            let count = indices.data.length\n\n            let j = 0\n            while (j < count) {\n                let token = indices.data[j]\n\n                let baseGrad = j * this.embedDim\n                let baseDW = token * this.embedDim\n\n                let k = 0\n                while (k < this.embedDim) {\n                    this.dW.data[baseDW + k] += gradOut.data[baseGrad + k]\n                    k++\n                }\n\n                j++\n            }\n\n            // Embedding has no gradient wrt input indices\n            return null\n        }\n    }\n}","avgpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        return arr\n    }\n    \n    export class AvgPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n        lastInput:Tensor;\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward pass\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, channels, ...spatial]\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over all output positions\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                // Compute average over kernel window\n                let sum = 0\n                let count = 0\n\n                // ND kernel loop using manual stack\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute input index\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            sum += x.get(idxX)\n                        }\n                        count++\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                out.data[p] = sum / count\n                p++\n            }\n\n            this.lastInput = x\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward pass\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Allocate dX\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, gradOut.shape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n                let g = gradOut.data[p]\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                let count = 1\n                let i = 0\n                while (i < dims) {\n                    count *= this.kernelShape[i]\n                    i++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let idx = dX.index(idxX)\n                            dX.data[idx] += g / count\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","flatten.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Flatten {\n        lastShape: number[]\n\n        constructor() {\n            // No parameters\n        }\n\n        // Forward: reshape input to 2D [batch, -1]\n        forward(x: Tensor): Tensor {\n            this.lastShape = x.shape.slice(0)\n\n            let batch = x.shape[0]\n\n            // Compute flattened size\n            let flatSize = 1\n            let i = 1\n            while (i < x.shape.length) {\n                flatSize *= x.shape[i]\n                i++\n            }\n\n            // Output shape: [batch, flatSize]\n            let outShape = [batch, flatSize]\n\n            // Data is already flat, so we just reuse it\n            let outData = alloc(x.data.length)\n            let j = 0\n            while (j < x.data.length) {\n                outData[j] = x.data[j]\n                j++\n            }\n\n            return new Tensor(outData, outShape)\n        }\n\n        // Backward: reshape gradient back to original shape\n        backward(gradOut: Tensor): Tensor {\n            // gradOut shape: [batch, flatSize]\n            // We reshape back to lastShape\n\n            let total = 1\n            let i = 0\n            while (i < this.lastShape.length) {\n                total *= this.lastShape[i]\n                i++\n            }\n\n            let gradData = alloc(total)\n            let j = 0\n            while (j < total) {\n                gradData[j] = gradOut.data[j]\n                j++\n            }\n\n            return new Tensor(gradData, this.lastShape.slice(0))\n        }\n    }\n}","layernorm.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class LayerNorm {\n        normalizedShape: number[]\n        eps: number\n\n        gamma: Tensor\n        beta: Tensor\n\n        dGamma: Tensor\n        dBeta: Tensor\n\n        lastInput: Tensor\n        lastMean: number[]\n        lastVar: number[]\n        lastNorm: number[]\n\n        constructor(normalizedShape: number[], eps: number = 1e-5) {\n            this.normalizedShape = normalizedShape.slice(0)\n            this.eps = eps\n\n            // Parameter size = product of normalizedShape\n            let size = 1\n            let i = 0\n            while (i < normalizedShape.length) {\n                size *= normalizedShape[i]\n                i++\n            }\n\n            // gamma initialized to 1\n            let gdata = alloc(size)\n            i = 0\n            while (i < size) {\n                gdata[i] = 1\n                i++\n            }\n            this.gamma = new Tensor(gdata, normalizedShape.slice(0))\n\n            // beta initialized to 0\n            let bdata = alloc(size)\n            this.beta = new Tensor(bdata, normalizedShape.slice(0))\n\n            // gradients\n            this.dGamma = new Tensor(alloc(size), normalizedShape.slice(0))\n            this.dBeta = new Tensor(alloc(size), normalizedShape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let dims = this.normalizedShape.length\n            let total = x.data.length\n\n            // Compute size of the normalized block\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            // Number of blocks = total / blockSize\n            let blocks = Math.idiv(total, blockSize)\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, x.shape.slice(0))\n\n            this.lastMean = alloc(blocks)\n            this.lastVar = alloc(blocks)\n            this.lastNorm = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n\n                // Compute mean\n                let mean = 0\n                let j = 0\n                while (j < blockSize) {\n                    mean += x.data[start + j]\n                    j++\n                }\n                mean /= blockSize\n                this.lastMean[b] = mean\n\n                // Compute variance\n                let variance = 0\n                j = 0\n                while (j < blockSize) {\n                    let d = x.data[start + j] - mean\n                    variance += d * d\n                    j++\n                }\n                variance /= blockSize\n                this.lastVar[b] = variance\n\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Normalize + scale + shift\n                j = 0\n                while (j < blockSize) {\n                    let norm = (x.data[start + j] - mean) * invStd\n                    this.lastNorm[start + j] = norm\n\n                    outData[start + j] =\n                        norm * this.gamma.data[j] + this.beta.data[j]\n\n                    j++\n                }\n\n                b++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let total = x.data.length\n\n            let dims = this.normalizedShape.length\n\n            // Compute block size\n            let blockSize = 1\n            let i = 0\n            while (i < dims) {\n                blockSize *= this.normalizedShape[i]\n                i++\n            }\n\n            let blocks = Math.idiv(total, blockSize)\n\n            // Zero gradients\n            let size = this.gamma.data.length\n            i = 0\n            while (i < size) {\n                this.dGamma.data[i] = 0\n                this.dBeta.data[i] = 0\n                i++\n            }\n\n            let dXdata = alloc(total)\n\n            let b = 0\n            while (b < blocks) {\n                let start = b * blockSize\n                let mean = this.lastMean[b]\n                let variance = this.lastVar[b]\n                let invStd = 1 / Math.sqrt(variance + this.eps)\n\n                // Compute dGamma and dBeta\n                let j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    this.dGamma.data[j] += go * norm\n                    this.dBeta.data[j] += go\n\n                    j++\n                }\n\n                // Compute dX\n                // Formula:\n                // dX = (1/N)*gamma*invStd * [N*gradOut - sum(gradOut) - norm*sum(gradOut*norm)]\n                let sumGO = 0\n                let sumGONorm = 0\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    sumGO += go\n                    sumGONorm += go * norm\n                    j++\n                }\n\n                j = 0\n                while (j < blockSize) {\n                    let go = gradOut.data[start + j]\n                    let norm = this.lastNorm[start + j]\n\n                    let term = go * blockSize - sumGO - norm * sumGONorm\n                    term /= blockSize\n\n                    dXdata[start + j] = this.gamma.data[j] * invStd * term\n\n                    j++\n                }\n\n                b++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","dropout.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Dropout {\n        p: number\n        training: boolean\n        lastMask: number[]\n\n        constructor(p: number) {\n            this.p = p\n            this.training = true\n        }\n\n        // Enable/disable training mode\n        train(): void {\n            this.training = true\n        }\n\n        eval(): void {\n            this.training = false\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let size = x.data.length\n            let outData = alloc(size)\n\n            // If not training, dropout does nothing\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    outData[i] = x.data[i]\n                    i++\n                }\n                return new Tensor(outData, x.shape.slice(0))\n            }\n\n            // Training mode: generate mask\n            this.lastMask = alloc(size)\n\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                // 1 = keep, 0 = drop\n                let keep = Math.random() > this.p ? 1 : 0\n                this.lastMask[i] = keep\n\n                outData[i] = x.data[i] * keep * scale\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n            let gradData = alloc(size)\n\n            // If not training, gradient passes unchanged\n            if (!this.training) {\n                let i = 0\n                while (i < size) {\n                    gradData[i] = gradOut.data[i]\n                    i++\n                }\n                return new Tensor(gradData, gradOut.shape.slice(0))\n            }\n\n            // Training mode: apply mask\n            let scale = 1 / (1 - this.p)\n            let i = 0\n            while (i < size) {\n                gradData[i] = gradOut.data[i] * this.lastMask[i] * scale\n                i++\n            }\n\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","crossentropyloss.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class CrossEntropyLoss {\n        lastInput: Tensor\n        lastTarget: Tensor\n        lastSoftmax: Tensor\n\n        constructor() { }\n\n        // ---------------------------------------------------------\n        // Forward: logits + class indices -> scalar loss\n        // ---------------------------------------------------------\n        forward(logits: Tensor, target: Tensor): Tensor {\n            // logits shape: [batch, classes]\n            // target shape: [batch] (integer class indices)\n\n            this.lastInput = logits\n            this.lastTarget = target\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let lossSum = 0\n\n            let i = 0\n            while (i < batch) {\n                // Compute stable max\n                let rowStart = i * classes\n                let maxVal = logits.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = logits.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // Compute log-sum-exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    sumExp += Math.exp(logits.data[rowStart + j] - maxVal)\n                    j++\n                }\n\n                let logSumExp = Math.log(sumExp) + maxVal\n\n                // Pick correct class logit\n                let y = target.data[i]\n                let correctLogit = logits.data[rowStart + y]\n\n                // Loss = logSumExp - correctLogit\n                lossSum += logSumExp - correctLogit\n\n                i++\n            }\n\n            let loss = lossSum / batch\n\n            // Return scalar tensor\n            return new Tensor([loss], [1])\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dL/dlogits = softmax(logits) - one_hot(target)\n        // ---------------------------------------------------------\n        backward(): Tensor {\n            let logits = this.lastInput\n            let target = this.lastTarget\n\n            let batch = logits.shape[0]\n            let classes = logits.shape[1]\n\n            let gradData = alloc(logits.data.length)\n\n            // Compute softmax for backward\n            let soft = this.softmax(logits)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n                let y = target.data[i]\n\n                let j = 0\n                while (j < classes) {\n                    let s = soft.data[rowStart + j]\n                    let indicator = (j == y) ? 1 : 0\n                    gradData[rowStart + j] = (s - indicator) / batch\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(gradData, logits.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Internal stable softmax (same as Softmax layer)\n        // ---------------------------------------------------------\n        softmax(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let classes = x.shape[1]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let rowStart = i * classes\n\n                // max\n                let maxVal = x.data[rowStart]\n                let j = 1\n                while (j < classes) {\n                    let v = x.data[rowStart + j]\n                    if (v > maxVal) maxVal = v\n                    j++\n                }\n\n                // exp\n                let sumExp = 0\n                j = 0\n                while (j < classes) {\n                    let e = Math.exp(x.data[rowStart + j] - maxVal)\n                    outData[rowStart + j] = e\n                    sumExp += e\n                    j++\n                }\n\n                // normalize\n                j = 0\n                while (j < classes) {\n                    outData[rowStart + j] /= sumExp\n                    j++\n                }\n\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","positalencoding.ts":"// Add your code here\nnamespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class PositionalEncoding {\n        maxLen: number\n        dim: number\n        encoding: Tensor\n\n        constructor(maxLen: number, dim: number) {\n            this.maxLen = maxLen\n            this.dim = dim\n\n            // Precompute positional encodings\n            let data = alloc(maxLen * dim)\n\n            let pos = 0\n            while (pos < maxLen) {\n                let i = 0\n                while (i < dim) {\n                    let angle = pos / Math.pow(10000, (2 * Math.idiv(i, 2)) / dim)\n\n                    if (i % 2 == 0) {\n                        data[pos * dim + i] = Math.sin(angle)\n                    } else {\n                        data[pos * dim + i] = Math.cos(angle)\n                    }\n\n                    i++\n                }\n                pos++\n            }\n\n            this.encoding = new Tensor(data, [maxLen, dim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward: add positional encoding to input\n        // x shape: [batch, seq, dim]\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let i = 0\n            while (i < batch) {\n                let j = 0\n                while (j < seq) {\n                    let k = 0\n                    while (k < dim) {\n                        let idx = i * seq * dim + j * dim + k\n                        let pe = this.encoding.data[j * dim + k]\n                        outData[idx] = x.data[idx] + pe\n                        k++\n                    }\n                    j++\n                }\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: gradient passes through unchanged\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradData = alloc(gradOut.data.length)\n            let i = 0\n            while (i < gradOut.data.length) {\n                gradData[i] = gradOut.data[i]\n                i++\n            }\n            return new Tensor(gradData, gradOut.shape.slice(0))\n        }\n    }\n}","maxpoolnd.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class MaxPoolND {\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        lastInput: Tensor\n        lastMaxIndex: number[]   // stores argmax positions for backward\n\n        constructor(kernelShape: number[], stride: number[], padding: number[]) {\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let channels = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let pad = this.padding[i]\n                let s = this.stride[i]\n\n                outSpatial[i] = Math.idiv(inSize + 2 * pad - k, s) + 1\n                i++\n            }\n\n            // Output shape: [batch, channels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = channels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Store argmax indices for backward\n            this.lastMaxIndex = alloc(total)\n\n            // Temp index arrays\n            let idxOut = alloc(2 + dims)\n            let idxX = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            let p = 0\n            let sizeOut = out.data.length\n\n            while (p < sizeOut) {\n                idxOut = Tensor.unravelIndex(p, outShape)\n\n                let b = idxOut[0]\n                let c = idxOut[1]\n\n                let maxVal = -1e30\n                let maxIndex = -1\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d = 0\n                        while (d < dims) {\n                            let oPos = idxOut[2 + d]\n                            let k = kpos[d]\n                            let pad = this.padding[d]\n                            let s = this.stride[d]\n\n                            let iPos = oPos * s + k - pad\n                            idxX[2 + d] = iPos\n\n                            if (iPos < 0 || iPos >= x.shape[2 + d]) {\n                                inside = false\n                            }\n                            d++\n                        }\n\n                        idxX[0] = b\n                        idxX[1] = c\n\n                        if (inside) {\n                            let flat = x.index(idxX)\n                            let v = x.data[flat]\n                            if (v > maxVal) {\n                                maxVal = v\n                                maxIndex = flat\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                outData[p] = maxVal\n                this.lastMaxIndex[p] = maxIndex\n\n                p++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let sizeDX = x.data.length\n            let dXdata = alloc(sizeDX)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            let sizeOut = gradOut.data.length\n            let p = 0\n\n            while (p < sizeOut) {\n                let idx = this.lastMaxIndex[p]\n                if (idx >= 0) {\n                    dXdata[idx] += gradOut.data[p]\n                }\n                p++\n            }\n\n            return dX\n        }\n    }\n}","convtransposend.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class ConvTransposeND {\n        inChannels: number\n        outChannels: number\n        kernelShape: number[]\n        stride: number[]\n        padding: number[]\n\n        weight: Tensor\n        bias: Tensor\n\n        dW: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n\n        constructor(inChannels: number, outChannels: number,\n            kernelShape: number[], stride: number[], padding: number[]) {\n\n            this.inChannels = inChannels\n            this.outChannels = outChannels\n            this.kernelShape = kernelShape.slice(0)\n            this.stride = stride.slice(0)\n            this.padding = padding.slice(0)\n\n            let dims = kernelShape.length\n\n            // Weight shape: [inChannels, outChannels, ...kernelShape]\n            let wShape = alloc(2 + dims)\n            wShape[0] = inChannels\n            wShape[1] = outChannels\n            let i = 0\n            while (i < dims) {\n                wShape[2 + i] = kernelShape[i]\n                i++\n            }\n\n            let wSize = 1\n            i = 0\n            while (i < wShape.length) {\n                wSize *= wShape[i]\n                i++\n            }\n\n            let wData = alloc(wSize)\n            let scale = 1 / Math.sqrt(inChannels)\n            i = 0\n            while (i < wSize) {\n                wData[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n\n            this.weight = new Tensor(wData, wShape)\n            this.dW = new Tensor(alloc(wSize), wShape)\n\n            // Bias: [outChannels]\n            let bData = alloc(outChannels)\n            this.bias = new Tensor(bData, [outChannels])\n            this.dB = new Tensor(alloc(outChannels), [outChannels])\n        }\n\n        // ---------------------------------------------------------\n        // Forward (fractionally strided convolution)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let inC = x.shape[1]\n            let dims = this.kernelShape.length\n\n            // Compute output spatial dims\n            let outSpatial = alloc(dims)\n            let i = 0\n            while (i < dims) {\n                let inSize = x.shape[2 + i]\n                let k = this.kernelShape[i]\n                let s = this.stride[i]\n                let p = this.padding[i]\n\n                // Transposed conv output formula:\n                outSpatial[i] = (inSize - 1) * s - 2 * p + k\n                i++\n            }\n\n            // Output shape: [batch, outChannels, ...outSpatial]\n            let outShape = alloc(2 + dims)\n            outShape[0] = batch\n            outShape[1] = this.outChannels\n            i = 0\n            while (i < dims) {\n                outShape[2 + i] = outSpatial[i]\n                i++\n            }\n\n            // Allocate output\n            let total = 1\n            i = 0\n            while (i < outShape.length) {\n                total *= outShape[i]\n                i++\n            }\n\n            let outData = alloc(total)\n            let out = new Tensor(outData, outShape)\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Compute base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        // Compute output index\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= outSpatial[d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                // Weight index\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n\n                                let wVal = this.weight.get(idxW)\n                                let outIdx = out.index(idxOut)\n                                outData[outIdx] += x.data[p] * wVal\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            // Add bias\n            let q = 0\n            while (q < outData.length) {\n                let idx = Tensor.unravelIndex(q, outShape)\n                let oc = idx[1]\n                outData[q] += this.bias.data[oc]\n                q++\n            }\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let dims = this.kernelShape.length\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dW.data.length) {\n                this.dW.data[i] = 0\n                i++\n            }\n            i = 0\n            while (i < this.dB.data.length) {\n                this.dB.data[i] = 0\n                i++\n            }\n\n            // dB: sum over gradOut\n            let q = 0\n            while (q < gradOut.data.length) {\n                let idx = Tensor.unravelIndex(q, gradOut.shape)\n                let oc = idx[1]\n                this.dB.data[oc] += gradOut.data[q]\n                q++\n            }\n\n            // dX\n            let dXdata = alloc(x.data.length)\n            let dX = new Tensor(dXdata, x.shape.slice(0))\n\n            // Temp index arrays\n            let idxX = alloc(2 + dims)\n            let idxW = alloc(2 + dims)\n            let idxOut = alloc(2 + dims)\n            let kpos = alloc(dims)\n\n            // Loop over input positions for dW and dX\n            let p = 0\n            let sizeX = x.data.length\n\n            while (p < sizeX) {\n                idxX = Tensor.unravelIndex(p, x.shape)\n\n                let b = idxX[0]\n                let ic = idxX[1]\n\n                // Base output position\n                let baseOut = alloc(dims)\n                let d = 0\n                while (d < dims) {\n                    baseOut[d] = idxX[2 + d] * this.stride[d] - this.padding[d]\n                    d++\n                }\n\n                // ND kernel loop\n                let level = 0\n                let reset = 0\n                while (reset < dims) {\n                    kpos[reset] = 0\n                    reset++\n                }\n\n                while (level >= 0) {\n                    if (level == dims) {\n                        let inside = true\n                        let d2 = 0\n                        while (d2 < dims) {\n                            let oPos = baseOut[d2] + kpos[d2]\n                            idxOut[2 + d2] = oPos\n                            if (oPos < 0 || oPos >= gradOut.shape[2 + d2]) inside = false\n                            d2++\n                        }\n\n                        idxOut[0] = b\n\n                        if (inside) {\n                            let j = 0\n                            while (j < this.outChannels) {\n                                idxOut[1] = j\n\n                                let go = gradOut.get(idxOut)\n\n                                // dW\n                                idxW[0] = ic\n                                idxW[1] = j\n                                let d3 = 0\n                                while (d3 < dims) {\n                                    idxW[2 + d3] = kpos[d3]\n                                    d3++\n                                }\n                                let wIdx = this.weight.index(idxW)\n                                this.dW.data[wIdx] += x.data[p] * go\n\n                                // dX\n                                dXdata[p] += this.weight.data[wIdx] * go\n\n                                j++\n                            }\n                        }\n\n                        level--\n                        continue\n                    }\n\n                    if (kpos[level] < this.kernelShape[level]) {\n                        kpos[level]++\n                        level++\n                    } else {\n                        kpos[level] = 0\n                        level--\n                    }\n                }\n\n                p++\n            }\n\n            return dX\n        }\n    }\n}","rnn.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n\n    // tanh implementation (same as in activations.ts)\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class RNN {\n        inputDim: number\n        hiddenDim: number\n\n        Wxh: Tensor\n        Whh: Tensor\n        b: Tensor\n\n        dWxh: Tensor\n        dWhh: Tensor\n        dB: Tensor\n\n        lastInput: Tensor\n        lastH: number[]      // all hidden states (flattened)\n        lastA: number[]      // all pre-activations (Wxh x + Whh h + b)\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            // Weight shapes\n            this.Wxh = new Tensor(\n                this.initWeights(inputDim * hiddenDim),\n                [hiddenDim, inputDim]\n            )\n\n            this.Whh = new Tensor(\n                this.initWeights(hiddenDim * hiddenDim),\n                [hiddenDim, hiddenDim]\n            )\n\n            this.b = new Tensor(\n                alloc(hiddenDim),\n                [hiddenDim]\n            )\n\n            this.dWxh = new Tensor(alloc(inputDim * hiddenDim), [hiddenDim, inputDim])\n            this.dWhh = new Tensor(alloc(hiddenDim * hiddenDim), [hiddenDim, hiddenDim])\n            this.dB = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeights(size: number): number[] {\n            let arr = alloc(size)\n            let scale = 1 / Math.sqrt(size)\n            let i = 0\n            while (i < size) {\n                arr[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return arr\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x shape: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            // Store hidden states and pre-activations\n            this.lastH = alloc(batch * (seq + 1) * hDim)  // includes h0 = 0\n            this.lastA = alloc(batch * seq * hDim)\n\n            // Initialize h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            // Forward through time\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    // Compute h_t\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.b.data[j]\n\n                        // Wxh * x_t\n                        let k = 0\n                        while (k < inDim) {\n                            let w = this.Wxh.data[j * inDim + k]\n                            let xv = x.data[baseX + k]\n                            sum += w * xv\n                            k++\n                        }\n\n                        // Whh * h_{t-1}\n                        k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[j * hDim + k]\n                            let hv = this.lastH[baseHprev + k]\n                            sum += w * hv\n                            k++\n                        }\n\n                        this.lastA[baseA + j] = sum\n                        this.lastH[baseHcur + j] = fastTanh(sum)\n\n                        // Output is h_t\n                        outData[(b * seq + t) * hDim + j] = this.lastH[baseHcur + j]\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWxh.data.length) { this.dWxh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWhh.data.length) { this.dWhh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dB.data.length) { this.dB.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)  // gradient from next time step\n\n            // BPTT\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseA = (b * seq + t) * hDim\n\n                    // dL/dh_t = gradOut + dHnext\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dA = dH * (1 - tanh(a)^2)\n                    let dA = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let h = this.lastH[baseHcur + j]\n                        dA[j] = dH[j] * (1 - h * h)\n                        j++\n                    }\n\n                    // Accumulate gradients\n                    j = 0\n                    while (j < hDim) {\n                        // dB\n                        this.dB.data[j] += dA[j]\n\n                        // dWxh\n                        let k = 0\n                        while (k < inDim) {\n                            let idx = j * inDim + k\n                            this.dWxh.data[idx] += dA[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        // dWhh\n                        k = 0\n                        while (k < hDim) {\n                            let idx = j * hDim + k\n                            this.dWhh.data[idx] += dA[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dX\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Wxh.data[k * inDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // dHnext = Whh^T * dA\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Whh.data[k * hDim + j]\n                            sum += dA[k] * w\n                            k++\n                        }\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","gru.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class GRU {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wz: Tensor; Uz: Tensor; bz: Tensor\n        Wr: Tensor; Ur: Tensor; br: Tensor\n        Wh: Tensor; Uh: Tensor; bh: Tensor\n\n        // Gradients\n        dWz: Tensor; dUz: Tensor; dBz: Tensor\n        dWr: Tensor; dUr: Tensor; dBr: Tensor\n        dWh: Tensor; dUh: Tensor; dBh: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastZ: number[]\n        lastR: number[]\n        lastHtilde: number[]\n        lastH: number[]   // includes h0\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wz = this.initWeight(hiddenDim, inputDim)\n            this.Uz = this.initWeight(hiddenDim, hiddenDim)\n            this.bz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wr = this.initWeight(hiddenDim, inputDim)\n            this.Ur = this.initWeight(hiddenDim, hiddenDim)\n            this.br = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wh = this.initWeight(hiddenDim, inputDim)\n            this.Uh = this.initWeight(hiddenDim, hiddenDim)\n            this.bh = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWz = this.initZero(hiddenDim, inputDim)\n            this.dUz = this.initZero(hiddenDim, hiddenDim)\n            this.dBz = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWr = this.initZero(hiddenDim, inputDim)\n            this.dUr = this.initZero(hiddenDim, hiddenDim)\n            this.dBr = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWh = this.initZero(hiddenDim, inputDim)\n            this.dUh = this.initZero(hiddenDim, hiddenDim)\n            this.dBh = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastZ = alloc(batch * seq * hDim)\n            this.lastR = alloc(batch * seq * hDim)\n            this.lastHtilde = alloc(batch * seq * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // h0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // ---- Compute z_t ----\n                    let j = 0\n                    while (j < hDim) {\n                        let sum = this.bz.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wz.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Uz.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let z = sigmoid(sum)\n                        this.lastZ[baseZ + j] = z\n                        j++\n                    }\n\n                    // ---- Compute r_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.br.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wr.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            sum += this.Ur.data[j * hDim + k] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        let r = sigmoid(sum)\n                        this.lastR[baseR + j] = r\n                        j++\n                    }\n\n                    // ---- Compute h~_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = this.bh.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += this.Wh.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            let r = this.lastR[baseR + k]\n                            sum += this.Uh.data[j * hDim + k] * (r * hprev)\n                            k++\n                        }\n\n                        let htilde = fastTanh(sum)\n                        this.lastHtilde[baseHtilde + j] = htilde\n                        j++\n                    }\n\n                    // ---- Final h_t ----\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        let h = (1 - z) * hprev + z * htilde\n                        this.lastH[baseHcur + j] = h\n\n                        outData[(b * seq + t) * hDim + j] = h\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWz.data.length) { this.dWz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUz.data.length) { this.dUz.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBz.data.length) { this.dBz.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWr.data.length) { this.dWr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUr.data.length) { this.dUr.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBr.data.length) { this.dBr.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWh.data.length) { this.dWh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUh.data.length) { this.dUh.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBh.data.length) { this.dBh.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseZ = (b * seq + t) * hDim\n                    let baseR = (b * seq + t) * hDim\n                    let baseHtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Compute gate derivatives\n                    let dZ = alloc(hDim)\n                    let dHtilde = alloc(hDim)\n                    let dHprev = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        let htilde = this.lastHtilde[baseHtilde + j]\n\n                        dZ[j] = dH[j] * (htilde - hprev)\n                        dHtilde[j] = dH[j] * z\n                        dHprev[j] = dH[j] * (1 - z)\n                        j++\n                    }\n\n                    // dA_h = dHtilde * (1 - htilde^2)\n                    let dA_h = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let htilde = this.lastHtilde[baseHtilde + j]\n                        dA_h[j] = dHtilde[j] * (1 - htilde * htilde)\n                        j++\n                    }\n\n                    // dA_z = dZ * z * (1 - z)\n                    let dA_z = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let z = this.lastZ[baseZ + j]\n                        dA_z[j] = dZ[j] * z * (1 - z)\n                        j++\n                    }\n\n                    // dA_r = dHtilde * Uh * hprev * r*(1-r)\n                    let dA_r = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            let w = this.Uh.data[k * hDim + j]\n                            let d = dA_h[k]\n                            sum += d * w\n                            k++\n                        }\n                        let r = this.lastR[baseR + j]\n                        let hprev = this.lastH[baseHprev + j]\n                        dA_r[j] = sum * hprev * r * (1 - r)\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWh, dUh, dBh\n                    j = 0\n                    while (j < hDim) {\n                        this.dBh.data[j] += dA_h[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWh.data[j * inDim + k] += dA_h[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let r = this.lastR[baseR + k]\n                            let hprev = this.lastH[baseHprev + k]\n                            this.dUh.data[j * hDim + k] += dA_h[j] * (r * hprev)\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWz, dUz, dBz\n                    j = 0\n                    while (j < hDim) {\n                        this.dBz.data[j] += dA_z[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWz.data[j * inDim + k] += dA_z[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUz.data[j * hDim + k] += dA_z[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWr, dUr, dBr\n                    j = 0\n                    while (j < hDim) {\n                        this.dBr.data[j] += dA_r[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWr.data[j * inDim + k] += dA_r[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUr.data[j * hDim + k] += dA_r[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Wz.data[k * inDim + j]\n                            sum += dA_r[k] * this.Wr.data[k * inDim + j]\n                            sum += dA_h[k] * this.Wh.data[k * inDim + j]\n                            k++\n                        }\n\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext ----\n                    j = 0\n                    while (j < hDim) {\n                        let sum = dHprev[j]\n\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dA_z[k] * this.Uz.data[k * hDim + j]\n                            sum += dA_r[k] * this.Ur.data[k * hDim + j]\n                            sum += dA_h[k] * this.Uh.data[k * hDim + j] * this.lastR[baseR + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sum\n                        j++\n                    }\n\n                    b++\n                }\n\n                t--\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n}","lstm.ts":"namespace TorchNew {\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    function sigmoid(x: number): number {\n        return 1 / (1 + Math.exp(-x))\n    }\n\n    function fastTanh(x: number): number {\n        let e2x = Math.exp(2 * x)\n        return (e2x - 1) / (e2x + 1)\n    }\n\n    export class LSTM {\n        inputDim: number\n        hiddenDim: number\n\n        // Parameters\n        Wi: Tensor; Ui: Tensor; bi: Tensor\n        Wf: Tensor; Uf: Tensor; bf: Tensor\n        Wo: Tensor; Uo: Tensor; bo: Tensor\n        Wc: Tensor; Uc: Tensor; bc: Tensor\n\n        // Gradients\n        dWi: Tensor; dUi: Tensor; dBi: Tensor\n        dWf: Tensor; dUf: Tensor; dBf: Tensor\n        dWo: Tensor; dUo: Tensor; dBo: Tensor\n        dWc: Tensor; dUc: Tensor; dBc: Tensor\n\n        // Saved for backward\n        lastInput: Tensor\n        lastI: number[]\n        lastF: number[]\n        lastO: number[]\n        lastCtilde: number[]\n        lastC: number[]\n        lastH: number[]\n\n        constructor(inputDim: number, hiddenDim: number) {\n            this.inputDim = inputDim\n            this.hiddenDim = hiddenDim\n\n            this.Wi = this.initWeight(hiddenDim, inputDim)\n            this.Ui = this.initWeight(hiddenDim, hiddenDim)\n            this.bi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wf = this.initWeight(hiddenDim, inputDim)\n            this.Uf = this.initWeight(hiddenDim, hiddenDim)\n            this.bf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wo = this.initWeight(hiddenDim, inputDim)\n            this.Uo = this.initWeight(hiddenDim, hiddenDim)\n            this.bo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.Wc = this.initWeight(hiddenDim, inputDim)\n            this.Uc = this.initWeight(hiddenDim, hiddenDim)\n            this.bc = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWi = this.initZero(hiddenDim, inputDim)\n            this.dUi = this.initZero(hiddenDim, hiddenDim)\n            this.dBi = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWf = this.initZero(hiddenDim, inputDim)\n            this.dUf = this.initZero(hiddenDim, hiddenDim)\n            this.dBf = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWo = this.initZero(hiddenDim, inputDim)\n            this.dUo = this.initZero(hiddenDim, hiddenDim)\n            this.dBo = new Tensor(alloc(hiddenDim), [hiddenDim])\n\n            this.dWc = this.initZero(hiddenDim, inputDim)\n            this.dUc = this.initZero(hiddenDim, hiddenDim)\n            this.dBc = new Tensor(alloc(hiddenDim), [hiddenDim])\n        }\n\n        initWeight(rows: number, cols: number): Tensor {\n            let size = rows * cols\n            let data = alloc(size)\n            let scale = 1 / Math.sqrt(cols)\n            let i = 0\n            while (i < size) {\n                data[i] = (Math.random() - 0.5) * 2 * scale\n                i++\n            }\n            return new Tensor(data, [rows, cols])\n        }\n\n        initZero(rows: number, cols: number): Tensor {\n            return new Tensor(alloc(rows * cols), [rows, cols])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, inputDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            let outData = alloc(batch * seq * hDim)\n\n            this.lastI = alloc(batch * seq * hDim)\n            this.lastF = alloc(batch * seq * hDim)\n            this.lastO = alloc(batch * seq * hDim)\n            this.lastCtilde = alloc(batch * seq * hDim)\n            this.lastC = alloc(batch * (seq + 1) * hDim)\n            this.lastH = alloc(batch * (seq + 1) * hDim)\n\n            // Initialize h0 = 0, c0 = 0\n            let i = 0\n            while (i < batch * hDim) {\n                this.lastH[i] = 0\n                this.lastC[i] = 0\n                i++\n            }\n\n            let t = 0\n            while (t < seq) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // ---- Compute gates ----\n                    let j = 0\n                    while (j < hDim) {\n                        // Input gate\n                        let sumI = this.bi.data[j]\n                        let sumF = this.bf.data[j]\n                        let sumO = this.bo.data[j]\n                        let sumC = this.bc.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            let xv = x.data[baseX + k]\n                            sumI += this.Wi.data[j * inDim + k] * xv\n                            sumF += this.Wf.data[j * inDim + k] * xv\n                            sumO += this.Wo.data[j * inDim + k] * xv\n                            sumC += this.Wc.data[j * inDim + k] * xv\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            let hprev = this.lastH[baseHprev + k]\n                            sumI += this.Ui.data[j * hDim + k] * hprev\n                            sumF += this.Uf.data[j * hDim + k] * hprev\n                            sumO += this.Uo.data[j * hDim + k] * hprev\n                            sumC += this.Uc.data[j * hDim + k] * hprev\n                            k++\n                        }\n\n                        let iGate = sigmoid(sumI)\n                        let fGate = sigmoid(sumF)\n                        let oGate = sigmoid(sumO)\n                        let cTilde = fastTanh(sumC)\n\n                        this.lastI[baseI + j] = iGate\n                        this.lastF[baseF + j] = fGate\n                        this.lastO[baseO + j] = oGate\n                        this.lastCtilde[baseCtilde + j] = cTilde\n\n                        // Cell state\n                        let cprev = this.lastC[baseCprev + j]\n                        let ccur = fGate * cprev + iGate * cTilde\n                        this.lastC[baseCcur + j] = ccur\n\n                        // Hidden state\n                        let hcur = oGate * fastTanh(ccur)\n                        this.lastH[baseHcur + j] = hcur\n\n                        outData[(b * seq + t) * hDim + j] = hcur\n\n                        j++\n                    }\n\n                    b++\n                }\n                t++\n            }\n\n            return new Tensor(outData, [batch, seq, hDim])\n        }\n\n        // ---------------------------------------------------------\n        // Backward (BPTT)\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = this.inputDim\n            let hDim = this.hiddenDim\n\n            // Zero gradients\n            let i = 0\n            while (i < this.dWi.data.length) { this.dWi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUi.data.length) { this.dUi.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBi.data.length) { this.dBi.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWf.data.length) { this.dWf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUf.data.length) { this.dUf.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBf.data.length) { this.dBf.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUo.data.length) { this.dUo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBo.data.length) { this.dBo.data[i] = 0; i++ }\n\n            i = 0\n            while (i < this.dWc.data.length) { this.dWc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dUc.data.length) { this.dUc.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dBc.data.length) { this.dBc.data[i] = 0; i++ }\n\n            let dXdata = alloc(x.data.length)\n            let dHnext = alloc(batch * hDim)\n            let dCnext = alloc(batch * hDim)\n\n            let t = seq - 1\n            while (t >= 0) {\n                let b = 0\n                while (b < batch) {\n\n                    let baseX = (b * seq + t) * inDim\n                    let baseHprev = (b * (seq + 1) + t) * hDim\n                    let baseCprev = (b * (seq + 1) + t) * hDim\n                    let baseHcur = (b * (seq + 1) + (t + 1)) * hDim\n                    let baseCcur = (b * (seq + 1) + (t + 1)) * hDim\n\n                    let baseI = (b * seq + t) * hDim\n                    let baseF = (b * seq + t) * hDim\n                    let baseO = (b * seq + t) * hDim\n                    let baseCtilde = (b * seq + t) * hDim\n\n                    // dL/dh_t\n                    let dH = alloc(hDim)\n                    let j = 0\n                    while (j < hDim) {\n                        dH[j] = gradOut.data[(b * seq + t) * hDim + j] + dHnext[b * hDim + j]\n                        j++\n                    }\n\n                    // dL/dc_t\n                    let dC = alloc(hDim)\n                    j = 0\n                    while (j < hDim) {\n                        let o = this.lastO[baseO + j]\n                        let ccur = this.lastC[baseCcur + j]\n                        dC[j] = dH[j] * o * (1 - fastTanh(ccur) * fastTanh(ccur)) + dCnext[b * hDim + j]\n                        j++\n                    }\n\n                    // Gate derivatives\n                    let dI = alloc(hDim)\n                    let dF = alloc(hDim)\n                    let dO = alloc(hDim)\n                    let dCtilde = alloc(hDim)\n\n                    j = 0\n                    while (j < hDim) {\n                        let iGate = this.lastI[baseI + j]\n                        let fGate = this.lastF[baseF + j]\n                        let oGate = this.lastO[baseO + j]\n                        let cTilde = this.lastCtilde[baseCtilde + j]\n                        let cprev = this.lastC[baseCprev + j]\n\n                        dI[j] = dC[j] * cTilde * iGate * (1 - iGate)\n                        dF[j] = dC[j] * cprev * fGate * (1 - fGate)\n                        dO[j] = dH[j] * fastTanh(this.lastC[baseCcur + j]) * oGate * (1 - oGate)\n                        dCtilde[j] = dC[j] * iGate * (1 - cTilde * cTilde)\n\n                        j++\n                    }\n\n                    // ---- Accumulate gradients ----\n\n                    // dWi, dUi, dBi\n                    j = 0\n                    while (j < hDim) {\n                        this.dBi.data[j] += dI[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWi.data[j * inDim + k] += dI[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUi.data[j * hDim + k] += dI[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWf, dUf, dBf\n                    j = 0\n                    while (j < hDim) {\n                        this.dBf.data[j] += dF[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWf.data[j * inDim + k] += dF[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUf.data[j * hDim + k] += dF[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWo, dUo, dBo\n                    j = 0\n                    while (j < hDim) {\n                        this.dBo.data[j] += dO[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWo.data[j * inDim + k] += dO[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUo.data[j * hDim + k] += dO[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // dWc, dUc, dBc\n                    j = 0\n                    while (j < hDim) {\n                        this.dBc.data[j] += dCtilde[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            this.dWc.data[j * inDim + k] += dCtilde[j] * x.data[baseX + k]\n                            k++\n                        }\n\n                        k = 0\n                        while (k < hDim) {\n                            this.dUc.data[j * hDim + k] += dCtilde[j] * this.lastH[baseHprev + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    // ---- Compute dX ----\n                    j = 0\n                    while (j < inDim) {\n                        let sum = 0\n                        let k = 0\n                        while (k < hDim) {\n                            sum += dI[k] * this.Wi.data[k * inDim + j]\n                            sum += dF[k] * this.Wf.data[k * inDim + j]\n                            sum += dO[k] * this.Wo.data[k * inDim + j]\n                            sum += dCtilde[k] * this.Wc.data[k * inDim + j]\n                            k++\n                        }\n                        dXdata[baseX + j] = sum\n                        j++\n                    }\n\n                    // ---- Compute dHnext and dCnext ----\n                    j = 0\n                    while (j < hDim) {\n                        // dCnext = dC * f_t\n                        let fGate = this.lastF[baseF + j]\n                        dCnext[b * hDim + j] = dC[j] * fGate\n\n                        // dHnext = contributions from all gates\n                        let sumH = 0\n\n                        // From input gate\n                        let k = 0\n                        while (k < hDim) {\n                            sumH += dI[k] * this.Ui.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From forget gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dF[k] * this.Uf.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From output gate\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dO[k] * this.Uo.data[k * hDim + j]\n                            k++\n                        }\n\n                        // From candidate cell\n                        k = 0\n                        while (k < hDim) {\n                            sumH += dCtilde[k] * this.Uc.data[k * hDim + j]\n                            k++\n                        }\n\n                        dHnext[b * hDim + j] = sumH\n\n                        j++\n                    }\n\n                    b++\n                    t--\n                }\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n    }\n\n}","residual.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class Residual {\n        layer: any\n        lastInput: Tensor\n        lastLayerOut: Tensor\n\n        constructor(layer: any) {\n            this.layer = layer\n        }\n\n        // ---------------------------------------------------------\n        // Forward: y = x + layer(x)\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let outLayer = this.layer.forward(x)\n            this.lastLayerOut = outLayer\n\n            let size = x.data.length\n            let outData = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                outData[i] = x.data[i] + outLayer.data[i]\n                i++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Backward: dX = gradOut + dLayer\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let gradLayer = this.layer.backward(gradOut)\n\n            let size = gradOut.data.length\n            let dXdata = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dXdata[i] = gradOut.data[i] + gradLayer.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","feedforward.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n    \n    export class FeedForward {\n        layer1: any\n        activation: any\n        layer2: any\n\n        lastInput: Tensor\n        lastHidden: Tensor\n\n        constructor(inputDim: number, hiddenDim: number, outputDim: number, activation: any) {\n            this.layer1 = new Linear(inputDim, hiddenDim)\n            this.activation = activation\n            this.layer2 = new Linear(hiddenDim, outputDim)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            let h1 = this.layer1.forward(x)\n            let h2 = this.activation.forward(h1)\n            this.lastHidden = h2\n\n            let out = this.layer2.forward(h2)\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            // Backprop through second linear\n            let g2 = this.layer2.backward(gradOut)\n\n            // Backprop through activation\n            let gAct = this.activation.backward(g2)\n\n            // Backprop through first linear\n            let g1 = this.layer1.backward(gAct)\n\n            return g1\n        }\n    }\n}","multiheadattention.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    function softmaxRow(data: number[], start: number, length: number): void {\n        let maxVal = data[start]\n        let i = 1\n        while (i < length) {\n            let v = data[start + i]\n            if (v > maxVal) maxVal = v\n            i++\n        }\n\n        let sum = 0\n        i = 0\n        while (i < length) {\n            let e = Math.exp(data[start + i] - maxVal)\n            data[start + i] = e\n            sum += e\n            i++\n        }\n\n        i = 0\n        while (i < length) {\n            data[start + i] /= sum\n            i++\n        }\n    }\n\n    export class MultiHeadAttention {\n        embedDim: number\n        numHeads: number\n        headDim: number\n\n        Wq: Tensor; bq: Tensor\n        Wk: Tensor; bk: Tensor\n        Wv: Tensor; bv: Tensor\n        Wo: Tensor; bo: Tensor\n\n        dWq: Tensor; dbq: Tensor\n        dWk: Tensor; dbk: Tensor\n        dWv: Tensor; dbv: Tensor\n        dWo: Tensor; dbo: Tensor\n\n        lastInput: Tensor\n        lastQ: Tensor\n        lastK: Tensor\n        lastV: Tensor\n        lastScores: number[]\n        lastSoftmax: number[]\n        lastAttention: number[]\n\n        lastDK: Tensor\n        lastDV: Tensor\n\n        constructor(embedDim: number, numHeads: number) {\n            this.embedDim = embedDim\n            this.numHeads = numHeads\n            this.headDim = Math.idiv(embedDim, numHeads)\n\n            this.Wq = new Linear(embedDim, embedDim).weight\n            this.bq = new Linear(embedDim, embedDim).bias\n\n            this.Wk = new Linear(embedDim, embedDim).weight\n            this.bk = new Linear(embedDim, embedDim).bias\n\n            this.Wv = new Linear(embedDim, embedDim).weight\n            this.bv = new Linear(embedDim, embedDim).bias\n\n            this.Wo = new Linear(embedDim, embedDim).weight\n            this.bo = new Linear(embedDim, embedDim).bias\n\n            this.dWq = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbq = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWk = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbk = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWv = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbv = new Tensor(alloc(embedDim), [embedDim])\n\n            this.dWo = new Tensor(alloc(embedDim * embedDim), [embedDim, embedDim])\n            this.dbo = new Tensor(alloc(embedDim), [embedDim])\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            // x: [batch, seq, embedDim]\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(x, this.Wq, this.bq)\n            let K = this.linearForward(x, this.Wk, this.bk)\n            let V = this.linearForward(x, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores and softmax\n            let scores = alloc(batch * H * seq * seq)\n            let softmaxOut = alloc(batch * H * seq * seq)\n            let attention = alloc(batch * seq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            // Compute attention scores\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n\n                    let i = 0\n                    while (i < seq) {\n                        let j = 0\n                        while (j < seq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * seq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < seq) {\n                        softmaxRow(scores, sBase + i2 * seq, seq)\n                        let j2 = 0\n                        while (j2 < seq) {\n                            softmaxOut[sBase + i2 * seq + j2] = scores[sBase + i2 * seq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * seq * E\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < seq) {\n                                let w = softmaxOut[sBase + i3 * seq + j3]\n                                let vv = V.data[kBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, seq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let x = this.lastInput\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection: attention -> gradAtt\n            let attTensor = new Tensor(this.lastAttention, [batch, seq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // 2) Allocate grads for Q, K, V and softmax/scores\n            let dQdata = alloc(batch * seq * E)\n            let dKdata = alloc(batch * seq * E)\n            let dVdata = alloc(batch * seq * E)\n\n            let dSoft = alloc(batch * H * seq * seq)\n            let dScores = alloc(batch * H * seq * seq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 3) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * seq * seq) + (h * seq * seq)\n                    let outBase = b * seq * E\n                    let qBase = (b * seq * E) + h * D\n                    let kBase = (b * seq * E) + h * D\n                    let vBase = (b * seq * E) + h * D\n\n                    // dSoft and dV from gradAtt\n                    let i2 = 0\n                    while (i2 < seq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < seq) {\n                                let w = this.lastSoftmax[sBase + i2 * seq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * seq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 4) Softmax backward: dScores from dSoft and probs\n                    let i3 = 0\n                    while (i3 < seq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < seq) {\n                            let p = this.lastSoftmax[sBase + i3 * seq + j3]\n                            let g = dSoft[sBase + i3 * seq + j3]\n                            dScores[sBase + i3 * seq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 5) Scores = (QK^T)/sqrt(D) -> dQ, dK\n                    let i4 = 0\n                    while (i4 < seq) {\n                        let j4 = 0\n                        while (j4 < seq) {\n                            let ds = dScores[sBase + i4 * seq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 6) Merge dQ, dK, dV back to [batch, seq, E] tensors\n            let dQ = new Tensor(dQdata, [batch, seq, E])\n            let dK = new Tensor(dKdata, [batch, seq, E])\n            let dV = new Tensor(dVdata, [batch, seq, E])\n\n            // 7) Back through Q, K, V linears to input x\n            let dXq = this.linearBackward(x, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(x, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(x, dV, this.Wv, this.dWv, this.dbv)\n\n            // 8) Sum contributions: dX = dXq + dXk + dXv\n            let dXdata = alloc(x.data.length)\n            i = 0\n            while (i < dXdata.length) {\n                dXdata[i] = dXq.data[i] + dXk.data[i] + dXv.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, x.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear forward\n        // ---------------------------------------------------------\n        linearForward(x: Tensor, W: Tensor, b: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = b.shape[0]\n\n            let outData = alloc(batch * seq * outDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseO = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let sum = b.data[j]\n\n                        let k = 0\n                        while (k < inDim) {\n                            sum += W.data[j * inDim + k] * x.data[baseX + k]\n                            k++\n                        }\n\n                        outData[baseO + j] = sum\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(outData, [batch, seq, outDim])\n        }\n\n        // ---------------------------------------------------------\n        // Helper: Linear backward (accumulates dW, db)\n        // ---------------------------------------------------------\n        linearBackward(x: Tensor, gradOut: Tensor, W: Tensor, dW: Tensor, db: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let inDim = x.shape[2]\n            let outDim = gradOut.shape[2]\n\n            let gradInData = alloc(batch * seq * inDim)\n\n            let bIdx = 0\n            while (bIdx < batch) {\n                let t = 0\n                while (t < seq) {\n                    let baseX = (bIdx * seq + t) * inDim\n                    let baseG = (bIdx * seq + t) * outDim\n\n                    let j = 0\n                    while (j < outDim) {\n                        let g = gradOut.data[baseG + j]\n\n                        db.data[j] += g\n\n                        let k = 0\n                        while (k < inDim) {\n                            dW.data[j * inDim + k] += g * x.data[baseX + k]\n                            gradInData[baseX + k] += g * W.data[j * inDim + k]\n                            k++\n                        }\n\n                        j++\n                    }\n\n                    t++\n                }\n                bIdx++\n            }\n\n            return new Tensor(gradInData, [batch, seq, inDim])\n        }\n\n        forwardKV(qInput: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            this.lastInput = qInput\n\n            let batch = qInput.shape[0]\n            let qSeq = qInput.shape[1]\n            let kvSeq = kInput.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Compute Q, K, V\n            let Q = this.linearForward(qInput, this.Wq, this.bq)\n            let K = this.linearForward(kInput, this.Wk, this.bk)\n            let V = this.linearForward(vInput, this.Wv, this.bv)\n\n            this.lastQ = Q\n            this.lastK = K\n            this.lastV = V\n\n            // Allocate scores, softmax, attention\n            let scores = alloc(batch * H * qSeq * kvSeq)\n            let softmaxOut = alloc(batch * H * qSeq * kvSeq)\n            let attention = alloc(batch * qSeq * E)\n\n            this.lastScores = scores\n            this.lastSoftmax = softmaxOut\n            this.lastAttention = attention\n\n            let scale = 1 / Math.sqrt(D)\n\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n\n                    // Compute scores = QK^T / sqrt(D)\n                    let i = 0\n                    while (i < qSeq) {\n                        let j = 0\n                        while (j < kvSeq) {\n                            let sum = 0\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i * E + d]\n                                let kv = K.data[kBase + j * E + d]\n                                sum += qv * kv\n                                d++\n                            }\n                            scores[sBase + i * kvSeq + j] = sum * scale\n                            j++\n                        }\n                        i++\n                    }\n\n                    // Softmax row-wise\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        softmaxRow(scores, sBase + i2 * kvSeq, kvSeq)\n                        let j2 = 0\n                        while (j2 < kvSeq) {\n                            softmaxOut[sBase + i2 * kvSeq + j2] = scores[sBase + i2 * kvSeq + j2]\n                            j2++\n                        }\n                        i2++\n                    }\n\n                    // Weighted sum: attention = softmax * V\n                    let outBase = b * qSeq * E\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let sum = 0\n                            let j3 = 0\n                            while (j3 < kvSeq) {\n                                let w = softmaxOut[sBase + i3 * kvSeq + j3]\n                                let vv = V.data[vBase + j3 * E + d3]\n                                sum += w * vv\n                                j3++\n                            }\n                            attention[outBase + i3 * E + h * D + d3] = sum\n                            d3++\n                        }\n                        i3++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // Output projection\n            let out = this.linearForward(\n                new Tensor(attention, [batch, qSeq, E]),\n                this.Wo,\n                this.bo\n            )\n\n            return out\n        }\n\n        backwardKV(gradOut: Tensor, kInput: Tensor, vInput: Tensor): Tensor {\n            let Q = this.lastQ\n            let K = this.lastK\n            let V = this.lastV\n\n            let batch = Q.shape[0]\n            let qSeq = Q.shape[1]\n            let kvSeq = K.shape[1]\n            let E = this.embedDim\n            let H = this.numHeads\n            let D = this.headDim\n\n            // Zero parameter grads\n            let i = 0\n            while (i < this.dWq.data.length) { this.dWq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbq.data.length) { this.dbq.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWk.data.length) { this.dWk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbk.data.length) { this.dbk.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWv.data.length) { this.dWv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbv.data.length) { this.dbv.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dWo.data.length) { this.dWo.data[i] = 0; i++ }\n            i = 0\n            while (i < this.dbo.data.length) { this.dbo.data[i] = 0; i++ }\n\n            // 1) Back through output projection\n            let attTensor = new Tensor(this.lastAttention, [batch, qSeq, E])\n            let gradAtt = this.linearBackward(\n                attTensor,\n                gradOut,\n                this.Wo,\n                this.dWo,\n                this.dbo\n            )\n\n            // Allocate grads for Q, K, V\n            let dQdata = alloc(batch * qSeq * E)\n            let dKdata = alloc(batch * kvSeq * E)\n            let dVdata = alloc(batch * kvSeq * E)\n\n            let dSoft = alloc(batch * H * qSeq * kvSeq)\n            let dScores = alloc(batch * H * qSeq * kvSeq)\n\n            let scale = 1 / Math.sqrt(D)\n\n            // 2) Backprop through attention = softmax * V\n            let b = 0\n            while (b < batch) {\n                let h = 0\n                while (h < H) {\n                    let sBase = (b * H * qSeq * kvSeq) + (h * qSeq * kvSeq)\n                    let outBase = b * qSeq * E\n                    let qBase = (b * qSeq * E) + h * D\n                    let kBase = (b * kvSeq * E) + h * D\n                    let vBase = (b * kvSeq * E) + h * D\n\n                    // dSoft and dV\n                    let i2 = 0\n                    while (i2 < qSeq) {\n                        let d3 = 0\n                        while (d3 < D) {\n                            let g = gradAtt.data[outBase + i2 * E + h * D + d3]\n\n                            let j2 = 0\n                            while (j2 < kvSeq) {\n                                let w = this.lastSoftmax[sBase + i2 * kvSeq + j2]\n                                let vv = V.data[vBase + j2 * E + d3]\n\n                                dSoft[sBase + i2 * kvSeq + j2] += g * vv\n                                dVdata[vBase + j2 * E + d3] += g * w\n\n                                j2++\n                            }\n\n                            d3++\n                        }\n                        i2++\n                    }\n\n                    // 3) Softmax backward\n                    let i3 = 0\n                    while (i3 < qSeq) {\n                        let dot = 0\n                        let j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dot += g * p\n                            j3++\n                        }\n\n                        j3 = 0\n                        while (j3 < kvSeq) {\n                            let p = this.lastSoftmax[sBase + i3 * kvSeq + j3]\n                            let g = dSoft[sBase + i3 * kvSeq + j3]\n                            dScores[sBase + i3 * kvSeq + j3] = (g - dot) * p\n                            j3++\n                        }\n\n                        i3++\n                    }\n\n                    // 4) Scores = QK^T / sqrt(D)\n                    let i4 = 0\n                    while (i4 < qSeq) {\n                        let j4 = 0\n                        while (j4 < kvSeq) {\n                            let ds = dScores[sBase + i4 * kvSeq + j4] * scale\n\n                            let d = 0\n                            while (d < D) {\n                                let qv = Q.data[qBase + i4 * E + d]\n                                let kv = K.data[kBase + j4 * E + d]\n\n                                dQdata[qBase + i4 * E + d] += ds * kv\n                                dKdata[kBase + j4 * E + d] += ds * qv\n\n                                d++\n                            }\n\n                            j4++\n                        }\n                        i4++\n                    }\n\n                    h++\n                }\n                b++\n            }\n\n            // 5) Wrap dQ, dK, dV as tensors\n            let dQ = new Tensor(dQdata, [batch, qSeq, E])\n            let dK = new Tensor(dKdata, [batch, kvSeq, E])\n            let dV = new Tensor(dVdata, [batch, kvSeq, E])\n\n            this.lastDK = dK\n            this.lastDV = dV\n\n\n            // 6) Back through Q, K, V linears\n            let dXq = this.linearBackward(this.lastInput, dQ, this.Wq, this.dWq, this.dbq)\n            let dXk = this.linearBackward(kInput, dK, this.Wk, this.dWk, this.dbk)\n            let dXv = this.linearBackward(vInput, dV, this.Wv, this.dWv, this.dbv)\n\n            // 7) Return gradients for decoder input (queries)\n            return dXq\n        }\n    }\n}","tranformerencoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerEncoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        mha: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterMHA: Tensor\n        lastAfterFF: Tensor\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            // LayerNorm expects an array of dims\n            this.ln1 = new LayerNorm([embedDim])\n            this.mha = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n\n            // FeedForward expects activation instance\n            this.ff = new FeedForward(\n                embedDim,\n                ffHiddenDim,\n                embedDim,\n                new TorchNew.Activations.ReLU()\n            )\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor): Tensor {\n            this.lastInput = x\n\n            // 1) LayerNorm â†’ MHA â†’ Residual\n            let xNorm1 = this.ln1.forward(x)\n            let mhaOut = this.mha.forward(xNorm1)\n\n            let size = x.data.length\n            let afterMHAdata = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterMHAdata[i] = x.data[i] + mhaOut.data[i]\n                i++\n            }\n            let afterMHA = new Tensor(afterMHAdata, x.shape.slice(0))\n            this.lastAfterMHA = afterMHA\n\n            // 2) LayerNorm â†’ FeedForward â†’ Residual\n            let xNorm2 = this.ln2.forward(afterMHA)\n            let ffOut = this.ff.forward(xNorm2)\n\n            let afterFFdata = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFdata[i] = afterMHA.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFdata, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // 1) Backprop through second residual: d(afterMHA) += gradOut\n            let dAfterMHAdata = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterMHAdata[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            // 2) Backprop through FeedForward\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n\n            // 3) Backprop through LayerNorm2\n            let dNorm2 = this.ln2.backward(dFF)\n\n            // Add to dAfterMHA\n            i = 0\n            while (i < size) {\n                dAfterMHAdata[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterMHA = new Tensor(dAfterMHAdata, gradOut.shape.slice(0))\n\n            // 4) Backprop through first residual: dX += dAfterMHA\n            let dXdata = alloc(size)\n            let dMHAout = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterMHA.data[i]\n                dMHAout[i] = dAfterMHA.data[i]\n                i++\n            }\n\n            // 5) Backprop through MHA\n            let dMHA = this.mha.backward(new Tensor(dMHAout, gradOut.shape.slice(0)))\n\n            // 6) Backprop through LayerNorm1\n            let dNorm1 = this.ln1.backward(dMHA)\n\n            // Add to dX\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n    }\n}","tranformerdecoder.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerDecoder {\n        ln1: LayerNorm\n        ln2: LayerNorm\n        ln3: LayerNorm\n\n        selfAtt: MultiHeadAttention\n        crossAtt: MultiHeadAttention\n        ff: FeedForward\n\n        lastInput: Tensor\n        lastAfterSelf: Tensor\n        lastAfterCross: Tensor\n        lastAfterFF: Tensor\n\n        lastDEnc: Tensor\n\n\n\n        constructor(embedDim: number, numHeads: number, ffHiddenDim: number) {\n            this.ln1 = new LayerNorm([embedDim])\n            this.selfAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln2 = new LayerNorm([embedDim])\n            this.crossAtt = new MultiHeadAttention(embedDim, numHeads)\n\n            this.ln3 = new LayerNorm([embedDim])\n            this.ff = new FeedForward(embedDim, ffHiddenDim, embedDim, new TorchNew.Activations.ReLU())\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(x: Tensor, encoderOut: Tensor): Tensor {\n            this.lastInput = x\n\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let embedDim = x.shape[2]\n\n            // -----------------------------\n            // 1) Masked Self-Attention\n            // -----------------------------\n            let xNorm1 = this.ln1.forward(x)\n\n            // Apply causal mask: disallow attending to future tokens\n            let masked = this.applyCausalMask(xNorm1)\n\n            let selfOut = this.selfAtt.forward(masked)\n\n            let size = x.data.length\n            let afterSelfData = alloc(size)\n            let i = 0\n            while (i < size) {\n                afterSelfData[i] = x.data[i] + selfOut.data[i]\n                i++\n            }\n            let afterSelf = new Tensor(afterSelfData, x.shape.slice(0))\n            this.lastAfterSelf = afterSelf\n\n            // -----------------------------\n            // 2) Cross-Attention\n            // -----------------------------\n            let xNorm2 = this.ln2.forward(afterSelf)\n\n            // Cross-attention: query = decoder, key/value = encoder\n            let crossOut = this.crossAtt.forwardKV(xNorm2, encoderOut, encoderOut)\n\n            let afterCrossData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterCrossData[i] = afterSelf.data[i] + crossOut.data[i]\n                i++\n            }\n            let afterCross = new Tensor(afterCrossData, x.shape.slice(0))\n            this.lastAfterCross = afterCross\n\n            // -----------------------------\n            // 3) FeedForward\n            // -----------------------------\n            let xNorm3 = this.ln3.forward(afterCross)\n            let ffOut = this.ff.forward(xNorm3)\n\n            let afterFFData = alloc(size)\n            i = 0\n            while (i < size) {\n                afterFFData[i] = afterCross.data[i] + ffOut.data[i]\n                i++\n            }\n            let afterFF = new Tensor(afterFFData, x.shape.slice(0))\n            this.lastAfterFF = afterFF\n\n            return afterFF\n        }\n\n        // ---------------------------------------------------------\n        // Backward\n        // ---------------------------------------------------------\n        backward(gradOut: Tensor, encoderOut: Tensor): Tensor {\n            let size = gradOut.data.length\n\n            // -----------------------------\n            // 1) Backprop through FF residual\n            // -----------------------------\n            let dAfterCrossData = alloc(size)\n            let dFFout = alloc(size)\n\n            let i = 0\n            while (i < size) {\n                dAfterCrossData[i] = gradOut.data[i]\n                dFFout[i] = gradOut.data[i]\n                i++\n            }\n\n            let dFF = this.ff.backward(new Tensor(dFFout, gradOut.shape.slice(0)))\n            let dNorm3 = this.ln3.backward(dFF)\n\n            i = 0\n            while (i < size) {\n                dAfterCrossData[i] += dNorm3.data[i]\n                i++\n            }\n\n            let dAfterCross = new Tensor(dAfterCrossData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 2) Backprop through cross-attention residual\n            // -----------------------------\n            let dAfterSelfData = alloc(size)\n            let dCrossOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] = dAfterCross.data[i]\n                dCrossOut[i] = dAfterCross.data[i]\n                i++\n            }\n\n            let dCross = this.crossAtt.backwardKV(\n                new Tensor(dCrossOut, gradOut.shape.slice(0)),\n                encoderOut,\n                encoderOut\n            )\n\n            let dEncData = alloc(encoderOut.data.length)\n            let j = 0\n            while (j < dEncData.length) {\n                dEncData[j] = this.crossAtt.lastDK.data[j] + this.crossAtt.lastDV.data[j]\n                j++\n            }\n            this.lastDEnc = new Tensor(dEncData, encoderOut.shape.slice(0))\n\n\n            let dNorm2 = this.ln2.backward(dCross)\n\n            i = 0\n            while (i < size) {\n                dAfterSelfData[i] += dNorm2.data[i]\n                i++\n            }\n\n            let dAfterSelf = new Tensor(dAfterSelfData, gradOut.shape.slice(0))\n\n            // -----------------------------\n            // 3) Backprop through self-attention residual\n            // -----------------------------\n            let dXdata = alloc(size)\n            let dSelfOut = alloc(size)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] = dAfterSelf.data[i]\n                dSelfOut[i] = dAfterSelf.data[i]\n                i++\n            }\n\n            let dSelf = this.selfAtt.backward(new Tensor(dSelfOut, gradOut.shape.slice(0)))\n            let dNorm1 = this.ln1.backward(dSelf)\n\n            i = 0\n            while (i < size) {\n                dXdata[i] += dNorm1.data[i]\n                i++\n            }\n\n            return new Tensor(dXdata, gradOut.shape.slice(0))\n        }\n\n        // ---------------------------------------------------------\n        // Causal mask: zero out future positions\n        // ---------------------------------------------------------\n        applyCausalMask(x: Tensor): Tensor {\n            let batch = x.shape[0]\n            let seq = x.shape[1]\n            let dim = x.shape[2]\n\n            let outData = alloc(x.data.length)\n\n            let b = 0\n            while (b < batch) {\n                let i = 0\n                while (i < seq) {\n                    let j = 0\n                    while (j < seq) {\n                        let k = 0\n                        while (k < dim) {\n                            let idx = b * seq * dim + i * dim + k\n                            let src = b * seq * dim + j * dim + k\n\n                            if (j <= i) {\n                                outData[idx] = x.data[src]\n                            } else {\n                                outData[idx] = 0\n                            }\n\n                            k++\n                        }\n                        j++\n                    }\n                    i++\n                }\n                b++\n            }\n\n            return new Tensor(outData, x.shape.slice(0))\n        }\n    }\n}","tranformermodel.ts":"namespace TorchNew {\n\n    function alloc(size: number): number[] {\n        let arr = Array.repeat(0, size)\n        let i = 0\n        while (i < size) {\n            arr[i] = 0\n            i++\n        }\n        return arr\n    }\n\n    export class TransformerModel {\n        embed: Embedding\n        posEnc: PositionalEncoding\n\n        encoderBlocks: TransformerEncoder[]\n        decoderBlocks: TransformerDecoder[]\n\n        finalLinear: Linear\n\n        lastInput: Tensor\n        lastEncoderOut: Tensor\n        lastDecoderOut: Tensor\n\n        constructor(\n            vocabSize: number,\n            embedDim: number,\n            numHeads: number,\n            ffHiddenDim: number,\n            numEncoderLayers: number,\n            numDecoderLayers: number,\n            maxSeqLen: number\n        ) {\n            this.embed = new Embedding(vocabSize, embedDim)\n            this.posEnc = new PositionalEncoding(maxSeqLen, embedDim)\n\n            this.encoderBlocks = []\n            let i = 0\n            while (i < numEncoderLayers) {\n                this.encoderBlocks.push(\n                    new TransformerEncoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.decoderBlocks = []\n            i = 0\n            while (i < numDecoderLayers) {\n                this.decoderBlocks.push(\n                    new TransformerDecoder(embedDim, numHeads, ffHiddenDim)\n                )\n                i++\n            }\n\n            this.finalLinear = new Linear(embedDim, vocabSize)\n        }\n\n        // ---------------------------------------------------------\n        // Forward\n        // ---------------------------------------------------------\n        forward(srcTokens: Tensor, tgtTokens: Tensor): Tensor {\n            // srcTokens: [batch, srcSeq]\n            // tgtTokens: [batch, tgtSeq]\n\n            this.lastInput = srcTokens\n\n            // 1) Embed + positional encode source\n            let srcEmb = this.embed.forward(srcTokens)\n            let srcPE = this.posEnc.forward(srcEmb)\n\n            // 2) Pass through encoder stack\n            let encOut = srcPE\n            let i = 0\n            while (i < this.encoderBlocks.length) {\n                encOut = this.encoderBlocks[i].forward(encOut)\n                i++\n            }\n            this.lastEncoderOut = encOut\n\n            // 3) Embed + positional encode target\n            let tgtEmb = this.embed.forward(tgtTokens)\n            let tgtPE = this.posEnc.forward(tgtEmb)\n\n            // 4) Pass through decoder stack (with cross-attention)\n            let decOut = tgtPE\n            i = 0\n            while (i < this.decoderBlocks.length) {\n                decOut = this.decoderBlocks[i].forward(decOut, encOut)\n                i++\n            }\n            this.lastDecoderOut = decOut\n\n            // 5) Final projection to vocab logits\n            let logits = this.finalLinear.forward(decOut)\n            return logits\n        }\n\n        backward(gradOut: Tensor): { dSrc: Tensor, dTgt: Tensor } {\n            // 1) Backprop through final linear projection\n            // Use the same input x that was used in forward (adjust name if different)\n            const linGrad = this.finalLinear.backward(gradOut, this.lastDecoderOut)\n            let dDec: Tensor = linGrad.dx  // [batch, tgtSeq, embedDim]\n\n            // 2) Backprop through decoder stack (right-to-left)\n            let i = this.decoderBlocks.length - 1\n\n            // Accumulate gradients w.r.t. encoder output from all decoder blocks\n            let dEncAccum: Tensor = null\n\n            while (i >= 0) {\n                const block = this.decoderBlocks[i]\n\n                // block.backward returns { dX, dEnc }\n                // block.backward returns only dX (Tensor)\n                const dDecIn: Tensor = block.backward(dDec, this.lastEncoderOut)\n\n                // encoder gradient is stored internally\n                const dEncFromBlock: Tensor = block.lastDEnc\n\n\n\n\n                if (dEncAccum == null) {\n                    dEncAccum = dEncFromBlock\n                } else {\n                    const size = dEncAccum.data.length\n                    let j = 0\n                    while (j < size) {\n                        dEncAccum.data[j] += dEncFromBlock.data[j]\n                        j++\n                    }\n                }\n\n                dDec = dDecIn\n                i--\n            }\n\n            // 3) Backprop through encoder stack (right-to-left)\n            let dEnc: Tensor = dEncAccum\n            i = this.encoderBlocks.length - 1\n            while (i >= 0) {\n                dEnc = this.encoderBlocks[i].backward(dEnc)\n                i--\n            }\n\n            // 4) Backprop through positional encoding (source path)\n            const dSrcPE: Tensor = this.posEnc.backward(dEnc)\n\n            // 5) Backprop through embedding (source tokens)\n            const dSrc: Tensor = this.embed.backward(dSrcPE)\n\n            // 6) Backprop through positional encoding (target path)\n            const dTgtPE: Tensor = this.posEnc.backward(dDec)\n\n            // 7) Backprop through embedding (target tokens)\n            const dTgt: Tensor = this.embed.backward(dTgtPE)\n\n            return {\n                dSrc: dSrc,\n                dTgt: dTgt\n            }\n        }\n    }\n}","fasttensor.ts":"namespace TorchNew {\n    export interface FastShape {\n        rows: number;\n        columns: number;\n    }\n\n    type Function = (x: number) => number\n\n    /**\n        * Represents a multi-dimensional tensor for number[][] computations.\n        */\n    export class FastTensor {\n        shape: FastShape\n\n        /** \n         * The Data Of the Tensor\n        */\n        data: number[][];\n        /**\n        * Creates a new tensor from a 2D `Matrix`.\n        * @param data A 2D `Matrix` representing the tensor values.\n        */\n        constructor(data: number[][]) {\n            this.data = data;\n            this.shape = { rows: data.length, columns: data[0].length }\n        }\n\n        /**\n         * Returns a new tensor with each element rounded to the nearest integer.\n         *\n         * @returns {FastTensor} A tensor with rounded values.\n         */\n        round(): FastTensor {\n            return this.applyFunction((x:number) => Math.round(x))\n        }\n\n        /**\n         * Returns a new tensor with each element rounded down to the nearest whole number.\n         *\n         * @returns {FastTensor} A tensor with floored values.\n         */\n        floor(): FastTensor {\n            return this.applyFunction((x:number) => Math.floor(x))\n        }\n\n        /**\n         * Flattens the 2D tensor into a 1D array in row-major order.\n         *\n         * @returns {number[]} A flat array containing all elements of the tensor.\n         */\n        flat(): number[] {\n            let data: number[] = []\n            this.data.forEach((a) => a.forEach((b) => data.push(b)))\n            return data\n        }\n\n        /**\n        * Performs matrix multiplication (A * B) and returns the resulting tensor.\n        * @param other The tensor to multiply with.\n        * @returns The resulting tensor, or `null` if dimensions do not match.\n        */\n        matmul(other: FastTensor): FastTensor | null {\n            let temp1 = this.data; // Ensure a true copy\n            let temp2 = other.data; // Prevent referencing original tensor\n            let rowsA = temp1.length;\n            let colsA = temp1[0].length;\n            let rowsB = temp2.length;\n            let colsB = temp2[0].length;\n\n            if (colsA !== rowsB) {\n                return null; // Dimension mismatch\n            }\n\n            let result: number[][] = [];\n\n            // Optimized number[] multiplication\n            for (let r = 0; r < rowsA; r++) { // Process row-wise first\n                for (let i = 0; i < colsA; i++) {\n                    let temp3 = temp1[r][i]; // Store lookup value for row\n                    for (let c = 0; c < colsB; c++) {\n                        result[r][c] += temp3 * temp2[i][c]; // Perform multiplication\n                    }\n                }\n            }\n            return new FastTensor(result);\n        }\n        /**\n        * Applies a function to every element in the tensor and returns a new transformed tensor.\n        * @param func The function to apply to each tensor element.\n        * @returns A new tensor with transformed values.\n        */\n        applyFunction(func: Function): FastTensor {\n            let data = this.data;\n            let result = data.map(row => row.map(func)); // Direct transformation without extra storage\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        * Adds another tensor element-wise and returns the resulting tensor.\n        * @param other The tensor to add.\n        * @returns The resulting tensor after addition.\n        */\n        add(other: FastTensor): FastTensor {\n            let rows = Math.min(this.data.length, other.data.length);\n            let cols = Math.min(this.data[0].length, other.data[0].length);\n\n            // Manual preallocation to prevent dynamic resizing overhead\n            let result: number[][] = [];\n            let data1 = this.data;\n            let data2 = other.data;\n            // Optimized addition loop\n            for (let r = 0; r < rows; r++) {\n                for (let c = 0; c < cols; c++) {\n                    result[r][c] = data1[r][c] + data2[r][c]; // Direct assignment avoids push overhead\n                }\n            }\n\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        *Subtracts another tensor element-wise and returns the resulting tensor.\n        * @param other The tensor to subtract.\n        * @returns The resulting tensor after subtraction.\n        */\n        sub(other: FastTensor): FastTensor {\n            let rows = Math.min(this.data.length, other.data.length);\n            let cols = Math.min(this.data[0].length, other.data[0].length);\n\n            // Manual array allocation without `new Array()`\n            let result: number[][] = [];\n            for (let r = 0; r < rows; r++) {\n                result[r] = [];  // No `new Array()`, just an empty array\n                for (let c = 0; c < cols; c++) {\n                    result[r][c] = this.data[r][c] - other.data[r][c];\n                }\n            }\n\n            return new TorchNew.FastTensor(result);\n        }\n\n        /**\n        * Computes the sum of all elements in the tensor.\n        * @returns The sum of all tensor elements.\n        */\n        sum(): number {\n            return this.data.reduce((acc: number, row: number[]) => acc + row.reduce((rowAcc: number, value: number) => rowAcc + value, 0), 0);\n        }\n    }\n}","FastLinear.ts":"namespace TorchNew {\r\n\r\n    export class FastLinear {\r\n        inFeatures: number\r\n        outFeatures: number\r\n        weight: TorchNew.FastTensor\r\n        bias: TorchNew.FastTensor\r\n        lastInput:FastTensor;\r\n\r\n        constructor(inFeatures: number, outFeatures: number) {\r\n            this.inFeatures = inFeatures\r\n            this.outFeatures = outFeatures\r\n\r\n            // Weight: [outFeatures][inFeatures]\r\n            let w: number[][] = []\r\n            for (let r = 0; r < outFeatures; r++) {\r\n                let row: number[] = []\r\n                for (let c = 0; c < inFeatures; c++) {\r\n                    // Xavier-like init\r\n                    row.push((Math.random() - 0.5) * 2 / Math.sqrt(inFeatures))\r\n                }\r\n                w.push(row)\r\n            }\r\n            this.weight = new TorchNew.FastTensor(w)\r\n\r\n            // Bias: [outFeatures][1] (or just a row vector)\r\n            let b: number[][] = []\r\n            let brow: number[] = []\r\n            for (let i = 0; i < outFeatures; i++) brow.push(0)\r\n            b.push(brow)\r\n            this.bias = new TorchNew.FastTensor(b)\r\n        }\r\n\r\n        // Forward: X (batch Ã— inFeatures) * W^T (inFeatures Ã— outFeatures) + b\r\n        forward(x: TorchNew.FastTensor): TorchNew.FastTensor {\r\n            // x.data: [batch][inFeatures]\r\n            // weight.data: [outFeatures][inFeatures]\r\n            // but matmul expects: A(rowsA Ã— colsA) * B(rowsB Ã— colsB)\r\n            // so we need weight^T: [inFeatures][outFeatures]\r\n\r\n            this.lastInput = x\r\n\r\n            let Wt = this.transpose2D(this.weight.data)\r\n\r\n            // matmul: (batch Ã— inFeatures) * (inFeatures Ã— outFeatures)\r\n            let out = x.matmul(new TorchNew.FastTensor(Wt)) as TorchNew.FastTensor\r\n\r\n            // Add bias row-wise\r\n            let batch = out.data.length\r\n            let outF = this.outFeatures\r\n\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < outF; c++) {\r\n                    out.data[r][c] += this.bias.data[0][c]\r\n                }\r\n            }\r\n\r\n            return out\r\n        }\r\n\r\n        backward(dY: TorchNew.FastTensor, x: TorchNew.FastTensor) {\r\n            // dx = dY @ W\r\n            let dx = dY.matmul(this.weight) as TorchNew.FastTensor\r\n\r\n            // dW = dY^T @ x\r\n            let dYt = new TorchNew.FastTensor(this.transpose2D(dY.data))\r\n            let dW = dYt.matmul(x) as TorchNew.FastTensor\r\n\r\n            // db = sum over batch rows of dY\r\n            let dbRow: number[] = []\r\n            for (let i = 0; i < this.outFeatures; i++) dbRow.push(0)\r\n\r\n            let batch = dY.data.length\r\n            for (let r = 0; r < batch; r++) {\r\n                for (let c = 0; c < this.outFeatures; c++) {\r\n                    dbRow[c] += dY.data[r][c]\r\n                }\r\n            }\r\n\r\n            let db = new TorchNew.FastTensor([dbRow])\r\n\r\n            return {\r\n                dx: dx,\r\n                dW: dW,\r\n                db: db\r\n            }\r\n        }\r\n\r\n\r\n\r\n        // Simple 2D transpose helper\r\n        private transpose2D(m: number[][]): number[][] {\r\n            let rows = m.length\r\n            let cols = m[0].length\r\n            let out: number[][] = []\r\n\r\n            for (let c = 0; c < cols; c++) {\r\n                let row: number[] = []\r\n                for (let r = 0; r < rows; r++) {\r\n                    row.push(m[r][c])\r\n                }\r\n                out.push(row)\r\n            }\r\n            return out\r\n        }\r\n    }\r\n}","fastsequtial.ts":"namespace TorchNew {\n\n    export class FastSequential {\n        layers: any[]\n\n        constructor(layers: any[]) {\n            this.layers = layers\n        }\n\n        // Forward pass through all fast layers\n        forward(x: FastTensor): FastTensor {\n            let out = x\n            for (let i = 0; i < this.layers.length; i++) {\n                out = this.layers[i].forward(out)\n            }\n            return out\n        }\n\n        // Backward pass (reverse order)\n        backward(grad: FastTensor, x: FastTensor): FastTensor {\n            // For FastSequential, we assume each layer stores its own input\n            // OR the user passes the correct x for the first layer.\n            let g = grad\n\n            // We need to track inputs for each layer\n            // So we require each layer to store lastInput during forward\n            for (let i = this.layers.length - 1; i >= 0; i--) {\n                let layer = this.layers[i]\n\n                if (!layer.lastInput) {\n                    // If a layer didn't store its input, we cannot backprop\n                    // Fast layers should ALWAYS store lastInput\n                    console.log(\"FastSequential WARNING: layer missing lastInput\")\n                    g = layer.backward(g)\n                } else {\n                    g = layer.backward(g, layer.lastInput)\n                }\n            }\n\n            return g\n        }\n\n        // Collect parameters from all fast layers\n        parameters(): FastTensor[] {\n            let params: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.weight) params.push(L.weight)\n                if (L.bias) params.push(L.bias)\n            }\n            return params\n        }\n\n        // Collect gradients from all fast layers\n        gradients(): FastTensor[] {\n            let grads: FastTensor[] = []\n            for (let i = 0; i < this.layers.length; i++) {\n                let L = this.layers[i]\n                if (L.dW) grads.push(L.dW)\n                if (L.db) grads.push(L.db)\n            }\n            return grads\n        }\n    }\n}","FastActivaons.ts":"namespace TorchNew.Activations {\n\n    export interface FastActivation {\n        forward(x: FastTensor): FastTensor\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // ReLU\n    // ---------------------------------------------------------\n    export class FastReLU implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            // Store input for backward\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : 0)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            return new FastTensor(\n                x.data.map((row, r) =>\n                    row.map((v, c) => v > 0 ? gradOut.data[r][c] : 0)\n                )\n            )\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Sigmoid\n    // ---------------------------------------------------------\n    export class FastSigmoid implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => 1 / (1 + Math.exp(-v)))\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let s = 1 / (1 + Math.exp(-x.data[r][c]))\n                    row.push(gradOut.data[r][c] * s * (1 - s))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // Tanh\n    // ---------------------------------------------------------\n    export class FastTanh implements FastActivation {\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => {\n                let e2 = Math.exp(2 * v)\n                return (e2 - 1) / (e2 + 1)\n            })\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    let e2 = Math.exp(2 * v)\n                    let t = (e2 - 1) / (e2 + 1)\n                    row.push(gradOut.data[r][c] * (1 - t * t))\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n\n        lastInput: FastTensor\n    }\n\n    // ---------------------------------------------------------\n    // LeakyReLU\n    // ---------------------------------------------------------\n    export class FastLeakyReLU implements FastActivation {\n        alpha: number\n        lastInput: FastTensor\n\n        constructor(alpha: number) {\n            this.alpha = alpha\n        }\n\n        forward(x: FastTensor): FastTensor {\n            this.lastInput = x\n            return x.applyFunction(v => v > 0 ? v : this.alpha * v)\n        }\n\n        backward(x: FastTensor, gradOut: FastTensor): FastTensor {\n            let out: number[][] = []\n            for (let r = 0; r < x.data.length; r++) {\n                let row: number[] = []\n                for (let c = 0; c < x.data[0].length; c++) {\n                    let v = x.data[r][c]\n                    row.push(v > 0 ? gradOut.data[r][c] : this.alpha * gradOut.data[r][c])\n                }\n                out.push(row)\n            }\n            return new FastTensor(out)\n        }\n    }\n}","tofast.ts":"namespace TorchNew {\n\n    export function ToFast(t: Tensor): FastTensor {\n        // Reject 0D tensors\n        if (t.shape.length === 0) {\n            console.log(\"ToFast ERROR: Cannot convert 0D tensor\")\n            return null\n        }\n\n        // If 1D: [N] â†’ [1][N]\n        if (t.shape.length === 1) {\n            let row: number[] = []\n            for (let i = 0; i < t.data.length; i++) {\n                row.push(t.data[i])\n            }\n            return new FastTensor([row])\n        }\n\n        // If 2D: [A, B] â†’ [A][B]\n        if (t.shape.length === 2) {\n            let rows = t.shape[0]\n            let cols = t.shape[1]\n            let out: number[][] = []\n            let idx = 0\n\n            for (let r = 0; r < rows; r++) {\n                let row: number[] = []\n                for (let c = 0; c < cols; c++) {\n                    row.push(t.data[idx])\n                    idx++\n                }\n                out.push(row)\n            }\n\n            return new FastTensor(out)\n        }\n\n        // ND case: flatten all dims except last\n        // shape [D1, D2, ..., Dk, F] â†’ [D1*D2*...*Dk][F]\n        let lastDim = t.shape[t.shape.length - 1]\n        let batch = 1\n\n        for (let i = 0; i < t.shape.length - 1; i++) {\n            batch *= t.shape[i]\n        }\n\n        let out: number[][] = []\n        let idx = 0\n\n        for (let r = 0; r < batch; r++) {\n            let row: number[] = []\n            for (let c = 0; c < lastDim; c++) {\n                row.push(t.data[idx])\n                idx++\n            }\n            out.push(row)\n        }\n\n        return new FastTensor(out)\n    }\n}","fromfast.ts":"namespace TorchNew {\n\n    // Convert FastTensor (2D) â†’ full Tensor (ND) with a given shape\n    export function FromFast(ft: FastTensor, shape: number[]): Tensor {\n        // Validate shape\n        if (shape.length === 0) {\n            console.log(\"FromFast ERROR: Cannot create 0D tensor\")\n            return null\n        }\n\n        // Flatten FastTensor.data (number[][]) into number[]\n        let flat: number[] = []\n        for (let r = 0; r < ft.data.length; r++) {\n            for (let c = 0; c < ft.data[0].length; c++) {\n                flat.push(ft.data[r][c])\n            }\n        }\n\n        // Validate total size\n        let expected = 1\n        for (let i = 0; i < shape.length; i++) {\n            expected *= shape[i]\n        }\n\n        if (expected !== flat.length) {\n            console.log(\"FromFast ERROR: Shape mismatch. Expected size \" + expected + \" but got \" + flat.length)\n            return null\n        }\n\n        // Create full Tensor\n        return new Tensor(flat, shape.slice(0))\n    }\n}","pxt.json":"{\n    \"name\": \"Torch-new\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"device\": \"*\",\n        \"Promise<somehting>\": \"workspace:2da7244f-461a-41b8-3361-81237d8dceed\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"assets.json\",\n        \"tensor.ts\",\n        \"linear.ts\",\n        \"cnn.ts\",\n        \"adam.ts\",\n        \"activactions.ts\",\n        \"softmax.ts\",\n        \"sequtial.ts\",\n        \"embedding.ts\",\n        \"avgpoolnd.ts\",\n        \"flatten.ts\",\n        \"layernorm.ts\",\n        \"dropout.ts\",\n        \"crossentropyloss.ts\",\n        \"positalencoding.ts\",\n        \"maxpoolnd.ts\",\n        \"convtransposend.ts\",\n        \"rnn.ts\",\n        \"gru.ts\",\n        \"lstm.ts\",\n        \"residual.ts\",\n        \"feedforward.ts\",\n        \"multiheadattention.ts\",\n        \"tranformerencoder.ts\",\n        \"tranformerdecoder.ts\",\n        \"tranformermodel.ts\",\n        \"fasttensor.ts\",\n        \"FastLinear.ts\",\n        \"fastsequtial.ts\",\n        \"FastActivaons.ts\",\n        \"tofast.ts\",\n        \"fromfast.ts\"\n    ],\n    \"preferredEditor\": \"tsprj\"\n}\n"}}],"shares":[],"lastSaveTime":1771528493844}